<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Preparing for an Engagement</h1>
                </header>
            
            <article>
                
<p class="mce-root">When you've narrowed down your search to the application you'd like to test, it's time to start collecting information. Getting a full sitemap, unmasking hidden content, and discovering artifacts left over from development (commented-out code, inline documentation, and so on) can help your narrow your focus to fertile areas. And by understanding what information you'll need for your vulnerability report, you can ensure you're collecting everything you need for when it's time to submit, right from the start.</p>
<p class="mce-root">This chapter discusses techniques to map your target application's attack surface, search the site for hidden directories and leftover (but accessible) services, make informed decisions about what tools to use in a pentesting session, and document your sessions for your eventual report.</p>
<p>We'll cover the following topics:</p>
<ul>
<li>Understanding your target application's points of interest</li>
<li>Setting up and using Burp Suite</li>
<li>Where to find open source lists of XSS snippets, SQLi payloads, and other code</li>
<li>Gathering DNS and other network information about your target</li>
<li>Creating a stable of small, versatile scripts for information-gathering</li>
<li>Checking for known component vulnerabilities</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical Requirements</h1>
                </header>
            
            <article>
                
<p>This chapter, like many, will rely on a <kbd>unix</kbd> command shell (<kbd>zsh</kbd>) to bootstrap and interact with programs installed via  their graphical installer, a package manager (<kbd>homebrew</kbd>), or a tarball. It will also include several desktop apps, all of which we'll install, via similar methods, into a macOS High Sierra (<kbd>10.13.2</kbd>) environment. When a web browser is required, we will use Chrome (<kbd>66.0.3359.139</kbd>).</p>
<p>For some of these, there will be an explicit Windows option. In that case, the menus may look different but the available actions will be the same. When no Windows option is available, you might have to dual-boot with one of the more user-friendly Linux distros.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tools</h1>
                </header>
            
            <article>
                
<p>We'll be using a variety of tools this chapter, some of which we'll be coming back to throughout the book:</p>
<ul>
<li><kbd>wfuzz</kbd></li>
<li><kbd>scrapy</kbd></li>
<li><kbd>striker</kbd></li>
<li>Burp Suite</li>
<li>Homebrew (package manager)</li>
<li>SecLists</li>
<li><kbd>virtualenv</kbd></li>
<li><kbd>jenv</kbd>(Java version manager)</li>
<li><strong>Java Development Kit</strong> (<strong>JDK</strong>)</li>
<li><strong>Java Runtime Environment</strong> (<strong>JRE</strong>) 1.6 or greater</li>
</ul>
<p><kbd>wfuzz</kbd> is a fuzzer and discovery tool built by pentesters for pentesters. To install it, simply use <kbd>pip</kbd>: <kbd>pip install wfuzz</kbd>.</p>
<p>Homebrew is an excellent package manager for macOS that allows you to install dependencies from the command line, much like you would with <kbd>apt-get</kbd> in Debian or <kbd>yum</kbd> in Redhat-flavored Linux distributions. Homebrew is easily installed via its website (<a href="https://brew.sh/">https://brew.sh/</a>), then packages can be installed simply via <kbd>brew install &lt;PACKAGE_NAME&gt;</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Burp Suite requires a JRE (version 1.6 or greater), but we'll also need the JDK to use the <kbd>java</kbd> command line tool to bootstrap Burp Suite from the command line. Running Burp from the command line lets us pass in settings via arguments that give us more control over the execution environment.</p>
<div class="packt_infobox"><br/>
Please install Burp Suite by following the directions on Portswigger's website: <a href="https://portswigger.net/burp/help/suite_gettingstarted">https://portswigger.net/burp/help/suite_gettingstarted</a>.</div>
<p>To use Burp Suite, you need to run a legacy version of Java. If you try to start Burp from its CLI with Java 10.0.0 or later, you'll receive a message to the effect that Burp has not been tested on this version and is susceptible to errors.</p>
<p>If you just need Java for Burp, you can install an older version—we'll be using Java <kbd>1.8.0</kbd> (Java 8)—and use that system-wide. But if you need a more up-to-date Java installation for other programs, you can still run legacy Java by using the <kbd>jenv</kbd> command-line utility that allows you to switch between versions. <kbd>jenv</kbd> is similar to the Ruby version manager <kbd>rvm</kbd> or the Node version manager <kbd>nvm</kbd>, they all allow you add, list, and switch between versions of the language with just a few commands.</p>
<div class="packt_infobox">Please install <kbd>jenv</kbd> from its website: <a href="http://www.jenv.be/">http://www.jenv.be/</a>.</div>
<p>After you've installed <kbd>jenv</kbd>, you can add a new Java version to it simply by using the path to its <kbd>/Home</kbd> directory. Then we'll set our system to use it:</p>
<pre><strong>jenv add /Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home</strong><br/><strong>jenv global 1.8</strong></pre>
<p>You might have to restart your Terminal. But you should have Java 8 installed! Check it's Java 8 with <kbd>java -version</kbd>. You should see this output:</p>
<pre><strong>java version "1.8.0_172"</strong><br/><strong>Java(TM) SE Runtime Environment (build 1.8.0_172-b11)</strong><br/><strong>Java HotSpot(TM) 64-Bit Server VM (build 25.172-b11, mixed mode)</strong></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Burp</h1>
                </header>
            
            <article>
                
<p>Now let's start Burp <span>–</span> the <kbd>4G</kbd> part of the command is where we're specifying Burp Suite should run on 4 GB memory:</p>
<pre><strong>java -jar -Xmx4G "/Applications/Burp Suite Community Edition.app/Contents/java/app/burp/burpsuite_community_1.7.33-9.jar"</strong></pre>
<p>Since this is a mouthful, we can create a small wrapper script that will use the <kbd>$@</kbd> variable to add any options we may want to pass, without making us rewrite our path to the <kbd>.jar</kbd> executable. Here's <kbd>bootstrap_burp.sh</kbd>:</p>
<pre><strong>#!/bin/sh</strong><br/><br/><strong>java -jar -Xmx3G "/Applications/Burp Suite Community Edition.app/Contents/java/app/burp/burpsuite_community_1.7.33-9.jar" $@</strong></pre>
<p>Now you can make the file executable and symlink it to <kbd>/usr/local/bin</kbd> or the appropriate utility so it's available in your <kbd>$PATH</kbd>:</p>
<pre><strong>chmod u+x bootstrap_burp.sh</strong><br/><strong>sudo ln -s /Full/path/to/bootstrap_burp.sh /usr/local/bin/bootstrap_burp</strong></pre>
<p>This allows us to start the program with just <kbd>bootstrap_burp</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Attack Surface Reconnaisance – Strategies and the Value of Standardization</h1>
                </header>
            
            <article>
                
<p>The Attack Surface of an application is, put succinctly, wherever data can enter or exit the app. Attack-surface analysis describes the methods used to describe the vulnerable parts of an application. There are formal processes, such as the<span> </span><strong>Relative Attack Surface Quotient</strong> (<strong>RASQ</strong>) developed by Michael Howard and other researchers at Microsoft that counts a system's attack opportunities and indicates an app's general attackability. There are programmatic means available through scanners and manual methods, involving navigating a site directly, documenting weak points via screenshots and other notes. We'll talk about low- and high-tech methods you can use to focus your attention on profitable lines of attack, in addition to methods you can use to find hidden or leftover content not listed on the sitemap.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sitemaps</h1>
                </header>
            
            <article>
                
<p>Sitemaps are an absurdly simple way of doing basic research with zero effort. Doing a little URL hacking with the  <kbd>sitemap.xml</kbd> slug will often return either an actual XML file detailing the site's structure, or a Yoast-or-other-seo-plugin-supplied HTML page documenting different areas of the site, with separate sitemaps for posts, pages, and so on.</p>
<p>The following is an example of a Yoast-generated sitemap page:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5fe190f8-d9b7-4db7-b2f4-083af5cd0c23.png" style=""/></div>
<p>It helpfully exposes the high-level structure of the site while allowing you to focus on important points. Some areas can be skipped: the <kbd>post-sitemap1.xml</kbd> and <kbd>post-sitemap2.xml</kbd> sections, listing the links to every blog post on the site, aren't useful because every blog post will more or less have the same points of attack (comments, like/dislike buttons, and social sharing).</p>
<p>While <kbd>wp_quiz-sitemap.xml</kbd> hints at a tantalizing set of form fields, along with telling us the site is a WordPress application if we didn't already know, the <kbd>page-sitemap.xml</kbd> will give us a broader swath of site functionality:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/ba9a32c4-62dc-4933-9963-b864c49d4a74.png" style=""/></div>
<p>Here, too, there are candidates for immediate follow-up and dismissal. Purely informational pages such as <kbd>/privacy-policy</kbd>, <kbd>/method/rule-two</kbd>, and <kbd>/pricing-guarantee</kbd>, are simple markup, with no opportunity to interact with the server or an external service. Pages such as <kbd>/contact-us</kbd>, <kbd>/book-preorder-entry-form</kbd> (the form's in the title!), and <kbd>/referral</kbd> (which might have a form for submitting them) are all worth a follow-up. <kbd>/jobs</kbd>, which could have a resume-submission field or could be just job listings, is a gray area. Some pages will simply need to be perused.</p>
<p>Sitemaps aren't always available <span>– </span>and they're always limited to what the site wants to show you <span>–</span> but they can be useful starting points for further investigation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scanning and Target Reconaissance</h1>
                </header>
            
            <article>
                
<p>Automated information-gathering is a great way to get consistent, easy-to-understand information about site layout, attack surface, and security posture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Brute-forcing Web Content</h1>
                </header>
            
            <article>
                
<p>Fuzzing tools such as <kbd>wfuzz</kbd> can be used to discover web content by trying different paths, with URIs taken from giant wordlists, then analyzing the HTTP status codes of the responses to discover hidden directories and files. <kbd>wfuzz</kbd> is versatile and can do both content-discovery and form-manipulation. It's easy to get started with, and because <kbd>wfuzz</kbd> supports plugins, recipes, and other advanced features, it can be extended and customized into other workflows.</p>
<p>The quality of the wordlists you're using to brute-force-discover hidden content is important. After installing <kbd>wfuzz</kbd>, clone the SecLists GitHub repository (a curated collection of fuzz lists, SQLi injection scripts, XSS snippets, and other generally malicious input) at <a href="https://github.com/danielmiessler/SecLists">https://github.com/danielmiessler/SecLists</a>. We can start a scan of the target site simply be replacing the part of the URL we'd like to replace with the wordlist with the <kbd>FUZZ</kbd> string:</p>
<pre><strong>wfuzz -w ~/Code/SecLists/Discovery/Web-Content/SVNDigger/all.txt --hc 404 http://webscantest.com/FUZZ</strong></pre>
<p>As you can tell from the command, we passed in the web-content discovery list from SVNDigger with the <kbd>-w</kbd> flag, <kbd>-hc</kbd> tells the scan to ignore 404 status codes (hide code), and then the final argument is the URL we want to target:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b3390347-3cd8-480d-94d7-4cb1082f1e42.png" style=""/></div>
<p>You can see some interesting points to explore. While the effectiveness of brute-force tools is dictated by their wordlists, you can find effective jumping-off points as long as you do your research.</p>
<p>Keep in mind that brute-forcers are very noisy. Only use them against isolated staging/QA environments, and only with permission. If your brute-forcer overwhelms a production server, it's really no different from a DoS attack.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spidering and Other Data-Collection Techniques</h1>
                </header>
            
            <article>
                
<p>Parallel to brute-forcing for sensitive assets, spidering can help you get a picture of a site that, without a sitemap, just brute-forcing itself can't provide. That link base can also be shared with other tools, pruned of any out-of-scope or irrelevant entries, and subjected to more in-depth analysis. There are a couple of useful spiders, each with its own advantages. The first one we'll cover, Burp's native spider functionality, is obviously a contender because it's part of (and integrates with) a tool that's probably already part of your toolset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Burp Spider</h1>
                </header>
            
            <article>
                
<p>To kick-off a spidering session, make sure you have the appropriate domains in scope:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6bdea000-cc41-4ad8-8e62-c88df3a23401.png"/></div>
<p class="mce-root"/>
<p>You can then right-click the target domain and select <span class="packt_screen">Spider this host</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/00ba4914-e8cf-4037-93d7-5a90a3b664de.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Striker</h1>
                </header>
            
            <article>
                
<p>Striker (<a href="https://github.com/s0md3v/Striker">https://github.com/s0md3v/Striker</a>) is a Python-offensive information and vulnerability scanner that does a number of checks using different sources, but has a particular focus on DNS and network information. You can install it by following the instructions on its Github page. Like many Python projects, it simply requires cloning the code and downloading the dependencies listed in <kbd>requirements.txt</kbd>.</p>
<p>Striker provides useful, bedrock network identification and scanning capabilities:</p>
<ul>
<li>Fingerprinting the target web server</li>
<li>Detecting CMS (197+ supported)</li>
<li>Scanning target ports</li>
<li>Looking up <kbd>whois</kbd> information</li>
</ul>
<p>It also provides a grab-bag of other functionality, such as  launching WPScan for WordPress instances or bypassing Cloudflare:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3ee7f15d-7702-46da-8803-c5c6a7258804.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scrapy and Custom Pipelines</h1>
                </header>
            
            <article>
                
<p><kbd>scrapy</kbd> is a popular web-crawling framework for Python that allows you to create web crawlers out of the box. It's a powerful general-purpose tool that, since it allows a lot of customization, has naturally found its way into professional security workflows. Projects such as XSScrapy, an XSS and SQLi scanning tool built on Scrapy, show the underlying base code's adaptability. Unlike the Burp Suite Spider, whose virtue is that it integrates easily with other Burp tools, and Striker, whose value comes in collecting DNS and networking info from its default configuration, Scrapy's appeal is that it can be set up easily and then customized to create any kind of data pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Manual Walkthroughs</h1>
                </header>
            
            <article>
                
<p>If the app doesn't have a sitemap, and you don't want to use a scanner, you can still create a layout of the site's structure by navigating through it, without having to take notes or screenshots. Burp allows you to link your browser to the application's proxy, where it will then keep a record of all the pages you visit as you step through the site. As you map the site's attack surface, you can add or remove pages from the scope to ensure you control what gets investigated with automated workflows.</p>
<p>Doing this manual-with-an-assist method can actually be preferable to using an automated scanner. Besides being less noisy and less damaging to target servers, the manual method lets you tightly control what gets considered in-scope and investigated.</p>
<p>First, connect your browser to the Burp proxy.</p>
<p>Portswigger provides support articles to help you. If you're using Chrome, you can follow along with me here. Even though we're using Chrome, we're going to use the Burp support article for Safari because the setting in question is in your Mac settings: <a href="https://support.portswigger.net/customer/portal/articles/1783070-Installing_Configuring%20your%20Browser%20-%20Safari.html">https://support.portswigger.net/customer/portal/articles/1783070-Installing_Configuring%20your%20Browser%20-%20Safari.html</a>.<a href="https://support.portswigger.net/customer/portal/articles/1783070-Installing_Configuring%20your%20Browser%20-%20Safari.html"/></p>
<p>Once your browser is connected and on (and you've turned the <span class="packt_screen">Intercept</span> function off), go to <kbd>http://burp/</kbd>.</p>
<p>If you do this through your Burp proxy, you'll be redirected to a page where you can download the Burp certificate. We'll need the certificate to remove any security warnings and allow our browser to install static assets:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fc25ed5a-8120-4266-b260-bf06e6c5f281.png" style=""/></div>
<p class="mce-root"/>
<p>After you download the certificate, you just need to go to your <span class="packt_screen">K<span class="packt_screen">eycha</span></span><span class="packt_screen">ins</span> settings, <span class="packt_screen">File </span>| <span class="packt_screen">Import Items</span>, and upload your Burp certificate(a  <kbd>.der</kbd> file). Then you can double-click it to open another window where you can select <span class="packt_screen">Always Trust This Certificate</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/444b695c-ad20-4709-b140-77b89d26cb93.png"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>After browsing around a site, you'll start to see it populating information in Burp. Under the <span class="packt_screen">Target</span> | <span class="packt_screen">Site map</span> tabs, you can see URLs you've hit as you browse through Burp:</p>
<div class="CDPAlignCenter CDPAlign">020<img src="assets/5f77b98a-44e8-4819-9352-0e730456c77e.png"/></div>
<p>Logging into every form, clicking on every tab, following every button <span>–</span> eventually you'll build up a good enough picture of the application to inform the rest of your research. And because you're building this picture within Burp, you can add or remove URLs from scope, and send the information you're gathering for follow-up investigations in other Burp tools.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Source Code</h1>
                </header>
            
            <article>
                
<p>Source-code analysis is typically thought of as something that only takes place in a white box, an internal testing scenario, either as part of an automated build chain or as a manual review. But analyzing client-side code available to the browser is also an effective way of looking for vulnerabilities as an outside researcher.</p>
<p>We're specifically going to look at <kbd>retire</kbd> (Retire.js), a node module that has both Node and CLI components, and analyzes client-side JavaScript and Node modules for previously-reported vulnerabilities. You can install it easily using <kbd>npm</kbd> and then using the global flag (<kbd>-g</kbd>) to make it accessible in your <kbd>$PATH</kbd>: <kbd>npm install -g retire</kbd>. Reporting a bug that may have been discovered in a vendor's software, but still requires addressing/patching in a company's web application, will often merit a reward. The easy-to-use CLI of <kbd>retire</kbd> makes it simple to write short, purpose-driven scripts in the Unix style. We'll be using it to elaborate on a general philosophy of pentesting automation.</p>
<p><kbd>retire --help</kbd> shows you the general contour of functionality:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/8ac7181a-92ef-4a6b-86b0-4988dbbb00d6.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Let's test it against an old project of mine written in Angular and node:</p>
<pre class="mce-root"><strong>retire --path ~/Code/Essences/demo</strong></pre>
<p class="mce-root"/>
<p>It's a little hard to read. And the attempt to show the vulnerable modules within their nested dependencies makes it even harder:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9cae1a88-0452-4554-bb5e-75a5243f2e36.png"/></div>
<p>But we can use some of its available flags to rectify this. As we pass in options to output the data in the <kbd>json</kbd> format and specify the name of the file we want to save, we can also wrap it in a script to make it a handier reference from the command line. Let's make a script called <kbd>scanjs.sh</kbd>:</p>
<pre><strong>#!/bin/sh</strong><br/><br/><strong>retire --path $1 --outputformat json --outputpath $2; python -m json.tool $2</strong></pre>
<p class="mce-root">This script requires two arguments, the path to the files being analyzed and a name for the file it will output. Basically the script analyzes the target code repository, creates a <kbd>json</kbd> file of the vulnerabilities it discovers, then prints out a pretty version of the <kbd>json</kbd> file to <kbd>STDOUT</kbd>. The script has two outputs so that it can use the <kbd>json</kbd> file as a local flat file log, and the <kbd>STDOUT</kbd> output to pass on to the next step, a formatting script.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a Process</h1>
                </header>
            
            <article>
                
<p>If we think about how to build processes the Unix way, with small scripts responsible for single concerns, chained together into more complex workflows (all built on the common foundation of plain text) it makes sense to boil down our automated reconnaissance tools into the smallest reusable parts.<br/>
One part is that wrapper script we just wrote, <kbd>scanjs.sh</kbd>. This script scans the client-side code of a website (currently from a repo) and compiles a report in <kbd>json</kbd>, which it both saves and displays.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Formatting the JS Report</h1>
                </header>
            
            <article>
                
<p>But to make better sense of that <kbd>json</kbd>, we need to format it in a way that pulls out the critical info (for example, severity, description, and location) while leaving out noise (for example, dependency graphs). Let's use Python, which is great for string manipulation and general data munging, to write a script that formats that <kbd>json</kbd> into a plain text report. We'll call the script <kbd>formatjs.py</kbd> to associate it with our other tool. The first thing we need to do is pull in <kbd>json</kbd> from <kbd>STDIN</kbd> and encode it as a Python data structure:</p>
<pre><strong>#!/usr/bin/env python2.7</strong><br/><br/><strong>import sys, json</strong><br/><br/><strong>data = json.load(sys.stdin)</strong></pre>
<p>Our goal is to create a table to display the data from the report, covering the <kbd>severity</kbd>, <kbd>summary</kbd>, <kbd>info</kbd>, and <kbd>file</kbd> attributes for each vulnerability.</p>
<p>We'll be using a simple Python table library, <kbd>tabulate</kbd> (which you can install via <kbd>pip install tabulate</kbd>). As per the <kbd>tabulate</kbd> docs, you can create a table using a nested list, where the inner list contains the values of an individual table row. We're going to iterate over the different files analyzed, iterate over each vulnerability, and process their attributes into <kbd>row</kbd> lists that we'll collect in our <kbd>rows</kbd> nested list:</p>
<pre><strong>rows = []</strong><br/><br/><strong>for item in data:</strong><br/><strong>    for vulnerability in item['results'][0]['vulnerabilities']:</strong><br/><strong>        vulnerability['file'] = item.get('file', 'N/A')</strong><br/><strong>        row = format_bug(vulnerability)</strong><br/><strong>        rows.append(row)</strong></pre>
<p>That <kbd>format_bug()</kbd> function will just pull out the information we care about from the <kbd>vulnerability</kbd> dictionary and order the info properly in a list the function will return:</p>
<pre><strong>def format_bug(vulnerability):</strong><br/><strong>    row = [</strong><br/><strong>        vulnerability['severity'],</strong><br/><strong>        vulnerability.get('identifiers').get('summary', 'N/A') if vulnerability.get('identifiers', False) else 'N/A',</strong><br/><strong>        vulnerability['file'] + "\n" + vulnerability.get('info', ['N/A'])[0]</strong><br/><strong>    ]</strong><br/><strong>    return row</strong></pre>
<p>Then we'll sort the vulnerabilities by severity so that all the different types (high, medium, low, and so on) are grouped together:</p>
<pre><strong>print(</strong><br/><strong>"""</strong><br/><strong>     ,--. ,---.   ,-----.                        </strong><br/><strong>     |  |'   .-'  |  |) /_ ,--.,--. ,---.  ,---. </strong><br/><strong>,--. |  |`.  `-.  |  .-.  \|  ||  || .-. |(  .-' </strong><br/><strong>|  '-'  /.-'    | |  '--' /'  ''  '' '-' '.-'  `)</strong><br/><strong> `-----' `-----'  `------'  `----' .`-  / `----' </strong><br/><strong>                                   `---'            </strong><br/><strong>""")</strong><br/><strong>print tabulate(rows, headers=['Severity', 'Summary', 'Info &amp; File'])</strong></pre>
<p>Here's what it looks like all together, for reference:</p>
<pre><strong>#!/usr/bin/env python2.7</strong><br/><br/><strong>import sys, json</strong><br/><strong>from tabulate import tabulate</strong><br/><br/><strong>data = json.load(sys.stdin)</strong><br/><br/><strong>rows = []</strong><br/><br/><strong>def format_bug(vulnerability):</strong><br/><strong>    row = [</strong><br/><strong>        vulnerability['severity'],</strong><br/><strong>        vulnerability.get('identifiers').get('summary', 'N/A') if vulnerability.get('identifiers', False) else 'N/A',</strong><br/><strong>        vulnerability['file'] + "\n" + vulnerability.get('info', ['N/A'])[0]</strong><br/><strong>    ]</strong><br/><strong>    return row</strong><br/><br/><strong>for item in data:</strong><br/><strong>    for vulnerability in item['results'][0]['vulnerabilities']:</strong><br/><strong>        vulnerability['file'] = item.get('file', 'N/A')</strong><br/><strong>        row = format_bug(vulnerability)</strong><br/><strong>        rows.append(row)</strong><br/><br/><strong>rows = sorted(rows, key=lambda x: x[0])</strong><br/><br/><strong>print(</strong><br/><strong>"""</strong><br/><strong>     ,--. ,---.   ,-----.                        </strong><br/><strong>     |  |'   .-'  |  |) /_ ,--.,--. ,---.  ,---. </strong><br/><strong>,--. |  |`.  `-.  |  .-.  \|  ||  || .-. |(  .-' </strong><br/><strong>|  '-'  /.-'    | |  '--' /'  ''  '' '-' '.-'  `)</strong><br/><strong> `-----' `-----'  `------'  `----' .`-  / `----' </strong><br/><strong>                                   `---'            </strong><br/><strong>""")</strong><br/><strong>print tabulate(rows, headers=['Severity', 'Summary', 'Info &amp; File'])</strong></pre>
<p>And the following is what it looks like when it's run on the Terminal. I'm running the <kbd>scanjs.sh</kbd> script wrapper and then piping the data to <kbd>formatjs.py</kbd>. Here's the command:</p>
<pre><strong>./scanjs.sh ~/Code/Essences/demo test.json | python formatjs.py</strong></pre>
<p>And here's the output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4bd98ea7-6084-43ce-b026-ff4fea44c439.png"/></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading the JavaScript</h1>
                </header>
            
            <article>
                
<p>There's one more step before we can point this at a site <span>–</span> we need to download the actual JavaScript! Before analyzing the source code using our <kbd>scanjs</kbd> wrapper, we need to pull it from the target page. Pulling the code once in a single, discrete process (and from a single URL) means that, even as we develop more tooling around attack-surface reconnaissance, we can hook this script up to other services: it could pull the JavaScript from a URL supplied by a crawler, it could feed JavaScript or other assets into other analysis tools, or it could analyze other page metrics.</p>
<p>So the simplest version of this script should be: the script takes a URL, looks at the source code for that page to find all JavaScript libraries, and then downloads those files to the specified location.</p>
<p>The first thing we need to do is grab the HTML from the URL of the page we're inspecting. Let's add some code that accepts the <kbd>url</kbd> and <kbd>directory</kbd> CLI arguments, and defines our target and where to store the downloaded JavaScript. Then, let's use the <kbd>requests</kbd> library to pull the data and Beautiful Soup to make the HTML string a searchable object:</p>
<pre><strong>#!/usr/bin/env python2.7</strong><br/><br/><strong>import os, sys</strong><br/><strong>import requests</strong><br/><strong>from bs4 import BeautifulSoup</strong><br/><br/><strong>url = sys.argv[1]</strong><br/><strong>directory = sys.argv[2]</strong><br/><br/><strong>r = requests.get(url)</strong><br/><strong>soup = BeautifulSoup(r.text, 'html.parser')</strong></pre>
<p>Then we need to iterate over each script tag and use the <kbd>src</kbd> attribute data to download the file to a directory within our current root:</p>
<pre><strong>for script in soup.find_all('script'):</strong><br/><strong>    if script.get('src'): download_script(script.get('src'))</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>That <kbd>download_script()</kbd> function might not ring a bell because we haven't written it yet. But that's what we want <span>–</span> a function that takes the <kbd>src</kbd> attribute path, builds the link to the resource, and downloads it into the directory we've specified:</p>
<pre><strong>def download_script(uri):</strong><br/><strong>    address = url + uri if uri[0] == '/' else uri</strong><br/><strong>    filename = address[address.rfind("/")+1:address.rfind("js")+2] </strong><br/><strong>    req = requests.get(url)</strong><br/><strong>    with open(directory + '/' + filename, 'wb') as file:</strong><br/><strong>        file.write(req.content)</strong></pre>
<p>Each line is pretty direct. After the function definition, the HTTP address of the script is created using a Python ternary. If the <kbd>src</kbd> attribute starts with <kbd>/</kbd>, it's a relative path and can just be appended onto the hostname; if it doesn't, it must be a full/absolute link. Ternaries can be funky but also powerfully expressive once you get the hang of them.</p>
<p>The second line of the function creates the filename of the JavaScript library link by finding the character index of the last forward slash (<kbd>address.rfind("/")</kbd>) and the index of the <kbd>js</kbd> file extension, plus 2 to avoid slicing off the <kbd>js</kbd> part (<kbd>address.rfind("js")+2)</kbd>), and then uses the <kbd>[begin:end]</kbd> list-slicing syntax to create a new string from just the specified indices.</p>
<p>Then, in the third line, the script pulls data from the assembled address using <kbd>requests</kbd>, creates a new file using a context manager, and writes the page source code to <kbd>/directory/filename.js</kbd>. Now you have a location, the path passed in as an argument, and all of the JavaScript from a particular page saved inside of it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting It All Together</h1>
                </header>
            
            <article>
                
<p>So what does it look like when we put it all together? It's simple <span>– </span>we can construct a one-liner to scan the JavaScript of a target site just by passing the right directory references:</p>
<pre><strong>grabjs https://www.target.site sourcejs; scanjs sourcejs output.json | formatjs</strong></pre>
<p>Keep in mind we've already symlinked these scripts to our <kbd>/usr/local/bin</kbd> and changed their permissions using <kbd>chmod u+x</kbd> to make them executable and accessible from our path. With this command, we're telling our CL to download the JavaScript from <kbd>http://target.site</kbd> to the <kbd>sourcejs</kbd> directory, then scan that directory, create an <kbd>output.json</kbd> representation of the data, and finally format everything as a plain-text report.</p>
<p class="mce-root"/>
<p>As a means of testing the command, I  recently read a blog decrying the fact that jQuery, responsible for a large chunk of the web's client-side code, was running an out-of-date WordPress version on <a href="http://jquery.com/">http://jquery.com/</a>, so I decided to see whether their JavaScript had any issues:</p>
<pre><strong>grabjs https://jquery.com sourcejs; scanjs sourcejs output.json | formatjs</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b5b352c7-9a2d-4917-b3cf-6d27dc6f2207.png"/></div>
<p>The fact that <a href="http://jquery.com/">http://jquery.com/</a> has a few issues is nothing huge, but still surprising! Known component vulnerabilities in JavaScript are a widespread issue, affecting a sizable portion of sites (different methodologies put the number of affected sites at between one-third and three-quarters of the entire web).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Value Behind the Structure</h1>
                </header>
            
            <article>
                
<p>We've developed several scripts to achieve a single goal. The exercise begs this question: why didn't we write one program instead? We could've included all our steps (download the JSON, analyze it, print a report) in a Python or Shell script; wouldn't that have been easier?</p>
<p>But the advantage of our current setup is the modularity of the different pieces in the face of different workflows. For example, we might want to do all the steps at once, or we might just want a subset. If I've already downloaded all the JSON for a page and put it into a folder, scanned it, and created a report at <kbd>some-site-1-18-18.json</kbd>, then, when I visit the info, all I need is the ability to format the report from the raw <kbd>json</kbd>. I can achieve that with simple Unix:</p>
<pre><strong>cat output.json | formatjs</strong></pre>
<p class="mce-root"/>
<p>Or we might want to extend the workflow. Because the foundation is built on plain text, it's easy to add new pieces. If our <kbd>mail</kbd> utility is set up, we can email ourselves the results of the test:</p>
<pre><strong>grabjs https://www.target.site sourcejs; scanjs sourcejs output.json | formatjs | mail -s "JS Known Component Vulnerabilities" email@site.com</strong></pre>
<p>Or we could decide we only want to email ourselves the critical vulnerabilities. We could pull out the text we care about by using <kbd>ag</kbd>, a <kbd>grep</kbd>-like natural-language search utility known for its blazing speed:</p>
<pre><strong>grabjs https://www.target.site sourcejs; scanjs sourcejs output.json | formatjs | ag critical | mail -s "Critical JS Known Component Vulnerabilities" email@site.com</strong></pre>
<p>We could substitute using email as a notification with using a script invoking the Slack API or another messaging service <span>–</span> the possibilities are endless. The benefit from using these short, stitched-together programs, built around common input and output, is that they can be rearranged and added to at will. They are the building blocks for a wider range of combinations and services. They are also, individually, very simple scripts, and because they're invoked through and pass information back to the command line, can be written in a variety of languages. I've used Python and Shell in this work, but could employ Ruby, Perl, Node, or another scripting language, with similar success.</p>
<p>There are obviously a lot of ways these short scripts could be improved. They currently have no input-verification, error-handling, logging, default arguments, or other features meant to make them cleaner and more reliable. But as we progress through the book, we'll be building on top of the utilities we're developing until they become more reliable, professional tools. And by adding new options, we'll show the value of a small, interlocking toolset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter covered how to discover information about a site's attack surface using automated scanners, passive proxy interception, and command-line utilities wired into our own homebrew setup, and a couple of things in between. You learned some handy third-party tools, and also how to use them and others within the context of custom automation. Hopefully you've come away  not only with a sense of the tactics (the code we've written), but of the strategy as well (the design behind it).</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What's a good tool for finding hidden directories and secret files on a site?</li>
<li>How and where can you find a map of the site's architecture? How can you create one if it's not already there?</li>
<li>How can you safely create a map of an application's attack surface without using scanners or automated scripts?</li>
<li>What's a common resource in Python for scraping websites?</li>
<li>What are some advantages to writing scripts according to the Unix philosophy (single-purpose, connectable, built around text)?</li>
<li>What's a good resource for finding XSS submissions, SQLi snippets, and other fuzzing inputs?</li>
<li>What's a good resource for discovering DNS info associated with a target?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further Reading</h1>
                </header>
            
            <article>
                
<p>You can find out more about some of the topics we have discussed in this chapter at:</p>
<ul>
<li><strong>SecLists</strong>: <a href="https://github.com/danielmiessler/SecLists">https://github.com/danielmiessler/SecLists</a></li>
<li><strong>Measuring Relative Attack Surfaces</strong>: <a href="http://www.cs.cmu.edu/~wing/publications/Howard-Wing03.pdf">http://www.cs.cmu.edu/~wing/publications/Howard-Wing03.pdf</a></li>
<li><strong>XSScrapy</strong>: <a href="http://pentestools.com/xsscrapy-xsssqli-finder/">http://pentestools.com/xsscrapy-xsssqli-finder/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>