- en: Preparing for an Engagement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you've narrowed down your search to the application you'd like to test,
    it's time to start collecting information. Getting a full sitemap, unmasking hidden
    content, and discovering artifacts left over from development (commented-out code,
    inline documentation, and so on) can help your narrow your focus to fertile areas.
    And by understanding what information you'll need for your vulnerability report,
    you can ensure you're collecting everything you need for when it's time to submit,
    right from the start.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter discusses techniques to map your target application's attack surface,
    search the site for hidden directories and leftover (but accessible) services,
    make informed decisions about what tools to use in a pentesting session, and document
    your sessions for your eventual report.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding your target application's points of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up and using Burp Suite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where to find open source lists of XSS snippets, SQLi payloads, and other code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gathering DNS and other network information about your target
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a stable of small, versatile scripts for information-gathering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking for known component vulnerabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter, like many, will rely on a `unix` command shell (`zsh`) to bootstrap
    and interact with programs installed via their graphical installer, a package
    manager (`homebrew`), or a tarball. It will also include several desktop apps,
    all of which we'll install, via similar methods, into a macOS High Sierra (`10.13.2`)
    environment. When a web browser is required, we will use Chrome (`66.0.3359.139`).
  prefs: []
  type: TYPE_NORMAL
- en: For some of these, there will be an explicit Windows option. In that case, the
    menus may look different but the available actions will be the same. When no Windows
    option is available, you might have to dual-boot with one of the more user-friendly
    Linux distros.
  prefs: []
  type: TYPE_NORMAL
- en: Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll be using a variety of tools this chapter, some of which we''ll be coming
    back to throughout the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '`wfuzz`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scrapy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`striker`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burp Suite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Homebrew (package manager)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SecLists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtualenv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jenv`(Java version manager)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Java Development Kit** (**JDK**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Java Runtime Environment** (**JRE**) 1.6 or greater'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wfuzz` is a fuzzer and discovery tool built by pentesters for pentesters.
    To install it, simply use `pip`: `pip install wfuzz`.'
  prefs: []
  type: TYPE_NORMAL
- en: Homebrew is an excellent package manager for macOS that allows you to install
    dependencies from the command line, much like you would with `apt-get` in Debian
    or `yum` in Redhat-flavored Linux distributions. Homebrew is easily installed
    via its website ([https://brew.sh/](https://brew.sh/)), then packages can be installed
    simply via `brew install <PACKAGE_NAME>`.
  prefs: []
  type: TYPE_NORMAL
- en: Burp Suite requires a JRE (version 1.6 or greater), but we'll also need the
    JDK to use the `java` command line tool to bootstrap Burp Suite from the command
    line. Running Burp from the command line lets us pass in settings via arguments
    that give us more control over the execution environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please install Burp Suite by following the directions on Portswigger''s website:
    [https://portswigger.net/burp/help/suite_gettingstarted](https://portswigger.net/burp/help/suite_gettingstarted).'
  prefs: []
  type: TYPE_NORMAL
- en: To use Burp Suite, you need to run a legacy version of Java. If you try to start
    Burp from its CLI with Java 10.0.0 or later, you'll receive a message to the effect
    that Burp has not been tested on this version and is susceptible to errors.
  prefs: []
  type: TYPE_NORMAL
- en: If you just need Java for Burp, you can install an older version—we'll be using
    Java `1.8.0` (Java 8)—and use that system-wide. But if you need a more up-to-date
    Java installation for other programs, you can still run legacy Java by using the
    `jenv` command-line utility that allows you to switch between versions. `jenv`
    is similar to the Ruby version manager `rvm` or the Node version manager `nvm`,
    they all allow you add, list, and switch between versions of the language with
    just a few commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please install `jenv` from its website: [http://www.jenv.be/](http://www.jenv.be/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After you''ve installed `jenv`, you can add a new Java version to it simply
    by using the path to its `/Home` directory. Then we''ll set our system to use
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You might have to restart your Terminal. But you should have Java 8 installed!
    Check it''s Java 8 with `java -version`. You should see this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using Burp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s start Burp – the `4G` part of the command is where we''re specifying
    Burp Suite should run on 4 GB memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is a mouthful, we can create a small wrapper script that will use
    the `$@` variable to add any options we may want to pass, without making us rewrite
    our path to the `.jar` executable. Here''s `bootstrap_burp.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can make the file executable and symlink it to `/usr/local/bin` or
    the appropriate utility so it''s available in your `$PATH`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to start the program with just `bootstrap_burp`.
  prefs: []
  type: TYPE_NORMAL
- en: Attack Surface Reconnaisance – Strategies and the Value of Standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Attack Surface of an application is, put succinctly, wherever data can enter
    or exit the app. Attack-surface analysis describes the methods used to describe
    the vulnerable parts of an application. There are formal processes, such as the  **Relative
    Attack Surface Quotient** (**RASQ**) developed by Michael Howard and other researchers
    at Microsoft that counts a system's attack opportunities and indicates an app's
    general attackability. There are programmatic means available through scanners
    and manual methods, involving navigating a site directly, documenting weak points
    via screenshots and other notes. We'll talk about low- and high-tech methods you
    can use to focus your attention on profitable lines of attack, in addition to
    methods you can use to find hidden or leftover content not listed on the sitemap.
  prefs: []
  type: TYPE_NORMAL
- en: Sitemaps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sitemaps are an absurdly simple way of doing basic research with zero effort.
    Doing a little URL hacking with the `sitemap.xml` slug will often return either
    an actual XML file detailing the site's structure, or a Yoast-or-other-seo-plugin-supplied
    HTML page documenting different areas of the site, with separate sitemaps for
    posts, pages, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a Yoast-generated sitemap page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5fe190f8-d9b7-4db7-b2f4-083af5cd0c23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It helpfully exposes the high-level structure of the site while allowing you
    to focus on important points. Some areas can be skipped: the `post-sitemap1.xml`
    and `post-sitemap2.xml` sections, listing the links to every blog post on the
    site, aren''t useful because every blog post will more or less have the same points
    of attack (comments, like/dislike buttons, and social sharing).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While `wp_quiz-sitemap.xml` hints at a tantalizing set of form fields, along
    with telling us the site is a WordPress application if we didn''t already know,
    the `page-sitemap.xml` will give us a broader swath of site functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ba9a32c4-62dc-4933-9963-b864c49d4a74.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, too, there are candidates for immediate follow-up and dismissal. Purely
    informational pages such as `/privacy-policy`, `/method/rule-two`, and `/pricing-guarantee`,
    are simple markup, with no opportunity to interact with the server or an external
    service. Pages such as `/contact-us`, `/book-preorder-entry-form` (the form's
    in the title!), and `/referral` (which might have a form for submitting them)
    are all worth a follow-up. `/jobs`, which could have a resume-submission field
    or could be just job listings, is a gray area. Some pages will simply need to
    be perused.
  prefs: []
  type: TYPE_NORMAL
- en: Sitemaps aren't always available – and they're always limited to what the site
    wants to show you – but they can be useful starting points for further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Scanning and Target Reconaissance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automated information-gathering is a great way to get consistent, easy-to-understand
    information about site layout, attack surface, and security posture.
  prefs: []
  type: TYPE_NORMAL
- en: Brute-forcing Web Content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fuzzing tools such as `wfuzz` can be used to discover web content by trying
    different paths, with URIs taken from giant wordlists, then analyzing the HTTP
    status codes of the responses to discover hidden directories and files. `wfuzz`
    is versatile and can do both content-discovery and form-manipulation. It's easy
    to get started with, and because `wfuzz` supports plugins, recipes, and other
    advanced features, it can be extended and customized into other workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quality of the wordlists you''re using to brute-force-discover hidden content
    is important. After installing `wfuzz`, clone the SecLists GitHub repository (a
    curated collection of fuzz lists, SQLi injection scripts, XSS snippets, and other
    generally malicious input) at [https://github.com/danielmiessler/SecLists](https://github.com/danielmiessler/SecLists).
    We can start a scan of the target site simply be replacing the part of the URL
    we''d like to replace with the wordlist with the `FUZZ` string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can tell from the command, we passed in the web-content discovery list
    from SVNDigger with the `-w` flag, `-hc` tells the scan to ignore 404 status codes
    (hide code), and then the final argument is the URL we want to target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b3390347-3cd8-480d-94d7-4cb1082f1e42.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see some interesting points to explore. While the effectiveness of brute-force
    tools is dictated by their wordlists, you can find effective jumping-off points
    as long as you do your research.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that brute-forcers are very noisy. Only use them against isolated
    staging/QA environments, and only with permission. If your brute-forcer overwhelms
    a production server, it's really no different from a DoS attack.
  prefs: []
  type: TYPE_NORMAL
- en: Spidering and Other Data-Collection Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel to brute-forcing for sensitive assets, spidering can help you get a
    picture of a site that, without a sitemap, just brute-forcing itself can't provide.
    That link base can also be shared with other tools, pruned of any out-of-scope
    or irrelevant entries, and subjected to more in-depth analysis. There are a couple
    of useful spiders, each with its own advantages. The first one we'll cover, Burp's
    native spider functionality, is obviously a contender because it's part of (and
    integrates with) a tool that's probably already part of your toolset.
  prefs: []
  type: TYPE_NORMAL
- en: Burp Spider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To kick-off a spidering session, make sure you have the appropriate domains
    in scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6bdea000-cc41-4ad8-8e62-c88df3a23401.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can then right-click the target domain and select Spider this host:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/00ba4914-e8cf-4037-93d7-5a90a3b664de.png)'
  prefs: []
  type: TYPE_IMG
- en: Striker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Striker ([https://github.com/s0md3v/Striker](https://github.com/s0md3v/Striker))
    is a Python-offensive information and vulnerability scanner that does a number
    of checks using different sources, but has a particular focus on DNS and network
    information. You can install it by following the instructions on its Github page.
    Like many Python projects, it simply requires cloning the code and downloading
    the dependencies listed in `requirements.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Striker provides useful, bedrock network identification and scanning capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprinting the target web server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting CMS (197+ supported)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scanning target ports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking up `whois` information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It also provides a grab-bag of other functionality, such as launching WPScan
    for WordPress instances or bypassing Cloudflare:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3ee7f15d-7702-46da-8803-c5c6a7258804.png)'
  prefs: []
  type: TYPE_IMG
- en: Scrapy and Custom Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`scrapy` is a popular web-crawling framework for Python that allows you to
    create web crawlers out of the box. It''s a powerful general-purpose tool that,
    since it allows a lot of customization, has naturally found its way into professional
    security workflows. Projects such as XSScrapy, an XSS and SQLi scanning tool built
    on Scrapy, show the underlying base code''s adaptability. Unlike the Burp Suite
    Spider, whose virtue is that it integrates easily with other Burp tools, and Striker,
    whose value comes in collecting DNS and networking info from its default configuration,
    Scrapy''s appeal is that it can be set up easily and then customized to create
    any kind of data pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: Manual Walkthroughs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the app doesn't have a sitemap, and you don't want to use a scanner, you
    can still create a layout of the site's structure by navigating through it, without
    having to take notes or screenshots. Burp allows you to link your browser to the
    application's proxy, where it will then keep a record of all the pages you visit
    as you step through the site. As you map the site's attack surface, you can add
    or remove pages from the scope to ensure you control what gets investigated with
    automated workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Doing this manual-with-an-assist method can actually be preferable to using
    an automated scanner. Besides being less noisy and less damaging to target servers,
    the manual method lets you tightly control what gets considered in-scope and investigated.
  prefs: []
  type: TYPE_NORMAL
- en: First, connect your browser to the Burp proxy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Portswigger provides support articles to help you. If you''re using Chrome,
    you can follow along with me here. Even though we''re using Chrome, we''re going
    to use the Burp support article for Safari because the setting in question is
    in your Mac settings: [https://support.portswigger.net/customer/portal/articles/1783070-Installing_Configuring%20your%20Browser%20-%20Safari.html](https://support.portswigger.net/customer/portal/articles/1783070-Installing_Configuring%20your%20Browser%20-%20Safari.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Once your browser is connected and on (and you've turned the Intercept function
    off), go to `http://burp/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do this through your Burp proxy, you''ll be redirected to a page where
    you can download the Burp certificate. We''ll need the certificate to remove any
    security warnings and allow our browser to install static assets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/fc25ed5a-8120-4266-b260-bf06e6c5f281.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After you download the certificate, you just need to go to your Keychains settings,
    File | Import Items, and upload your Burp certificate(a `.der` file). Then you
    can double-click it to open another window where you can select Always Trust This
    Certificate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/444b695c-ad20-4709-b140-77b89d26cb93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After browsing around a site, you''ll start to see it populating information
    in Burp. Under the Target | Site map tabs, you can see URLs you''ve hit as you
    browse through Burp:'
  prefs: []
  type: TYPE_NORMAL
- en: 020![](assets/5f77b98a-44e8-4819-9352-0e730456c77e.png)
  prefs: []
  type: TYPE_NORMAL
- en: Logging into every form, clicking on every tab, following every button – eventually
    you'll build up a good enough picture of the application to inform the rest of
    your research. And because you're building this picture within Burp, you can add
    or remove URLs from scope, and send the information you're gathering for follow-up
    investigations in other Burp tools.
  prefs: []
  type: TYPE_NORMAL
- en: Source Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Source-code analysis is typically thought of as something that only takes place
    in a white box, an internal testing scenario, either as part of an automated build
    chain or as a manual review. But analyzing client-side code available to the browser
    is also an effective way of looking for vulnerabilities as an outside researcher.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re specifically going to look at `retire` (Retire.js), a node module that
    has both Node and CLI components, and analyzes client-side JavaScript and Node
    modules for previously-reported vulnerabilities. You can install it easily using
    `npm` and then using the global flag (`-g`) to make it accessible in your `$PATH`:
    `npm install -g retire`. Reporting a bug that may have been discovered in a vendor''s
    software, but still requires addressing/patching in a company''s web application,
    will often merit a reward. The easy-to-use CLI of `retire` makes it simple to
    write short, purpose-driven scripts in the Unix style. We''ll be using it to elaborate
    on a general philosophy of pentesting automation.'
  prefs: []
  type: TYPE_NORMAL
- en: '`retire --help` shows you the general contour of functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8ac7181a-92ef-4a6b-86b0-4988dbbb00d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s test it against an old project of mine written in Angular and node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s a little hard to read. And the attempt to show the vulnerable modules
    within their nested dependencies makes it even harder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9cae1a88-0452-4554-bb5e-75a5243f2e36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But we can use some of its available flags to rectify this. As we pass in options
    to output the data in the `json` format and specify the name of the file we want
    to save, we can also wrap it in a script to make it a handier reference from the
    command line. Let''s make a script called `scanjs.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This script requires two arguments, the path to the files being analyzed and
    a name for the file it will output. Basically the script analyzes the target code
    repository, creates a `json` file of the vulnerabilities it discovers, then prints
    out a pretty version of the `json` file to `STDOUT`. The script has two outputs
    so that it can use the `json` file as a local flat file log, and the `STDOUT`
    output to pass on to the next step, a formatting script.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we think about how to build processes the Unix way, with small scripts responsible
    for single concerns, chained together into more complex workflows (all built on
    the common foundation of plain text) it makes sense to boil down our automated
    reconnaissance tools into the smallest reusable parts.
  prefs: []
  type: TYPE_NORMAL
- en: One part is that wrapper script we just wrote, `scanjs.sh`. This script scans
    the client-side code of a website (currently from a repo) and compiles a report
    in `json`, which it both saves and displays.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting the JS Report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'But to make better sense of that `json`, we need to format it in a way that
    pulls out the critical info (for example, severity, description, and location)
    while leaving out noise (for example, dependency graphs). Let''s use Python, which
    is great for string manipulation and general data munging, to write a script that
    formats that `json` into a plain text report. We''ll call the script `formatjs.py`
    to associate it with our other tool. The first thing we need to do is pull in
    `json` from `STDIN` and encode it as a Python data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Our goal is to create a table to display the data from the report, covering
    the `severity`, `summary`, `info`, and `file` attributes for each vulnerability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be using a simple Python table library, `tabulate` (which you can install
    via `pip install tabulate`). As per the `tabulate` docs, you can create a table
    using a nested list, where the inner list contains the values of an individual
    table row. We''re going to iterate over the different files analyzed, iterate
    over each vulnerability, and process their attributes into `row` lists that we''ll
    collect in our `rows` nested list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'That `format_bug()` function will just pull out the information we care about
    from the `vulnerability` dictionary and order the info properly in a list the
    function will return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we''ll sort the vulnerabilities by severity so that all the different
    types (high, medium, low, and so on) are grouped together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what it looks like all together, for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following is what it looks like when it''s run on the Terminal. I''m
    running the `scanjs.sh` script wrapper and then piping the data to `formatjs.py`.
    Here''s the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And here''s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4bd98ea7-6084-43ce-b026-ff4fea44c439.png)'
  prefs: []
  type: TYPE_IMG
- en: Downloading the JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There''s one more step before we can point this at a site – we need to download
    the actual JavaScript! Before analyzing the source code using our `scanjs` wrapper,
    we need to pull it from the target page. Pulling the code once in a single, discrete
    process (and from a single URL) means that, even as we develop more tooling around
    attack-surface reconnaissance, we can hook this script up to other services: it
    could pull the JavaScript from a URL supplied by a crawler, it could feed JavaScript
    or other assets into other analysis tools, or it could analyze other page metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So the simplest version of this script should be: the script takes a URL, looks
    at the source code for that page to find all JavaScript libraries, and then downloads
    those files to the specified location.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is grab the HTML from the URL of the page we''re
    inspecting. Let''s add some code that accepts the `url` and `directory` CLI arguments,
    and defines our target and where to store the downloaded JavaScript. Then, let''s
    use the `requests` library to pull the data and Beautiful Soup to make the HTML
    string a searchable object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to iterate over each script tag and use the `src` attribute data
    to download the file to a directory within our current root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'That `download_script()` function might not ring a bell because we haven''t
    written it yet. But that''s what we want – a function that takes the `src` attribute
    path, builds the link to the resource, and downloads it into the directory we''ve
    specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Each line is pretty direct. After the function definition, the HTTP address
    of the script is created using a Python ternary. If the `src` attribute starts
    with `/`, it's a relative path and can just be appended onto the hostname; if
    it doesn't, it must be a full/absolute link. Ternaries can be funky but also powerfully
    expressive once you get the hang of them.
  prefs: []
  type: TYPE_NORMAL
- en: The second line of the function creates the filename of the JavaScript library
    link by finding the character index of the last forward slash (`address.rfind("/")`)
    and the index of the `js` file extension, plus 2 to avoid slicing off the `js`
    part (`address.rfind("js")+2)`), and then uses the `[begin:end]` list-slicing
    syntax to create a new string from just the specified indices.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the third line, the script pulls data from the assembled address using
    `requests`, creates a new file using a context manager, and writes the page source
    code to `/directory/filename.js`. Now you have a location, the path passed in
    as an argument, and all of the JavaScript from a particular page saved inside
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: Putting It All Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So what does it look like when we put it all together? It''s simple – we can
    construct a one-liner to scan the JavaScript of a target site just by passing
    the right directory references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind we've already symlinked these scripts to our `/usr/local/bin` and
    changed their permissions using `chmod u+x` to make them executable and accessible
    from our path. With this command, we're telling our CL to download the JavaScript
    from `http://target.site` to the `sourcejs` directory, then scan that directory,
    create an `output.json` representation of the data, and finally format everything
    as a plain-text report.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a means of testing the command, I recently read a blog decrying the fact
    that jQuery, responsible for a large chunk of the web''s client-side code, was
    running an out-of-date WordPress version on [http://jquery.com/](http://jquery.com/),
    so I decided to see whether their JavaScript had any issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/b5b352c7-9a2d-4917-b3cf-6d27dc6f2207.png)'
  prefs: []
  type: TYPE_IMG
- en: The fact that [http://jquery.com/](http://jquery.com/) has a few issues is nothing
    huge, but still surprising! Known component vulnerabilities in JavaScript are
    a widespread issue, affecting a sizable portion of sites (different methodologies
    put the number of affected sites at between one-third and three-quarters of the
    entire web).
  prefs: []
  type: TYPE_NORMAL
- en: The Value Behind the Structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve developed several scripts to achieve a single goal. The exercise begs
    this question: why didn''t we write one program instead? We could''ve included
    all our steps (download the JSON, analyze it, print a report) in a Python or Shell
    script; wouldn''t that have been easier?'
  prefs: []
  type: TYPE_NORMAL
- en: 'But the advantage of our current setup is the modularity of the different pieces
    in the face of different workflows. For example, we might want to do all the steps
    at once, or we might just want a subset. If I''ve already downloaded all the JSON
    for a page and put it into a folder, scanned it, and created a report at `some-site-1-18-18.json`,
    then, when I visit the info, all I need is the ability to format the report from
    the raw `json`. I can achieve that with simple Unix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we might want to extend the workflow. Because the foundation is built on
    plain text, it''s easy to add new pieces. If our `mail` utility is set up, we
    can email ourselves the results of the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we could decide we only want to email ourselves the critical vulnerabilities.
    We could pull out the text we care about by using `ag`, a `grep`-like natural-language
    search utility known for its blazing speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We could substitute using email as a notification with using a script invoking
    the Slack API or another messaging service – the possibilities are endless. The
    benefit from using these short, stitched-together programs, built around common
    input and output, is that they can be rearranged and added to at will. They are
    the building blocks for a wider range of combinations and services. They are also,
    individually, very simple scripts, and because they're invoked through and pass
    information back to the command line, can be written in a variety of languages.
    I've used Python and Shell in this work, but could employ Ruby, Perl, Node, or
    another scripting language, with similar success.
  prefs: []
  type: TYPE_NORMAL
- en: There are obviously a lot of ways these short scripts could be improved. They
    currently have no input-verification, error-handling, logging, default arguments,
    or other features meant to make them cleaner and more reliable. But as we progress
    through the book, we'll be building on top of the utilities we're developing until
    they become more reliable, professional tools. And by adding new options, we'll
    show the value of a small, interlocking toolset.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered how to discover information about a site's attack surface
    using automated scanners, passive proxy interception, and command-line utilities
    wired into our own homebrew setup, and a couple of things in between. You learned
    some handy third-party tools, and also how to use them and others within the context
    of custom automation. Hopefully you've come away not only with a sense of the
    tactics (the code we've written), but of the strategy as well (the design behind
    it).
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What's a good tool for finding hidden directories and secret files on a site?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How and where can you find a map of the site's architecture? How can you create
    one if it's not already there?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you safely create a map of an application's attack surface without using
    scanners or automated scripts?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a common resource in Python for scraping websites?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some advantages to writing scripts according to the Unix philosophy
    (single-purpose, connectable, built around text)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a good resource for finding XSS submissions, SQLi snippets, and other
    fuzzing inputs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a good resource for discovering DNS info associated with a target?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find out more about some of the topics we have discussed in this chapter
    at:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SecLists**: [https://github.com/danielmiessler/SecLists](https://github.com/danielmiessler/SecLists)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measuring Relative Attack Surfaces**: [http://www.cs.cmu.edu/~wing/publications/Howard-Wing03.pdf](http://www.cs.cmu.edu/~wing/publications/Howard-Wing03.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XSScrapy**: [http://pentestools.com/xsscrapy-xsssqli-finder/](http://pentestools.com/xsscrapy-xsssqli-finder/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
