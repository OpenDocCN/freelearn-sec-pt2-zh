["```\npip install sklearn pandas\n```", "```\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndf = pd.read_csv(\"north_korea_missile_test_database.csv\")\ny = df[\"Missile Name\"]\nX = df.drop(\"Missile Name\", axis=1)\n```", "```\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=31\n)\n```", "```\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.25, random_state=31\n)\n```", "```\npip install sklearn pandas\n```", "```\nimport pandas as pd\n\ndata = pd.read_csv(\"file_pe_headers.csv\", sep=\",\")\nX = data.drop([\"Name\", \"Malware\"], axis=1).to_numpy()\n```", "```\nfrom sklearn.preprocessing import StandardScaler\n\nX_standardized = StandardScaler().fit_transform(X)\n```", "```\npip install sklearn pandas\n```", "```\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\ndata = pd.read_csv(\"file_pe_headers.csv\", sep=\",\")\nX = data.drop([\"Name\", \"Malware\"], axis=1).to_numpy()\n```", "```\nfrom sklearn.preprocessing import StandardScaler\n\nX_standardized = StandardScaler().fit_transform(X)\n```", "```\npca = PCA()\npca.fit_transform(X_standardized)\n```", "```\nprint(pca.explained_variance_ratio_)\n```", "```\nsum(pca.explained_variance_ratio_[0:40])\n```", "```\n0.9068522354673663\n```", "```\npip install markovify pandas\n```", "```\nimport markovify\nimport pandas as pd\n\ndf = pd.read_csv(\"airport_reviews.csv\")\n```", "```\n\"The airport is certainly tiny! ...\"\n```", "```\nfrom itertools import chain\n\nN = 100\nreview_subset = df[\"content\"][0:N]\ntext = \"\".join(chain.from_iterable(review_subset))\nmarkov_chain_model = markovify.Text(text)\n```", "```\nfor i in range(5):\n    print(markov_chain_model.make_sentence())\n```", "```\nOn the positive side it's a clean airport transfer from A to C gates and outgoing gates is truly enormous - but why when we arrived at about 7.30 am for our connecting flight to Venice on TAROM.\nThe only really bother: you may have to wait in a polite manner.\nWhy not have bus after a short wait to check-in there were a lots of shops and less seating.\nVery inefficient and hostile airport. This is one of the time easy to access at low price from city center by train.\nThe distance between the incoming gates and ending with dirty and always blocked by never ending roadworks.\n```", "```\nfor i in range(3):\n    print(markov_chain_model.make_short_sentence(140))\n```", "```\nHowever airport staff member told us that we were put on a connecting code share flight.\nConfusing in the check-in agent was friendly.\nI am definitely not keen on coming to the lack of staff . Lack of staff . Lack of staff at boarding pass at check-in.\n```", "```\nclass Text(object):\n\n    reject_pat = re.compile(r\"(^')|('$)|\\s'|'\\s|[\\\"(\\(\\)\\[\\])]\")\n\n    def __init__(self, input_text, state_size=2, chain=None, parsed_sentences=None, retain_original=True, well_formed=True, reject_reg=''):\n        \"\"\"\n        input_text: A string.\n        state_size: An integer, indicating the number of words in the model's state.\n        chain: A trained markovify.Chain instance for this text, if pre-processed.\n        parsed_sentences: A list of lists, where each outer list is a \"run\"\n              of the process (e.g. a single sentence), and each inner list\n              contains the steps (e.g. words) in the run. If you want to simulate\n              an infinite process, you can come very close by passing just one, very\n              long run.\n        retain_original: Indicates whether to keep the original corpus.\n        well_formed: Indicates whether sentences should be well-formed, preventing\n              unmatched quotes, parenthesis by default, or a custom regular expression\n              can be provided.\n        reject_reg: If well_formed is True, this can be provided to override the\n              standard rejection pattern.\n        \"\"\"\n```", "```\npip install sklearn plotly pandas\n```", "```\nimport pandas as pd\nimport plotly.express as px\n\ndf = pd.read_csv(\"file_pe_headers.csv\", sep=\",\")\nfig = px.scatter_3d(\n    df,\n    x=\"SuspiciousImportFunctions\",\n    y=\"SectionsLength\",\n    z=\"SuspiciousNameSection\",\n    color=\"Malware\",\n)\nfig.show()\n```", "```\ny = df[\"Malware\"]\nX = df.drop([\"Name\", \"Malware\"], axis=1).to_numpy()\n```", "```\nfrom sklearn.cluster import KMeans\n\nestimator = KMeans(n_clusters=len(set(y)))\nestimator.fit(X)\n```", "```\ny_pred = estimator.predict(X)\ndf[\"pred\"] = y_pred\ndf[\"pred\"] = df[\"pred\"].astype(\"category\")\n```", "```\nfig = px.scatter_3d(\n    df,\n    x=\"SuspiciousImportFunctions\",\n    y=\"SectionsLength\",\n    z=\"SuspiciousNameSection\",\n    color=\"pred\",\n)\nfig.show()\n```", "```\npip install sklearn xgboost pandas\n```", "```\nimport pandas as pd\n\ndf = pd.read_csv(\"file_pe_headers.csv\", sep=\",\")\ny = df[\"Malware\"]\nX = df.drop([\"Name\", \"Malware\"], axis=1).to_numpy()\n```", "```\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n```", "```\nfrom xgboost import XGBClassifier\n\nXGB_model_instance = XGBClassifier()\nXGB_model_instance.fit(X_train, y_train)\n```", "```\nfrom sklearn.metrics import accuracy_score\n\ny_test_pred = XGB_model_instance.predict(X_test)\naccuracy = accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100))\n```", "```\npip install matplotlib statsmodels scipy\n```", "```\nfrom random import random\n\ntime_series = [2 * x + random() for x in range(1, 100)]\n```", "```\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.plot(time_series)\nplt.show()\n```", "```\nfrom statsmodels.tsa.ar_model import AR\n\nmodel = AR(time_series)\nmodel_fit = model.fit()\ny = model_fit.predict(len(time_series), len(time_series))\n```", "```\nfrom statsmodels.tsa.arima_model import ARMA\n\nmodel = ARMA(time_series, order=(0, 1))\nmodel_fit = model.fit(disp=False)\ny = model_fit.predict(len(time_series), len(time_series))\n```", "```\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\n\nmodel = SimpleExpSmoothing(time_series)\nmodel_fit = model.fit()\ny = model_fit.predict(len(time_series), len(time_series))\n```", "```\npip install matplotlib pandas scipy\n```", "```\nimport numpy as np\nimport pandas as pd\n\nrandom_seed = np.random.RandomState(12)\n```", "```\nX_train = 0.5 * random_seed.randn(500, 2)\nX_train = np.r_[X_train + 3, X_train]\nX_train = pd.DataFrame(X_train, columns=[\"x\", \"y\"])\n```", "```\nX_test = 0.5 * random_seed.randn(500, 2)\nX_test = np.r_[X_test + 3, X_test]\nX_test = pd.DataFrame(X_test, columns=[\"x\", \"y\"])\n```", "```\nX_outliers = random_seed.uniform(low=-5, high=5, size=(50, 2))\nX_outliers = pd.DataFrame(X_outliers, columns=[\"x\", \"y\"])\n```", "```\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\np1 = plt.scatter(X_train.x, X_train.y, c=\"white\", s=50, edgecolor=\"black\")\np2 = plt.scatter(X_test.x, X_test.y, c=\"green\", s=50, edgecolor=\"black\")\np3 = plt.scatter(X_outliers.x, X_outliers.y, c=\"blue\", s=50, edgecolor=\"black\")\nplt.xlim((-6, 6))\nplt.ylim((-6, 6))\nplt.legend(\n    [p1, p2, p3],\n    [\"training set\", \"normal testing set\", \"anomalous testing set\"],\n    loc=\"lower right\",\n)\n\nplt.show()\n```", "```\nfrom sklearn.ensemble import IsolationForest\n\nclf = IsolationForest()\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\n```", "```\nX_outliers = X_outliers.assign(pred=y_pred_outliers)\nX_outliers.head()\n```", "```\np1 = plt.scatter(X_train.x, X_train.y, c=\"white\", s=50, edgecolor=\"black\")\np2 = plt.scatter(\n    X_outliers.loc[X_outliers.pred == -1, [\"x\"]],\n    X_outliers.loc[X_outliers.pred == -1, [\"y\"]],\n    c=\"blue\",\n    s=50,\n    edgecolor=\"black\",\n)\np3 = plt.scatter(\n    X_outliers.loc[X_outliers.pred == 1, [\"x\"]],\n    X_outliers.loc[X_outliers.pred == 1, [\"y\"]],\n    c=\"red\",\n    s=50,\n    edgecolor=\"black\",\n)\n\nplt.xlim((-6, 6))\nplt.ylim((-6, 6))\nplt.legend(\n    [p1, p2, p3],\n    [\"training observations\", \"detected outliers\", \"incorrectly labeled outliers\"],\n    loc=\"lower right\",\n)\n\nplt.show()\n```", "```\nX_test = X_test.assign(pred=y_pred_test)\nX_test.head()\n```", "```\np1 = plt.scatter(X_train.x, X_train.y, c=\"white\", s=50, edgecolor=\"black\")\np2 = plt.scatter(\n    X_test.loc[X_test.pred == 1, [\"x\"]],\n    X_test.loc[X_test.pred == 1, [\"y\"]],\n    c=\"blue\",\n    s=50,\n    edgecolor=\"black\",\n)\np3 = plt.scatter(\n    X_test.loc[X_test.pred == -1, [\"x\"]],\n    X_test.loc[X_test.pred == -1, [\"y\"]],\n    c=\"red\",\n    s=50,\n    edgecolor=\"black\",\n)\n\nplt.xlim((-6, 6))\nplt.ylim((-6, 6))\nplt.legend(\n    [p1, p2, p3],\n    [\n        \"training observations\",\n        \"correctly labeled test observations\",\n        \"incorrectly labeled test observations\",\n    ],\n    loc=\"lower right\",\n)\n\nplt.show()\n```", "```\npip install sklearn\n```", "```\nwith open(\"anonops_short.txt\", encoding=\"utf8\") as f:\n    anonops_chat_logs = f.readlines()\n```", "```\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nmy_vector = HashingVectorizer(input=\"content\", ngram_range=(1, 2))\nX_train_counts = my_vector.fit_transform(anonops_chat_logs,)\ntf_transformer = TfidfTransformer(use_idf=True,).fit(X_train_counts)\nX_train_tf = tf_transformer.transform(X_train_counts)\n```", "```\nX_train_tf\n\n<180830 x 1048576 sparse matrix of type <class 'numpy.float64'>' with 3158166 stored elements in Compressed Sparse Row format> print(X_train_tf)\n```", "```\npip install scikit-learn==0.20.3 xgboost scikit-optimize pandas\n```", "```\nfrom sklearn import datasets\n\nwine_dataset = datasets.load_wine()\nX = wine_dataset.data\ny = wine_dataset.target\n```", "```\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n```", "```\nfrom skopt import BayesSearchCV\n\nn_iterations = 50\n```", "```\nestimator = xgb.XGBClassifier(\n    n_jobs=-1,\n    objective=\"multi:softmax\",\n    eval_metric=\"merror\",\n    verbosity=0,\n    num_class=len(set(y)),\n)\n```", "```\nsearch_space = {\n    \"learning_rate\": (0.01, 1.0, \"log-uniform\"),\n    \"min_child_weight\": (0, 10),\n    \"max_depth\": (1, 50),\n    \"max_delta_step\": (0, 10),\n    \"subsample\": (0.01, 1.0, \"uniform\"),\n    \"colsample_bytree\": (0.01, 1.0, \"log-uniform\"),\n    \"colsample_bylevel\": (0.01, 1.0, \"log-uniform\"),\n    \"reg_lambda\": (1e-9, 1000, \"log-uniform\"),\n    \"reg_alpha\": (1e-9, 1.0, \"log-uniform\"),\n    \"gamma\": (1e-9, 0.5, \"log-uniform\"),\n    \"min_child_weight\": (0, 5),\n    \"n_estimators\": (5, 5000),\n    \"scale_pos_weight\": (1e-6, 500, \"log-uniform\"),\n}\n```", "```\ncv = StratifiedKFold(n_splits=3, shuffle=True)\n```", "```\nbayes_cv_tuner = BayesSearchCV(\n    estimator=estimator,\n    search_spaces=search_space,\n    scoring=\"accuracy\",\n    cv=cv,\n    n_jobs=-1,\n    n_iter=n_iterations,\n    verbose=0,\n    refit=True,\n)\n```", "```\nimport pandas as pd\nimport numpy as np\n\ndef print_status(optimal_result):\n    \"\"\"Shows the best parameters found and accuracy attained of the search so far.\"\"\"\n    models_tested = pd.DataFrame(bayes_cv_tuner.cv_results_)\n    best_parameters_so_far = pd.Series(bayes_cv_tuner.best_params_)\n    print(\n        \"Model #{}\\nBest accuracy so far: {}\\nBest parameters so far: {}\\n\".format(\n            len(models_tested),\n            np.round(bayes_cv_tuner.best_score_, 3),\n            bayes_cv_tuner.best_params_,\n        )\n    )\n\n    clf_type = bayes_cv_tuner.estimator.__class__.__name__\n    models_tested.to_csv(clf_type + \"_cv_results_summary.csv\")\n```", "```\nresult = bayes_cv_tuner.fit(X, y, callback=print_status)\n```", "```\nModel #1\n Best accuracy so far: 0.972\n Best parameters so far: {'colsample_bylevel': 0.019767840658391753, 'colsample_bytree': 0.5812505808116454, 'gamma': 1.7784704701058755e-05, 'learning_rate': 0.9050859661329937, 'max_delta_step': 3, 'max_depth': 42, 'min_child_weight': 1, 'n_estimators': 2334, 'reg_alpha': 0.02886003776717955, 'reg_lambda': 0.0008507166793122457, 'scale_pos_weight': 4.801764874750116e-05, 'subsample': 0.7188797743009225}\n\n Model #2\n Best accuracy so far: 0.972\n Best parameters so far: {'colsample_bylevel': 0.019767840658391753, 'colsample_bytree': 0.5812505808116454, 'gamma': 1.7784704701058755e-05, 'learning_rate': 0.9050859661329937, 'max_delta_step': 3, 'max_depth': 42, 'min_child_weight': 1, 'n_estimators': 2334, 'reg_alpha': 0.02886003776717955, 'reg_lambda': 0.0008507166793122457, 'scale_pos_weight': 4.801764874750116e-05, 'subsample': 0.7188797743009225}\n\n<snip>\n\nModel #50\n Best accuracy so far: 0.989\n Best parameters so far: {'colsample_bylevel': 0.013417868502558758, 'colsample_bytree': 0.463490250419848, 'gamma': 2.2823050161337873e-06, 'learning_rate': 0.34006478878384533, 'max_delta_step': 9, 'max_depth': 41, 'min_child_weight': 0, 'n_estimators': 1951, 'reg_alpha': 1.8321791726476395e-08, 'reg_lambda': 13.098734837402576, 'scale_pos_weight': 0.6188077759379964, 'subsample': 0.7970035272497132}\n```"]