- en: Machine Learning for Cybersecurity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inthis chapter, we will cover the fundamental techniques of machine learning.
    We will use these throughout the book to solve interesting cybersecurity problems.
    We will cover both foundational algorithms, such as clustering and gradient boosting
    trees, and solutions to common data challenges, such as imbalanced data and false-positive
    constraints. A machine learning practitioner in cybersecurity is in a unique and
    exciting position to leverage enormous amounts of data and create solutions in
    a constantly evolving landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Train-test-splitting your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing large data using **principal component analysis** (**PCA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating text using Markov chains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing clustering using scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an XGBoost classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing time series using statsmodels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection using Isolation Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) using hashing vectorizer and tf-idf
    with scikit-learn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning with scikit-optimize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markovify
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: statsmodels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The installation instructions and code can be found at [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter01](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter01).
    [](https://github.com/emmanueltsukerman/MLforCSCookbook)
  prefs: []
  type: TYPE_NORMAL
- en: Train-test-splitting your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, our goal is to create a program that is able to perform
    tasks it has never been explicitly taught to perform. The way we do that is to
    use data we have collected to *train* or *fit* a mathematical or statistical model.
    The data used to fit the model is referred to as *training data*. The resulting
    trained model is then used to predict future, previously-unseen data. In this
    way, the program is able to manage new situations without human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: One of the major challenges for a machine learning practitioner is the danger
    of *overfitting* – creating a model that performs well on the training data but
    is not able to generalize to new, previously-unseen data. In order to combat the
    problem of overfitting, machine learning practitioners set aside a portion of
    the data, called *test data*, and use it only to assess the performance of the
    trained model, as opposed to including it as part of the training dataset. This
    careful setting aside of testing sets is key to training classifiers in cybersecurity,
    where overfitting is an omnipresent danger. One small oversight, such as using
    only benign data from one locale, can lead to a poor classifier.
  prefs: []
  type: TYPE_NORMAL
- en: There are various other ways to validate model performance, such as cross-validation.
    For simplicity, we will focus mainly on train-test splitting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the scikit-learn and `pandas`
    packages in `pip`. The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In addition, we have included the `north_korea_missile_test_database.csv` dataset
    for use in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps demonstrate how to take a dataset, consisting of features
    `X` and labels `y`, and split these into a training and testing subset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the `train_test_split` module and the `pandas` library,
    and read your features into `X` and labels into `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, randomly split the dataset and its labels into a training set consisting
    80% of the size of the original dataset and a testing set 20% of the size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the `train_test_split` method once more, to obtain a validation set,
    `X_val` and `y_val`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We end up with a training set that's 60% of the size of the original data, a
    validation set of 20%, and a testing set of 20%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e07b58f8-015b-4ff0-b98f-3ada859d0136.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by reading in our dataset, consisting of historical and continuing
    missile experiments in North Korea. We aim to predict the type of missile based
    on remaining features, such as facility and time of launch. This concludes step
    1\. In step 2, we apply scikit-learn's `train_test_split` method to subdivide
    `X` and `y` into a training set, `X_train` and `y_train`, and also a testing set,
    `X_test` and `y_test`. The `test_size = 0.2` parameter means that the testing
    set consists of 20% of the original data, while the remainder is placed in the
    training set. The `random_state` parameter allows us to reproduce the same *randomly
    generated* split. Next, concerning step 3, it is important to note that, in applications,
    we often want to compare several different models. The danger of using the testing
    set to select the best model is that we may end up overfitting the testing set.
    This is similar to the statistical sin of data fishing. In order to combat this
    danger, we create an additional dataset, called the validation set. We train our
    models on the training set, use the validation set to compare them, and finally
    use the testing set to obtain an accurate indicator of the performance of the
    model we have chosen. So, in step 3, we choose our parameters so that, mathematically
    speaking, the end result consists of a training set of 60% of the original dataset,
    a validation set of 20%, and a testing set of 20%. Finally, we double-check our
    assumptions by employing the `len` function to compute the length of the arrays
    (step 4).
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many machine learning algorithms, performance is highly sensitive to the
    relative scale of features. For that reason, it is often important to *standardize *your
    features. To standardize a feature means to shift all of its values so that their
    mean = 0 and to scale them so that their variance = 1.
  prefs: []
  type: TYPE_NORMAL
- en: One instance when normalizing is useful is when featuring the PE header of a
    file. The PE header contains extremely large values (for example, the `SizeOfInitializedData`
    field) and also very small ones (for example, the number of sections). For certain
    ML models, such as neural networks, the large discrepancy in magnitude between
    features can reduce performance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `scikit-learn` and `pandas`
    packages in `pip`. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In addition, you will find a dataset named `file_pe_headers.csv` in the repository for
    this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we utilize scikit-learn''s `StandardScaler` method
    to standardize our data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the required libraries and gathering a dataset, `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Dataset `X` looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dd02cfd2-e3d5-411d-9192-89815651d799.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, standardize `X` using a `StandardScaler` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The standardized dataset looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d70b9c6a-48d9-459c-bb08-8f2c50afc2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by reading in our dataset (step 1), which consists of the PE header
    information for a collection of PE files. These vary greatly, with some columns
    reaching hundreds of thousands of files, and others staying in the single digits.
    Consequently, certain models, such as neural networks, will perform poorly on
    such unstandardized data. In step 2, we instantiate `StandardScaler()` and then
    apply it to rescale `X` using `.fit_transform(X)`. As a result, we obtained a
    rescaled dataset, whose columns (corresponding to features) have a mean of 0 and
    a variance of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing large data using principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose that you would like to build a predictor for an individual''s expected
    net fiscal worth at age 45\. There are a huge number of variables to be considered:
    IQ, current fiscal worth, marriage status, height, geographical location, health,
    education, career state, age, and many others you might come up with, such as
    number of LinkedIn connections or SAT scores.'
  prefs: []
  type: TYPE_NORMAL
- en: The trouble with having so many features is several-fold. First, the amount
    of data, which will incur high storage costs and computational time for your algorithm.
    Second, with a large feature space, it is critical to have a large amount of data
    for the model to be accurate. That's to say, it becomes harder to distinguish
    the signal from the noise. For these reasons, when dealing with high-dimensional
    data such as this, we often employ dimensionality reduction techniques, such as
    PCA. More information on the topic can be found at [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).
  prefs: []
  type: TYPE_NORMAL
- en: PCA allows us to take our features and return a smaller number of new features,
    formed from our original ones, with maximal explanatory power. In addition, since
    the new features are linear combinations of the old features, this allows us to
    anonymize our data, which is very handy when working with financial information,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preparation for this recipe consists of installing the scikit-learn and
    `pandas` packages in `pip`. The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In addition, we will be utilizing the same dataset, `malware_pe_headers.csv`,
    as in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll walk through a recipe showing how to use PCA on data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the necessary libraries and reading in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Standardize the dataset, as is necessary before applying PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a `PCA` instance and use it to reduce the dimensionality of our
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Assess the effectiveness of your dimensionality reduction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/31a84424-30b5-4159-83a6-ee81ae91fccb.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin by reading in our dataset and then standardizing it, as in the recipe
    on standardizing data (steps 1 and 2). (It is necessary to work with standardized
    data before applying PCA). We now instantiate a new PCA transformer instance,
    and use it to both learn the transformation (fit) and also apply the transform
    to the dataset, using `fit_transform` (step 3). In step 4, we analyze our transformation.
    In particular, note that the elements of `pca.explained_variance_ratio_` indicate
    how much of the variance is accounted for in each direction. The sum is 1, indicating
    that all the variance is accounted for if we consider the full space in which
    the data lives. However, just by taking the first few directions, we can account
    for a large portion of the variance, while limiting our dimensionality. In our
    example, the first 40 directions account for 90% of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This means that we can reduce our number of features to 40 (from 78) while preserving
    90% of the variance. The implications of this are that many of the features of
    the PE header are closely correlated, which is understandable, as they are not
    designed to be independent.
  prefs: []
  type: TYPE_NORMAL
- en: Generating text using Markov chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Markov chains are simple stochastic models in which a system can exist in a
    number of states. To know the probability distribution of where the system will
    be next, it suffices to know where it currently is. This is in contrast with a
    system in which the probability distribution of the subsequent state may depend
    on the past history of the system. This simplifying assumption allows Markov chains
    to be easily applied in many domains, surprisingly fruitfully.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will utilize Markov chains to generate fake reviews, which
    is useful for pen-testing a review system's spam detector. In a later recipe,
    you will upgrade the technology from Markov chains to RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `markovify` and `pandas`
    packages in `pip`. The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In addition, the directory in the repository for this chapter includes a CSV
    dataset, `airport_reviews.csv`, which should be placed alongside the code for
    the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to generate text using Markov chains by performing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the `markovify` library and a text file whose style we would
    like to imitate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As an illustration, I have chosen a collection of airport reviews as my text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, join the individual reviews into one large text string and build a Markov
    chain model using the airport review text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes, the library computes the transition word probabilities from
    the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate five sentences using the Markov chain model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are using airport reviews, we will have the following as the output
    after executing the previous code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Surprisingly realistic! Although the reviews would have to be filtered down
    to the best ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate `3` sentences with a length of no more than `140` characters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With our running example, we will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin the recipe by importing the Markovify library, a library for Markov
    chain computations, and reading in text, which will inform our Markov model (step
    1). In step 2, we create a Markov chain model using the text. The following is
    a relevant snippet from the text object''s initialization code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The most important parameter to understand is `state_size = 2`, which means
    that the Markov chains will be computing transitions between consecutive pairs
    of words. For more realistic sentences, this parameter can be increased, at the
    cost of making sentences appear less original. Next, we apply the Markov chains
    we have trained to generate a few example sentences (steps 3 and 4). We can see
    clearly that the Markov chains have captured the tone and style of the text. Finally,
    in step 5, we create a few `tweets` in the style of the airport reviews using
    our Markov chains.
  prefs: []
  type: TYPE_NORMAL
- en: Performing clustering using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Clustering** is a collection of unsupervised machine learning algorithms
    in which parts of the data are grouped based on similarity. For example, clusters
    might consist of data that is close together in n-dimensional Euclidean space.
    Clustering is useful in cybersecurity for distinguishing between normal and anomalous
    network activity, and for helping to classify malware into families.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `scikit-learn`, `pandas`,
    and `plotly` packages in `pip`. The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In addition, a dataset named `file_pe_header.csv` is provided in the repository
    for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will see a demonstration of how scikit-learn''s
    K-means clustering algorithm performs on a toy PE malware classification:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing and plotting the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d5b81582-3439-462a-b2fd-90c0cb515aa0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Extract the features and target labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, import scikit-learn''s clustering module and fit a K-means model with
    two clusters to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the cluster using our trained algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To see how the algorithm did, plot the algorithm''s clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/869390eb-e597-455c-801f-844687dc056e.png)'
  prefs: []
  type: TYPE_IMG
- en: The results are not perfect, but we can see that the clustering algorithm captured
    much of the structure in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by importing our dataset of PE header information from a collection
    of samples (step 1). This dataset consists of two classes of PE files: malware
    and benign. We then use plotly to create a nice-looking interactive 3D graph (step
    1). We proceed to prepare our dataset for machine learning. Specifically, in step
    2, we set `X` as the features and y as the classes of the dataset. Based on the
    fact that there are two classes, we aim to cluster the data into two groups that
    will match the sample classification. We utilize the K-means algorithm (step 3),
    about which you can find more information at: [https://en.wikipedia.org/wiki/K-means_clustering](https://en.wikipedia.org/wiki/K-means_clustering).
    With a thoroughly trained clustering algorithm, we are ready to predict on the
    testing set. We apply our clustering algorithm to predict to which cluster each
    of the samples should belong (step 4). Observing our results in step 5, we see
    that clustering has captured a lot of the underlying information, as it was able
    to fit the data well.'
  prefs: []
  type: TYPE_NORMAL
- en: Training an XGBoost classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosting is widely considered the most reliable and accurate algorithm
    for generic machine learning problems. We will utilize XGBoost to create malware
    detectors in future recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preparation for this recipe consists of installing the scikit-learn, `pandas`,
    and `xgboost` packages in `pip`. The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In addition, a dataset named `file_pe_header.csv` is provided in the repository
    for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will demonstrate how to instantiate, train, and
    test an XGBoost classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by reading in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, train-test-split a dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Create one instance of an XGBoost model and train it on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, assess its performance on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0697316c-043c-48d1-9a3f-4eee7867f25f.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by reading in our data (step 1). We then create a train-test split
    (step 2). We proceed to instantiate an XGBoost classifier with default parameters
    and fit it to our training set (step 3). Finally, in step 4, we use our XGBoost
    classifier to predict on the testing set. We then produce the measured accuracy
    of our XGBoost model's predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing time series using statsmodels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A time series is a series of values obtained at successive times. For example,
    the price of the stock market sampled every minute forms a time series. In cybersecurity,
    time series analysis can be very handy for predicting a cyberattack, such as an
    insider employee exfiltrating data, or a group of hackers colluding in preparation
    for their next hit.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at several techniques for making predictions using time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `matplotlib`, `statsmodels`,
    and `scipy` packages in `pip`. The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we demonstrate several methods for making predictions
    using time series data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by generating a time series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot your data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1d04a34d-bfe3-441d-b431-a92458457502.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is a large variety of techniques we can use to predict the consequent
    value of a time series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Autoregression** (**AR**):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '**Moving average** (**MA**):'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '**Simple exponential smoothing** (**SES**):'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting predictions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b35f14f9-0bb6-425d-bbc3-c32e1b3be02c.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first step, we generate a simple toy time series. The series consists
    of values on a line sprinkled with some added noise. Next, we plot our time series
    in step 2\. You can see that it is very close to a straight line and that a sensible
    prediction for the value of the time series at time ![](assets/bb728aba-f0a1-493f-a6d3-c8a52662bd65.png)
    is ![](assets/f8365b1e-32c3-4f44-8eba-a235d3e09405.png). To create a forecast
    of the value of the time series, we consider three different schemes (step 3)
    for predicting the future values of the time series. In an autoregressive model,
    the basic idea is that the value of the time series at time *t* is a linear function
    of the values of the time series at the previous times. More precisely, there
    are some constants, ![](assets/78f5feb2-6573-41aa-a3e9-ace1a6660c99.png), and
    a number, ![](assets/039e0190-78eb-4e7a-ad1c-e30f1f0e6473.png), such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e6d48f5b-64b0-444d-9eee-4a7e31a2b667.png)'
  prefs: []
  type: TYPE_IMG
- en: As a hypothetical example, ![](assets/dba51c21-ae4d-485d-b878-0f7f04b88c3c.png)
    may be *3*, meaning that the value of the time series can be easily computed from
    knowing its last *3* values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the moving-average model, the time series is modeled as fluctuating about
    a mean. More precisely, let ![](assets/176f6dfd-d61f-4a5d-a8bf-7d1447efd15e.png)
    be a sequence of i.i.d normal variables and let ![](assets/1b146731-b7ad-42a1-9901-80df10e48329.png)
    be a constant. Then, the time series is modeled by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c4c6352b-d94c-4d62-90fe-99519dbea008.png)'
  prefs: []
  type: TYPE_IMG
- en: For that reason, it performs poorly in predicting the noisy linear time series
    we have generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in simple exponential smoothing, we propose a smoothing parameter, ![](assets/fe2653e2-e6f5-4665-b6cb-cd0585722359.png).
    Then, our model''s estimate, ![](assets/0b46364e-73ee-499f-8a9a-4ab7013880b4.png),
    is computed from the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5590c81b-0f15-4fd5-bf0b-b9b4dc074eb2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](assets/962f5ffe-7714-4e71-94ea-36e8b861cdfe.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, we keep track of an estimate, ![](assets/bd3be59f-8df1-4602-9425-552842dbd7c4.png),
    and adjust it slightly using the current time series value, ![](assets/9fe675db-c5a5-4b58-9190-28199fb71327.png).
    How strongly the adjustment is made is regulated by the ![](assets/91976144-4a0c-46e8-96e0-9d996cca4b39.png) parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection with Isolation Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anomaly detection is the identification of events in a dataset that do not conform
    to the expected pattern. In applications, these events may be of critical importance.
    For instance, they may be occurrences of a network intrusion or of fraud. We will
    utilize Isolation Forest to detect such anomalies. Isolation Forest relies on
    the observation that it is easy to isolate an outlier, while more difficult to
    describe a normal data point.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preparation for this recipe consists of installing the `matplotlib`, `pandas`,
    and `scipy` packages in `pip`. The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the next steps, we demonstrate how to apply the Isolation Forest algorithm
    to detecting anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries and set a random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a set of normal observations, to be used as training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a testing set, also consisting of normal observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a set of outlier observations. These are generated from a different
    distribution than the normal observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the data we have generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/62e1ed8c-6c95-4fbc-bf15-8f69ca89cb4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now train an Isolation Forest model on our training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how the algorithm performs. Append the labels to `X_outliers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | x | y | pred |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 3.947504 | 2.891003 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.413976 | -2.025841 | -1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -2.644476 | -3.480783 | -1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -0.518212 | -3.386443 | -1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2.977669 | 2.215355 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s plot the Isolation Forest predictions on the outliers to see how many
    it caught:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/724ae20b-2325-4b22-8324-849089f48d95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s see how it performed on the normal testing data. Append the predicted
    label to `X_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | x | y | pred |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 3.944575 | 3.866919 | -1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2.984853 | 3.142150 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3.501735 | 2.168262 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2.906300 | 3.233826 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3.273225 | 3.261790 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Now let''s plot the results to see whether our classifier labeled the normal
    testing data correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/60609e2c-972c-4f26-ad58-4000736e81d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Evidently, our Isolation Forest model performed quite well at capturing the
    anomalous points. There were quite a few false negatives (instances where normal
    points were classified as outliers), but by tuning our model's parameters, we
    may be able to reduce these.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step involves simply loading the necessary libraries that will allow
    us to manipulate data quickly and easily. In steps 2 and 3, we generate a training
    and testing set consisting of normal observations. These have the same distributions.
    In step 4, on the other hand, we generate the remainder of our testing set by
    creating outliers. This anomalous dataset has a different distribution from the
    training data and the rest of the testing data. Plotting our data, we see that
    some outlier points look indistinguishable from normal points (step 5). This guarantees
    that our classifier will have a significant percentage of misclassifications,
    due to the nature of the data, and we must keep this in mind when evaluating its
    performance. In step 6, we fit an instance of Isolation Forest with default parameters
    to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the algorithm is fed no information about the anomalous data. We use
    our trained instance of Isolation Forest to predict whether the testing data is
    normal or anomalous, and similarly to predict whether the anomalous data is normal
    or anomalous. To examine how the algorithm performs, we append the predicted labels
    to `X_outliers` (step 7) and then plot the predictions of the Isolation Forest
    instance on the outliers (step 8). We see that it was able to capture most of
    the anomalies. Those that were incorrectly labeled were indistinguishable from
    normal observations. Next, in step 9, we append the predicted label to `X_test`
    in preparation for analysis and then plot the predictions of the Isolation Forest
    instance on the normal testing data (step 10). We see that it correctly labeled
    the majority of normal observations. At the same time, there was a significant
    number of incorrectly classified normal observations (shown in red).
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how many false alarms we are willing to tolerate, we may need to
    fine-tune our classifier to reduce the number of false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing using a hashing vectorizer and tf-idf with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often find in data science that the objects we wish to analyze are textual.
    For example, they might be tweets, articles, or network logs. Since our algorithms
    require numerical inputs, we must find a way to convert such text into numerical
    features. To this end, we utilize a sequence of techniques.
  prefs: []
  type: TYPE_NORMAL
- en: A *token* is a unit of text. For example, we may specify that our tokens are
    words, sentences, or characters. A count vectorizer takes textual input and then
    outputs a vector consisting of the counts of the textual tokens. A **hashing vectorizer**
    is a variation on the count vectorizer that sets out to be faster and more scalable,
    at the cost of interpretability and hashing collisions. Though it can be useful,
    just having the counts of the words appearing in a document corpus can be misleading.
    The reason is that, often, unimportant words, such as *the* and *a* (known as
    *stop words*) have a high frequency of occurrence, and hence little informative
    content. For reasons such as this, we often give words different weights to offset
    this. The main technique for doing so is **tf-idf**, which stands for **Term-Frequency,
    Inverse-Document-Frequency**. The main idea is that we account for the number
    of times a term occurs, but discount it by the number of documents it occurs in.
  prefs: []
  type: TYPE_NORMAL
- en: In cybersecurity, text data is omnipresent; event logs, conversational transcripts,
    and lists of function names are just a few examples. Consequently, it is essential
    to be able to work with such data, something you'll learn in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preparation for this recipe consists of installing the scikit-learn package
    in `pip`. The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In addition, a log file, `anonops_short.log`, consisting of an excerpt of conversations
    taking place on the IRC channel, `#Anonops`, is included in the repository for
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the next steps, we will convert a corpus of text data into numerical form,
    amenable to machine learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import a textual dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, count the words in the text using the hash vectorizer and then perform
    weighting using tf-idf:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The end result is a sparse matrix with each row being a vector representing
    one of the texts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/84d753ae-9201-4079-aaad-ead4c340cd34.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We started by loading in the #Anonops text dataset (step 1). The Anonops IRC
    channel has been affiliated with the Anonymous hacktivist group. In particular,
    chat participants have in the past planned and announced their future targets
    on Anonops. Consequently, a well-engineered ML system would be able to predict
    cyber attacks by training on such data. In step 2, we instantiated a hashing vectorizer.
    The hashing vectorizer gave us counts of the 1- and 2-grams in the text, in other
    words, singleton and consecutive pairs of words (tokens) in the articles. We then
    applied a tf-idf transformer to give appropriate weights to the counts that the
    hashing vectorizer gave us. Our final result is a large, sparse matrix representing
    the occurrences of 1- and 2-grams in the texts, weighted by importance. Finally,
    we examined the frontend of a sparse matrix representation of our featured data
    in Scipy.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning with scikit-optimize
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, a **hyperparameter** is a parameter whose value is set
    before the training process begins. For example, the choice of learning rate of
    a gradient boosting model and the size of the hidden layer of a multilayer perceptron,
    are both examples of hyperparameters. By contrast, the values of other parameters
    are derived via training. Hyperparameter selection is important because it can
    have a huge effect on the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: The most basic approach to hyperparameter tuning is called a **grid search**.
    In this method, you specify a range of potential values for each hyperparameter,
    and then try them all out, until you find the best combination. This brute-force
    approach is comprehensive but computationally intensive. More sophisticated methods
    exist. In this recipe, you will learn how to use *Bayesian optimization* over
    hyperparameters using `scikit-optimize`. In contrast to a basic grid search, in
    Bayesian optimization, not all parameter values are tried out, but rather a fixed
    number of parameter settings is sampled from specified distributions. More details
    can be found at [https://scikit-optimize.github.io/notebooks/bayesian-optimization.html](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preparation for this recipe consists of installing a specific version of
    `scikit-learn`, installing `xgboost`, and installing `scikit-optimize` in `pip`.
    The command for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will load the standard `wine` dataset and use Bayesian
    optimization to tune the hyperparameters of an XGBoost model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `wine` dataset from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Import XGBoost and stratified K-fold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `BayesSearchCV` from `scikit-optimize` and specify the number of parameter
    settings to test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify your estimator. In this case, we select XGBoost and set it to be able
    to perform multi-class classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a parameter search space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the type of cross-validation to perform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Define `BayesSearchCV` using the settings you have defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `callback` function to print out the progress of the parameter search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the parameter search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the following shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In steps 1 and 2, we import a standard dataset, the `wine` dataset, as well
    as the libraries needed for classification. A more interesting step follows, in
    which we specify how long we would like the hyperparameter search to be, in terms
    of a number of combinations of parameters to try. The longer the search, the better
    the results, at the risk of overfitting and extending the computational time.
    In step 4, we select XGBoost as the model, and then specify the number of classes,
    the type of problem, and the evaluation metric. This part will depend on the type
    of problem. For instance, for a regression problem, we might set `eval_metric
    = 'rmse'` and drop `num_class` together.
  prefs: []
  type: TYPE_NORMAL
- en: Other models than XGBoost can be selected with the hyperparameter optimizer
    as well. In the next step, (step 5), we specify a probability distribution over
    each parameter that we will be exploring. This is one of the advantages of using
    `BayesSearchCV` over a simple grid search, as it allows you to explore the parameter
    space more intelligently. Next, we specify our cross-validation scheme (step 6).
    Since we are performing a classification problem, it makes sense to specify a
    stratified fold. However, for a regression problem, `StratifiedKFold` should be
    replaced with `KFold`.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that a larger splitting number is preferred for the purpose of measuring
    results, though it will come at a computational price. In step 7, you can see
    additional settings that can be changed. In particular, `n_jobs` allows you to
    parallelize the task. The verbosity and the method used for scoring can be altered
    as well. To monitor the search process and the performance of our hyperparameter
    tuning, we define a callback function to print out the progress in step 8\. The
    results of the grid search are also saved in a CSV file. Finally, we run the hyperparameter
    search (step 9). The output allows us to observe the parameters and the performance
    of each iteration of the hyperparameter search.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will refrain from tuning the hyperparameters of classifiers.
    The reason is in part brevity, and in part because hyperparameter tuning here
    would be *premature optimization*, as there is no specified requirement or goal
    for the performance of the algorithm from the end user. Having seen how to perform
    it here, you can easily adapt this recipe to the application at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Another prominent library for hyperparameter tuning to keep in mind is `hyperopt`.
  prefs: []
  type: TYPE_NORMAL
