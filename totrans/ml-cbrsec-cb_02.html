<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Machine Learning-Based Malware Detection</h1>
                </header>
            
            <article>
                
<p>In this chapte<span>r, w</span><span>e begin to get serious about applying data science to cybersecurity. We will begin by learning how to perform static and dynamic analysis on samples. Building on this knowledge, we will learn how to featurize samples in order to construct a dataset with informative features. The highlight of the chapter is learning how to build a static malware detector using the featurization skills we have learned. Finally, you will learn how to tackle important machine learning challenges that occur in the domain of cybersecurity, such as class imbalance and <strong>false positive rate</strong> (<strong>FPR</strong>) constraints.</span></p>
<p class="mce-root">The chapter covers the following recipes:</p>
<ul>
<li>Malware static analysis</li>
<li>Malware dynamic analysis</li>
<li>Using machine learning to detect the file type</li>
<li>Measuring the similarity between two strings</li>
<li>Measuring the similarity between two files</li>
<li>Extracting N-grams</li>
<li>Selecting the best N-grams</li>
<li>Building a static malware detector</li>
<li>Tackling class imbalance</li>
<li>Handling type I and type II errors</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be using the following:</p>
<ul>
<li>YARA</li>
<li><kbd>pefile</kbd></li>
<li><kbd>PyGitHub</kbd></li>
<li>Cuckoo Sandbox</li>
<li><strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>)</li>
<li><kbd>imbalanced-learn</kbd></li>
</ul>
<p><span>The code and datasets can be found at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter02">https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter02</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Malware static analysis</h1>
                </header>
            
            <article>
                
<p class="mce-root">In static analysis, we examine a sample without executing it. The amount of information that can be obtained this way is large, ranging from something as simple as the name of the file to the more complex, such as specialized YARA signatures. We will be covering a selection of the large variety of features you could obtain by statically analyzing a sample. Despite its power and convenience, static analysis is no silver bullet, mainly because software can be obfuscated. For this reason, we will be employing dynamic analysis and other techniques in later chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing the hash of a sample</h1>
                </header>
            
            <article>
                
<p class="mce-root">Without delving into the intricacies of hashing, a hash is essentially a short and unique string signature. For example, we may hash the sequence of bytes of a file to obtain an essentially unique code for that file. This allows us to quickly compare two files to see whether they are identical.</p>
<p class="mce-root">There exist many hash procedures out there, so we will focus on the most important ones, namely, SHA256 and MD5. Note that MD5 is known to exhibit vulnerabilities due to hash collisions—instances where two different objects have the same hash and, therefore, should be used with caution. In this recipe, we take an executable file and compute its MD5 and SHA256 hashes.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of downloading a test file, which is the Python executable from <a href="https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe">https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe</a>.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we will see how to obtain the hash of a file:</p>
<ol>
<li class="mce-root">Begin by importing the libraries and selecting the desired file you wish to hash:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import sys<br/>import hashlib<br/><br/>filename = "python-3.7.2-amd64.exe"</pre>
<ol start="2">
<li class="mce-root">Instantiate the MD5 and SHA256 objects, and specify the size of the chunks we will be reading<span>:</span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root">BUF_SIZE = 65536<br/>md5 = hashlib.md5()<br/>sha256 = hashlib.sha256()</pre>
<ol start="3">
<li class="mce-root">We then read in the file in chunks of 64 KB and incrementally construct our hashes<span>:</span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root">with open(filename, "rb") as f:<br/>    while True:<br/>        data = f.read(BUF_SIZE)<br/>        if not data:<br/>            break<br/>        md5.update(data)<br/>        sha256.update(data)</pre>
<ol start="4">
<li class="mce-root">Finally, print out the resulting hashes<span>:</span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root">print("MD5: {0}".format(md5.hexdigest()))<br/>print("SHA256: {0}".format(sha256.hexdigest()))</pre>
<p style="padding-left: 60px" class="mce-root">This results in the following output:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>MD5: ff258093f0b3953c886192dec9f52763</strong><br/><strong>SHA256: 0fe2a696f5a3e481fed795ef6896ed99157bcef273ef3c4a96f2905cbdb3aa13</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section will explain the steps that have been provided in the previous section:</p>
<ul>
<li class="mce-root">In step 1, we import <kbd>hashlib</kbd>, a standard <span>Python </span>library for hash computation. We also specify the file we will be hashing—in this case, the file is <kbd>python-3.7.2-amd64.exe</kbd>.</li>
<li class="mce-root">In step 2, we instantiate an <kbd>md5</kbd> <span>object </span>and <span>an</span> <kbd>sha256</kbd> object and specify the size of the chunks we will be reading.</li>
<li>In step 3, we utilize the <kbd>.update(data)</kbd> method. This method allows us to compute the hash incrementally because it computes the hash of the concatenation. In other words, <kbd>hash.update(a)</kbd> followed by <kbd>hash.update(b)</kbd> is equivalent to <kbd>hash.update(a+b)</kbd>.</li>
<li>In step 4, we print out the hashes in hexadecimal digits.</li>
</ul>
<p>We can also verify that our computation is consistent with the hash calculations given by other sources, such as VirusTotal and the official <span>Python </span>website. The MD5 hash is displayed on the Python web page (<a href="https://www.python.org/downloads/release/python-372/">https://www.python.org/downloads/release/python-372/</a>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1082 image-border" src="assets/b284d160-12e6-4289-8b11-4c1c3f4bea6b.png" style="width:114.67em;height:3.50em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">The SHA256 hash is computed by uploading the file to VirusTotal (<a href="https://www.virustotal.com/gui/home">https://www.virustotal.com/gui/home</a>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1078 image-border" src="assets/12683aeb-2465-444a-8c15-582da8136d2d.png" style="width:39.67em;height:20.92em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">YARA</h1>
                </header>
            
            <article>
                
<p class="mce-root">YARA is a computer language that allows a security expert to conveniently specify a rule that will then be used to classify all samples matching the rule. A minimal rule consists of a name and a condition, for example, the following:</p>
<pre class="mce-root"> rule my_rule_name { condition: false }</pre>
<p class="mce-root">This rule will not match any file. Conversely, the following rule will match every sample:</p>
<pre class="mce-root"> Rule my_rule_name { condition: true }</pre>
<p class="mce-root">A more useful example will match any file over 100 KB:</p>
<pre class="mce-root"> Rule over_100kb { condition: filesize &gt; 100KB }</pre>
<p class="mce-root">Another example is checking whether a particular file is a PDF. To do so, we check if the magic numbers of the file correspond to the PDF. Magic numbers are a sequence of several bytes that occurs at the beginning of a file and indicates the type of file it is. In the case of a PDF, the sequence is <kbd>25 50 44 46</kbd>:</p>
<pre class="mce-root"> rule is_a_pdf {<br/>  <br/> strings:<br/>   $pdf_magic = {25 50 44 46}<br/>  <br/> condition:<br/>   $pdf_magic at 0<br/> }</pre>
<p class="mce-root">Now, let's see how to run our rules against files.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing YARA on your device. Instructions can be found at <a href="https://yara.readthedocs.io/en/stable/">https://yara.readthedocs.io/en/stable/</a>. For Windows, you need to download an executable file for YARA.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we show you how to create YARA rules and test them against a file:</p>
<ol>
<li class="mce-root"> Copy your rules, as seen here, into a text file and name it <kbd>rules.yara</kbd>:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"> rule is_a_pdf<br/> {<br/>        strings:<br/>               $pdf_magic = {25 50 44 46}<br/>        condition:<br/>               $pdf_magic at 0<br/> }<br/>  <br/> rule dummy_rule1<br/> {<br/>        condition:<br/>               false<br/> }<br/>  <br/> rule dummy_rule2<br/> {<br/>        condition:<br/>               true<br/> }</pre>
<ol start="2">
<li class="mce-root"> Next, select a file you would like to check your rules against. Call it <kbd>target_file</kbd>. In a terminal, execute <kbd>Yara rules.yara target_file</kbd> as follows:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>Yara rule.yara PythonBrochure</strong></pre>
<p style="padding-left: 60px" class="mce-root">The result should be as follows:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>is_a_pdf target_file</strong><br/><strong>dummy_rule2 target_rule</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p class="mce-root">As you can observe, in <em>Step 1</em>, we copied several YARA rules. The first rule checks the magic numbers of a file to see if they match those of a PDF. The other two rules are trivial rules—one that matches every file, and one that matches no file. Then, in <em>Step 2</em>, we used the YARA program to run the rules against the target file. We saw from a printout that the file matched some rules but not others, as expected from an effective YARA ruleset.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examining the PE header</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Portable executable</strong> (<strong>PE</strong>) files are a common Windows file type. PE files include the <kbd>.exe</kbd>, <kbd>.dll</kbd>, and <kbd>.sys</kbd> files. All PE files are distinguished by having a PE header, which is a header section of the code that instructs Windows on how to parse the subsequent code. The fields from the PE header are often used as features in the detection of malware. To easily extract the multitude of values of the PE header, we will utilize the <kbd>pefile</kbd> <span>Python </span>module. In this recipe, we will parse the PE header of a file, and then print out notable portions of it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>pefile</kbd> package in <kbd>pip</kbd>. In a terminal of your <span>Python </span>environment, run the following:</p>
</div>
<div class="a-b-r-x">
<pre><strong>pip install pefile</strong></pre>
<p>In addition, download the test file <span>Python </span>executable from <a href="https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe">https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe</a>.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we will parse the PE header of a file, and then print out notable portions of it:</p>
<ol>
<li class="mce-root">Import the PE file and use it to parse the PE header of your desired file:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import pefile<br/><br/>desired_file = "python-3.7.2-amd64.exe"<br/>pe = pefile.PE(desired_file)</pre>
<ol start="2">
<li class="mce-root">List the imports of the PE file:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">for entry in pe.DIRECTORY_ENTRY_IMPORT:<br/>    print(entry.dll)<br/>    for imp in entry.imports:<br/>        print("\t", hex(imp.address), imp.name)</pre>
<p style="padding-left: 60px" class="mce-root">A small portion of the output is shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1079 image-border" src="assets/ab7af973-2df6-4626-9f0c-071d90b86067.png" style="width:26.92em;height:20.42em;"/></p>
<ol start="3">
<li class="mce-root">List the sections of the PE file:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">for section in pe.sections:<br/>    print(<br/>        section.Name,<br/>        hex(section.VirtualAddress),<br/>        hex(section.Misc_VirtualSize),<br/>        section.SizeOfRawData,<br/>    )</pre>
<p style="padding-left: 60px" class="mce-root">The output of the previous code is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1080 image-border" src="assets/494806e9-3dab-4482-890a-e74329520e6f.png" style="width:19.58em;height:6.00em;"/></p>
<ol start="4">
<li class="mce-root">Print a full dump of the parsed information:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">print(pe.dump_info())</pre>
<p style="padding-left: 60px">A small portion of the output is displayed here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1081 image-border" src="assets/b237281e-fa9c-4bcc-ae48-672e34331f3f.png" style="width:37.42em;height:28.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">We began in <em>step 1</em> by importing the <kbd>pefile</kbd> library and specifying which file we will be analyzing. In this case, the file was <kbd>python-3.7.2-amd64.exe</kbd>, though it is just as easy to analyze any other PE file. We then continued on to examine the DLLs being imported by the file, in order to understand which methods the file may be using in <em>Step 2</em>. DLLs answer this question because a DLL is a library of code that other applications may call upon. For example, <kbd>USER32.dll</kbd> is a library that contains Windows USER, a component of the Microsoft Windows operating system that provides core functionality for building user interfaces. The component allows other applications to leverage the functionality for window management, message passing, input processing, and standard controls. Logically then, if we see that a file is importing a method such as <kbd>GetCursorPos</kbd>, then it is likely to be looking to determine the position of the cursor. Continuing in <em>step 3</em>, we printed out the sections of the PE file. These provide a logical and physical separation to the different parts of a program, and therefore offer the analyst valuable information about the program. Finally, we printed out all of the parsed PE header information from the file in preparation for later utilizing it for feature engineering (<em>Step 4</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Featurizing the PE header</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will extract features from the PE header to be used in building a <kbd>malware/benign</kbd> samples classifier. We will continue utilizing the <kbd>pefile</kbd> <span>Python </span>module.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>pefile</kbd> package in <kbd>pip</kbd>. The command is as follows:</p>
</div>
<div class="a-b-r-x">
<pre><strong>pip install pefile</strong></pre>
<p>In addition, benign and malicious files have been provided for you in the <kbd>PE Samples Dataset</kbd> folder in the root of the repository. Extract all archives named <kbd>Benign PE Samples*.7z</kbd> to a folder named <kbd>Benign PE Samples</kbd>. Extract all archives named <kbd>Malicious PE Samples*.7z</kbd> to a folder named <kbd>Malicious PE Samples</kbd>.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we will collect notable portions of the PE header:</p>
<ol>
<li class="mce-root">Import <kbd>pefile</kbd> and modules for enumerating our samples:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import pefile<br/>from os import listdir<br/>from os.path import isfile, join<br/><br/>directories = ["Benign PE Samples", "Malicious PE Samples"]</pre>
<ol start="2">
<li>We define a function to collect the names of the sections of a file and preprocess them for readability and normalization:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def get_section_names(pe):<br/>    """Gets a list of section names from a PE file."""<br/>    list_of_section_names = []<br/>    for sec in pe.sections:<br/>        normalized_name = sec.Name.decode().replace("\x00", "").lower()<br/>        list_of_section_names.append(normalized_name)<br/>    return list_of_section_names</pre>
<ol start="3">
<li>We define a convenience function to preprocess and standardize our imports:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def preprocess_imports(list_of_DLLs):<br/>    """Normalize the naming of the imports of a PE file."""<br/>    return [x.decode().split(".")[0].lower() for x in list_of_DLLs]</pre>
<ol start="4">
<li>We then define a function to collect the imports from a file using <kbd>pefile</kbd>:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def get_imports(pe):<br/>    """Get a list of the imports of a PE file."""<br/>    list_of_imports = []<br/>    for entry in pe.DIRECTORY_ENTRY_IMPORT:<br/>        list_of_imports.append(entry.dll)<br/>    return preprocess_imports(list_of_imports)</pre>
<ol start="5">
<li class="mce-root">Finally, we prepare to iterate through all of our files and create lists to store our features:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">imports_corpus = []<br/>num_sections = []<br/>section_names = []<br/>for dataset_path in directories:<br/>    samples = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]<br/>    for file in samples:<br/>        file_path = dataset_path + "/" + file<br/>        try:</pre>
<ol start="6">
<li class="mce-root">In addition to collecting the preceding features, we also collect the number of sections of a file:</li>
</ol>
<pre class="mce-root">            pe = pefile.PE(file_path)<br/>            imports = get_imports(pe)<br/>            n_sections = len(pe.sections)<br/>            sec_names = get_section_names(pe)<br/>            imports_corpus.append(imports)<br/>            num_sections.append(n_sections)<br/>            section_names.append(sec_names)</pre>
<ol start="7">
<li class="mce-root">In case a file's PE header cannot be parsed, we define a try-catch clause:</li>
</ol>
<pre class="mce-root">        except Exception as e:<br/>            print(e)<br/>            print("Unable to obtain imports from " + file_path)</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>As you can see, in <em>Step 1</em>, we imported the <kbd>pefile</kbd> module to enumerate the samples. Once that is done, we define the convenience function, as you can see in <em>Step 2</em>. The reason being that it often imports using varying cases (upper/lower). This causes the same import to appear as distinct imports.</p>
<p>After preprocessing the imports, we then define another function to collect all the imports of a file into a list. We will also define a function to collect the names of the sections of a file in order to standardize these names such as <kbd>.text</kbd>, <kbd>.rsrc</kbd>, and <kbd>.reloc</kbd> while containing distinct parts of the file (<em>Step 3</em>). The files are then enumerated in our folders and empty lists will be created to hold the features we will be extracting. The predefined functions will then collect the imports (<em>Step 4</em>), section names, and the number of sections of each file (<em>Steps 5</em> and <em>6</em>). Lastly, a try-catch clause will be defined in case a file's PE header cannot be parsed (<em>Step 7</em>). This can happen for many reasons. One reason being that the file is not actually a PE file. <span>Another reason is that its PE header is intentionally or unintentionally malformed.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Malware dynamic analysis</h1>
                </header>
            
            <article>
                
<p class="mce-root">Unlike static analysis, dynamic analysis is a malware analysis technique in which the expert executes the sample, and then studies the sample's behavior as it is being run. The main advantage of dynamic analysis over static is that it allows you to bypass obfuscation by simply observing how a sample behaves, rather than trying to decipher the sample's contents and behavior. Since malware is intrinsically unsafe, researchers resort to executing samples in a <strong>virtual machine</strong> (<strong>VM</strong>). This is called <strong>sandboxing</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">One of the most prominent tools for automating the analysis of samples in a VM is Cuckoo Sandbox. The initial installation of Cuckoo Sandbox is straightforward; simply run the following command:</p>
<pre class="mce-root"><strong>pip install -U cuckoo</strong></pre>
<p class="mce-root">You must make sure that you also have a VM that your machine can control. Configuring the sandbox can be a challenge, but instructions are available at <a href="https://cuckoo.sh/docs/">https://cuckoo.sh/docs/</a>.</p>
<p class="mce-root">We show now how to utilize Cuckoo Sandbox to obtain a dynamic analysis of a sample.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Once your Cuckoo Sandbox is set up, and has a web interface running, follow these steps to gather runtime information about a sample:</p>
<ol>
<li class="mce-root CDPAlignLeft CDPAlign">Open up your web interface (the default location is <kbd>127.0.0.1:8000</kbd>), click <strong>SUBMIT A FILE FOR ANALYSIS</strong>, and select the sample you wish to analyze:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1083 image-border" src="assets/291e48cd-c544-4e1e-a6a6-db1011cdb26d.png" style="width:125.58em;height:81.83em;"/></p>
<ol start="2">
<li class="mce-root CDPAlignLeft CDPAlign">The following screen will appear automatically. In it, select the type of analysis you wish to perform on your sample:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1084 image-border" src="assets/e3968f76-8e95-4f15-b910-28254321e6da.png" style="width:50.75em;height:34.92em;"/></p>
<ol start="3">
<li class="mce-root CDPAlignLeft CDPAlign">Click <strong>Analyze</strong> to analyze the sample in your sandbox. The result should look as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1085 image-border" src="assets/8f250a32-58bd-4c91-82bf-2c007cfaee3d.png" style="width:146.42em;height:70.08em;"/></p>
<ol start="4">
<li class="mce-root CDPAlignLeft CDPAlign">Next, open up the report for the sample you have analyzed:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1086 image-border" src="assets/9bb0c653-8e33-4dee-9e86-434341350844.png" style="width:150.08em;height:77.17em;"/></p>
<ol start="5">
<li class="mce-root CDPAlignLeft CDPAlign">Select the <strong>Behavioral Analysis</strong> tab:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1179 image-border" src="assets/01b72b50-2410-48d1-8f35-fb5aeb472c81.png" style="width:128.83em;height:75.08em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">The displayed sequence of API calls, registry key changes, and other events can all be used as input to a classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">At a conceptual level, obtaining dynamic analysis results consists of running samples in environments that allow the analyst to collect runtime information. Cuckoo Sandbox is a flexible framework with prebuilt modules to do just that. We began our recipe for using Cuckoo Sandbox by opening up the web portal (<em>Step 1</em>). A <strong>command-line interface</strong> (<strong>CLI</strong>) exists as well. We proceeded to submit a sample and select the type of analysis we wished to perform (<em>Steps 2</em> and <em>3</em>). These steps, too, can be performed through the Cuckoo CLI. We proceeded to examine the analysis report (<em>Step 4</em>). You can see at this stage how the many modules of Cuckoo Sandbox reflect in the final analysis output. For instance, if a module for capturing traffic is installed and used, then the report will contain the data captured in the network tab. We proceeded to focus our view of the analysis to behavioral analysis (<em>Step 5</em>), and in particular to observe the sequence of API calls. API calls are basically operations performed by the OS. This sequence makes up a fantastic feature set that we will utilize to detect malware in future recipes. Finally, note that in a production environment, it may make sense to create a custom-made sandbox with custom modules for data collection, as well as equip it with anti-VM detection software to facilitate successful analysis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using machine learning to detect the file type</h1>
                </header>
            
            <article>
                
<p class="mce-root">One of the techniques hackers use to sneak their malicious files into security systems is to obfuscate their file types. For example, a (malicious) PowerShell script is expected to have an extension, <kbd>.ps1</kbd>. A system administrator can aim to combat the execution of all PowerShell scripts on a system by preventing the execution of all files with the <kbd>.ps1</kbd> extension. However, the mischievous hacker can remove or change the extension, rendering the file's identity a mystery. Only by examining the contents of the file can it then be distinguished from an ordinary text file. For practical reasons, it is not possible for humans to examine all text files on a system. Consequently, it is expedient to resort to automated methods. In this chapter, we will demonstrate how you can use machine learning to detect the file type of an unknown file. Our first step is to curate a dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scraping GitHub for files of a specific type</h1>
                </header>
            
            <article>
                
<p>To curate a dataset, we will scrape GitHub for the specific file types we are interested in. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>PyGitHub</kbd> package in <kbd>pip</kbd> by running the following command:</p>
</div>
<div class="a-b-r-x">
<pre><strong>pip install PyGitHub</strong></pre>
<p>In addition, you will need GitHub account credentials.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we curate a dataset and then use it to create a classifier to determine the file type. For demonstration purposes, we show how to obtain a collection of PowerShell scripts, Python scripts, and JavaScript files by scraping GitHub. A collection of samples obtained in this way can be found in the accompanying repository as <kbd>PowerShellSamples.7z</kbd>, <kbd>PythonSamples.7z</kbd>, and <kbd>JavascriptSamples.7z</kbd>. First, we will write the code for the JavaScript scraper:</p>
<ol>
<li class="mce-root">Begin by importing the <kbd>PyGitHub</kbd> library in order to be able to call the GitHub API. We also import the <kbd>base64</kbd> module for decoding the <kbd>base64</kbd> encoded files:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import os<br/>from github import Github<br/>import base64</pre>
<ol start="2">
<li class="mce-root">We must supply our credentials, and then specify a query—in this case, for JavaScript—to select our repositories:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">username = "your_github_username"<br/>password = "your_password"<br/>target_dir = "/path/to/JavascriptSamples/"<br/>g = Github(username, password)<br/>repositories = g.search_repositories(query='language:javascript')<br/>n = 5<br/>i = 0</pre>
<ol start="3">
<li class="mce-root">We loop over the repositories matching our criteria:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">for repo in repositories:<br/>    repo_name = repo.name<br/>    target_dir_of_repo = target_dir+"\\"+repo_name<br/>    print(repo_name)<br/>    try:</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li class="mce-root">We create a directory for each repository matching our search criteria, and then read in its contents:</li>
</ol>
<pre class="mce-root">        os.mkdir(target_dir_of_repo)<br/>        i += 1<br/>        contents = repo.get_contents("")</pre>
<ol start="5">
<li>We add all directories of the repository to a queue in order to list all of the files contained within the directories:</li>
</ol>
<pre class="mce-root">        while len(contents) &gt; 1:<br/>            file_content = contents.pop(0)<br/>            if file_content.type == "dir":<br/>                contents.extend(repo.get_contents(file_content.path))<br/>            else:</pre>
<ol start="6">
<li class="mce-root">If we find a non-directory file, we check whether its extension is <kbd>.js</kbd>:</li>
</ol>
<pre class="mce-root">                st = str(file_content)<br/>                filename = st.split("\"")[1].split("\"")[0]<br/>                extension = filename.split(".")[-1]<br/>                if extension == "js":</pre>
<ol start="7">
<li class="mce-root">If the extension is <kbd>.js</kbd>, we write out a copy of the file:</li>
</ol>
<pre class="mce-root">                    file_contents = repo.get_contents(file_content.path)<br/>                    file_data = base64.b64decode(file_contents.content)<br/>                    filename = filename.split("/")[-1]<br/>                    file_out = open(target_dir_of_repo+"/"+filename, "wb")<br/>                    file_out.write(file_data)<br/>      except:<br/>        pass<br/>    if i==n:<br/>        break</pre>
<ol start="8">
<li class="mce-root">Once finished, it is convenient to move all the JavaScript files into one folder.<br/>
<br/>
To obtain PowerShell samples, run the same code, changing the following:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">target_dir = "/path/to/JavascriptSamples/"<br/>repositories = g.search_repositories(query='language:javascript')</pre>
<p style="padding-left: 60px" class="mce-root">To the following:</p>
<pre style="padding-left: 60px" class="mce-root">target_dir = "/path/to/PowerShellSamples/"<br/>repositories = g.search_repositories(query='language:powershell').</pre>
<p style="padding-left: 60px" class="mce-root">Similarly, for <span>Python </span>files, we do the following:</p>
<pre style="padding-left: 60px" class="mce-root">target_dir = "/path/to/PythonSamples/"<br/>repositories = g.search_repositories(query='language:python').</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">We start by importing the <kbd>PyGitHub</kbd> library in <em>Step 1</em> in order to be able to conveniently call the GitHub APIs. These will allow us to scrape and explore the universe of repositories. We also import the <kbd>base64</kbd> module for decoding the <kbd>base64</kbd> encoded files that we will be downloading from GitHub. <span>Note that there is a rate limit on the number of API calls a generic user can make to GitHub. For this reason, you will find that if you attempt to download too many files in a short duration, your script will not get all of the files. Our next step is to</span> supply our credentials to GitHub (<em>step 2</em>), and specify that we are looking for repositories with JavaScript, using the <kbd>query='language:javascript'</kbd> command. We enumerate such repositories matching our criteria of being associated with JavaScript, and if they do, we search through these for files ending with <kbd>.js</kbd> and create local copies (steps 3 to 6). Since these files are encoded in <kbd>base64</kbd>, we make sure to decode them to plaintext in step 7. Finally, we show you how to adjust the script in order to scrape other file types, such as Python and PowerShell (Step 8).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying files by type</h1>
                </header>
            
            <article>
                
<p>Now that we have a dataset, we would like to train a classifier. Since the files in question are scripts, we approach the problem as an NLP problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>scikit-learn</kbd> package in <kbd>pip</kbd>. The instructions are as follows:</p>
</div>
<div class="a-b-r-x">
<pre><strong>pip install sklearn</strong></pre>
<p>In addition, we have supplied you with samples of each file type in the <kbd>JavascriptSamples.7z</kbd>, <kbd>PythonSamples.7z</kbd>, and <kbd>PowerShellSamples.7z</kbd> archives, in case you would like to supplement your own dataset. Extract these into separate folders for the following recipe.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The code for the following can be found on <a href="https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/blob/master/Chapter02/Classifying%20Files%20by%20Type/File%20Type%20Classifier.ipynb">https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/blob/master/Chapter02/Classifying%20Files%20by%20Type/File%20Type%20Classifier.ipynb</a>. We build a classifier using this data to predict files as JavaScript, Python, or PowerShell:</p>
<ol>
<li class="mce-root">Begin by importing the necessary libraries and specifying the paths of the samples we will be using to train and test:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import os<br/>from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score, confusion_matrix<br/>from sklearn.pipeline import Pipeline<br/>  <br/>javascript_path = "/path/to/JavascriptSamples/"<br/>python_path = "/path/to/PythonSamples/"<br/>powershell_path = "/path/to/PowerShellSamples/"</pre>
<ol start="2">
<li class="mce-root">Next, we read in all of the file types. We also create an array of labels with -1, 0, and 1 representing the JavaScript, Python, and PowerShell scripts, respectively:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">corpus = []<br/>labels = []<br/>file_types_and_labels = [(javascript_path, -1), (python_path, 0), (powershell_path, 1)]<br/>for files_path, label in file_types_and_labels:<br/>    files = os.listdir(files_path)<br/>    for file in files:<br/>        file_path = files_path + "/" + file<br/>        try:<br/>            with open(file_path, "r") as myfile:<br/>                data = myfile.read().replace("\n", "")<br/>        except:<br/>            pass<br/>        data = str(data)<br/>        corpus.append(data)<br/>        labels.append(label)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="3">
<li class="mce-root">We go on to create a train-test split and a pipeline that will perform basic NLP on the files, followed by a random forest classifier:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">X_train, X_test, y_train, y_test = train_test_split(<br/>    corpus, labels, test_size=0.33, random_state=11<br/>)<br/>text_clf = Pipeline(<br/>    [<br/>        ("vect", HashingVectorizer(input="content", ngram_range=(1, 3))),<br/>        ("tfidf", TfidfTransformer(use_idf=True,)),<br/>        ("rf", RandomForestClassifier(class_weight="balanced")),<br/>    ]<br/>)</pre>
<ol start="4">
<li class="mce-root">We fit the pipeline to the training data, and then use it to predict on the testing data. Finally, we print out the accuracy and the confusion matrix:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">text_clf.fit(X_train, y_train)<br/>y_test_pred = text_clf.predict(X_test)<br/>print(accuracy_score(y_test, y_test_pred))<br/>print(confusion_matrix(y_test, y_test_pred))</pre>
<p style="padding-left: 60px" class="mce-root CDPAlignLeft CDPAlign">This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1088 image-border" src="assets/cb75e173-58ff-41db-a345-5959d5436b5d.png" style="width:6.42em;height:3.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Leveraging the dataset we built up in the <em>Scraping GitHub for files of a specific type</em> recipe, we place files in different directories, based on their file type, and then specify the paths in preparation for building our classifier (step 1). The code for this recipe assumes that the <kbd>"JavascriptSamples"</kbd> directory and others contain the samples, and have no subdirectories. We read in all files into a corpus, and record their labels (step 2). We train-test split the data and prepare a pipeline that will perform basic NLP on the files, followed by a random forest classifier (step 3). The choice of classifier here is meant for illustrative purposes, rather than to imply a best choice of classifier for this type of data. Finally, we perform the basic, but important, steps in the process of creating a machine learning classifier, consisting of fitting the pipeline to the training data and then assessing its performance on the testing set by measuring its accuracy and confusion matrix (step 4).</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring the similarity between two strings</h1>
                </header>
            
            <article>
                
<p class="mce-root">To check whether two files are identical, we utilize standard cryptographic hash functions, such as SHA256 and MD5. However, at times, we would like to also know to what extent two files are similar. For that purpose, we utilize similarity hashing algorithms. The one we will be demonstrating here is <kbd>ssdeep</kbd>.</p>
<p class="mce-root">First, let's see how to use <kbd>ssdeep</kbd> to compare two strings. This can be useful to detect tampering in a text or script and also plagiarism.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>ssdeep</kbd> package in <kbd>pip</kbd>. The installation is a little tricky and does not always work on Windows. Instructions can be found at <a href="https://python-ssdeep.readthedocs.io/en/latest/installation.html">https://python-ssdeep.readthedocs.io/en/latest/installation.html.</a></p>
<p>If you only have a Windows machine and installing <kbd>ssdeep</kbd> <span>does not work, then one possible solution is to run <kbd>ssdeep</kbd> on an Ubuntu VM, and then install it in <kbd>pip</kbd>, using the following command: </span></p>
<pre><strong>pip install ssdeep</strong></pre></div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li class="mce-root"> Begin by importing the <kbd>ssdeep</kbd> library and creating three strings:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import ssdeep<br/><br/>str1 = "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua."<br/>str2 = "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore Magna aliqua."<br/>str3 = "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore aliqua."<br/>str4 = "Something completely different from the other strings."</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li class="mce-root">Hash the strings:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">hash1 = ssdeep.hash(str1)<br/>hash2 = ssdeep.hash(str2)<br/>hash3 = ssdeep.hash(str3)<br/>hash4 = ssdeep.hash(str4)</pre>
<div class="mce-root packt_quote packt_infobox"><span>As a reference,</span><br/>
hash1 is <kbd>u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BSnJi:f4kPvtHMCMubyFtQ'</kbd>,<br/>
hash2 is <kbd>u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BS+EFECJi:f4kPvtHMCMubyFIsJQ'</kbd>,<br/>
hash3 is <kbd>u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BS6:f4kPvtHMCMubyF0'</kbd>, and<br/>
hash4 is <kbd>u'3:60QKZ+4CDTfDaRFKYLVL:ywKDC2mVL'</kbd>.</div>
<ol start="3">
<li class="mce-root">Next, we see what kind of similarity scores the strings have:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">ssdeep.compare(hash1, hash1)<br/>ssdeep.compare(hash1, hash2)<br/>ssdeep.compare(hash1, hash3)<br/>ssdeep.compare(hash1, hash4)<br/> </pre>
<p style="padding-left: 60px" class="mce-root">The numerical results are as follows:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>100</strong><br/><strong>39</strong><br/><strong>37</strong><br/><strong>0</strong></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The basic idea behind <kbd>ssdeep</kbd> is to combine a number of traditional hashes whose boundaries are determined by the context of the input. This collection of hashes can then be used to identify modified versions of known files even when they have been modified by insertion, modification, or deletion.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For our recipe, we began by creating a set of four test strings meant as a toy example to illustrate how changes in a string will affect its similarity measures (step 1). The first, <kbd>str1</kbd>, is simply the first sentence of Lorem Ipsum. The second string, <kbd>str2</kbd>, differs in the capitalization of <kbd>m</kbd> in magna. The third string, <kbd>str3</kbd>, is missing the word magna altogether. Finally, the fourth string is an entirely different string. Our next step, step 2, is to hash the strings using the similarity hashing <kbd>ssdeep</kbd> library. Observe that similar strings have visibly similar similarity hashes. This should be contrasted with traditional hashes, in which even a small alteration produces a completely different hash. Next, we derive the similarity score between the various strings using <kbd>ssdeep</kbd> (step 3). In particular, observe that the <kbd>ssdeep</kbd> similarity score between two strings is an integer ranging between 0 and 100, with 100 being identical and 0 being dissimilar. Two identical strings will have a similarity score of 100. Changing the case of one letter in our string lowered the similarity score significantly to 39 because the strings are relatively short. Removing a word lowered it to 37. And two completely different strings had a similarity of 0.</p>
<p>Although other, in some cases better, fuzzy hashes are available, <kbd>ssdeep</kbd> is still a primary choice because of its speed and being a de facto standard.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring the similarity between two files</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, we are going to see how to apply <kbd>ssdeep</kbd> to measure the similarity between two binary files. The applications of this concept are many, but one in particular is using the similarity measure as a distance in clustering.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>ssdeep</kbd> package in <kbd>pip</kbd>. The installation is a little tricky and does not always work on Windows. Instructions can be found at <a href="https://python-ssdeep.readthedocs.io/en/latest/installation.html">https://python-ssdeep.readthedocs.io/en/latest/installation.html</a>.</p>
<p>If you only have a Windows machine and it does not work, then one possible solution is to run <kbd>ssdeep</kbd> on an Ubuntu VM by installing <kbd>pip</kbd> with this command:</p>
<pre><strong>pip install ssdeep</strong></pre>
<p><span>In addition, download a test file such as the Python executable from <a href="https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe">https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe</a>.</span></p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following recipe, we tamper with a binary file. We then compare it to the original to see that <kbd>ssdeep</kbd> determines that the two files are highly similar but not identical:</p>
<ol>
<li class="mce-root">First, we download the latest version of Python, <kbd>python-3.7.2-amd64.exe</kbd>. I am going to create a copy, rename it <kbd>python-3.7.2-amd64-fake.exe</kbd>, and add a null byte at the end:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>truncate -s +1 python-3.7.2-amd64-fake.exe</strong></pre>
<ol start="2">
<li class="mce-root">Using <kbd>hexdump</kbd>, I can verify that the operation was successful by looking at the file before and after:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>hexdump -C python-3.7.2-amd64.exe |tail -5</strong></pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px" class="mce-root"><br/><strong>018ee0f0  e3 af d6 e9 05 3f b7 15  a1 c7 2a 5f b6 ae 71 1f  |.....?....*_..q.|</strong><br/><strong>018ee100  6f 46 62 1c 4f 74 f5 f5  a1 e6 91 b7 fe 90 06 3e  |oFb.Ot.........&gt;|</strong><br/><strong>018ee110  de 57 a6 e1 83 4c 13 0d  b1 4a 3d e5 04 82 5e 35  |.W...L...J=...^5|</strong><br/><strong>018ee120  ff b2 e8 60 2d e0 db 24  c1 3d 8b 47 b3 00 00 00  |...`-..$.=.G....|</strong><br/><br/></pre>
<p style="padding-left: 60px">The same can be verified with a second file using the following command:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>hexdump -C python-3.7.2-amd64-fake.exe |tail -5</strong></pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>018ee100  6f 46 62 1c 4f 74 f5 f5  a1 e6 91 b7 fe 90 06 3e  |oFb.Ot.........&gt;|</strong><br/><strong>018ee110  de 57 a6 e1 83 4c 13 0d  b1 4a 3d e5 04 82 5e 35  |.W...L...J=...^5|</strong><br/><strong>018ee120  ff b2 e8 60 2d e0 db 24  c1 3d 8b 47 b3 00 00 00  |...`-..$.=.G....|</strong><br/><strong>018ee130  00                                                |.|</strong><br/><strong>018ee131</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li class="mce-root">Now, I will hash the two files using <kbd>ssdeep</kbd> and compare the result:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import ssdeep<br/><br/>hash1 = ssdeep.hash_from_file("python-3.7.2-amd64.exe")<br/>hash2 = ssdeep.hash_from_file("python-3.7.2-amd64-fake.exe")<br/>ssdeep.compare(hash1, hash2)<br/> </pre>
<p class="mce-root">The output to the preceding code is <kbd>99</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">This scenario simulates tampering with a file and then utilizing similarity hashing to detect the existence of tampering, as well as measuring the size of the delta. We begin with a vanilla Python executable and then tamper with it by adding a null byte at the end (step 1). In real life, a hacker may take a legitimate program and insert malicious code into the sample. We double-checked that the tempering was successful and examined its nature using a <kbd>hexdump</kbd> in step 2. We then ran a similarity computation using similarity hashing on the original and tempered file, to observe that a minor alteration took place (step 3). Utilizing only standard hashing, we would have no idea how the two files are related, other than to conclude that they are not the same file. Knowing how to compare files allows us to cluster malware and benign files in machine learning algorithms, as well as group them into families.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting N-grams</h1>
                </header>
            
            <article>
                
<p class="mce-root">In standard quantitative analysis of text, N-grams are sequences of N tokens (for example, words or characters). For instance, given the text <em>The quick brown fox jumped over the lazy dog,</em> if our tokens are words, then the 1-grams are <em>the</em>, <em>quick</em>, <em>brown</em>, <em>fox</em>, <em>jumped</em>, <em>over</em>, <em>the</em>, <em>lazy</em>, and <em>dog</em>. The 2-grams are <em>the quick</em>, <em>quick brown</em>, <em>brown fox</em>, and so on. The 3-grams are <em>the quick brown</em>, <em>quick brown fox</em>, <em>brown fox jumped</em>, and so on. Just like the local statistics of the text allowed us to build a Markov chain to perform statistical predictions and text generation from a corpus, N-grams allow us to model the local statistical properties of our corpus. Our ultimate goal is to utilize the counts of N-grams to help us predict whether a sample is malicious or benign. In this recipe, we demonstrate how to extract N-gram counts from a sample.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>nltk</kbd> package in <kbd>pip</kbd>. The instructions are as follows:</p>
<pre><strong>pip install nltk</strong></pre>
<p>In addition, download a test file, such as the Python executable from <a href="https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe">https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe</a>.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we will enumerate all the 4-grams of a sample file and select the 50 most frequent ones:</p>
<ol>
<li class="mce-root">We begin by importing the <kbd>collections</kbd> library to facilitate counting and the <kbd>ngrams</kbd> library from <kbd>nltk</kbd> to ease extraction of N-grams:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import collections<br/>from nltk import ngrams</pre>
<ol start="2">
<li class="mce-root">We specify which file we would like to analyze:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">file_to_analyze = "python-3.7.2-amd64.exe"</pre>
<ol start="3">
<li>We define a convenience function to read in a file's bytes:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def read_file(file_path):<br/>    """Reads in the binary sequence of a binary file."""<br/>    with open(file_path, "rb") as binary_file:<br/>        data = binary_file.read()<br/>    return data</pre>
<ol start="4">
<li>We write a convenience function to take a byte sequence and obtain N-grams:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def byte_sequence_to_Ngrams(byte_sequence, N):<br/>    """Creates a list of N-grams from a byte sequence."""<br/>    Ngrams = ngrams(byte_sequence, N)<br/>    return list(Ngrams)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>We write a function to take a file and obtain its count of N-grams:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def binary_file_to_Ngram_counts(file, N):<br/>    """Takes a binary file and outputs the N-grams counts of its binary sequence."""<br/>    filebyte_sequence = read_file(file)<br/>    file_Ngrams = byte_sequence_to_Ngrams(filebyte_sequence, N)<br/>    return collections.Counter(file_Ngrams)</pre>
<ol start="6">
<li>We specify that our desired value is N=4 and obtain the counts of all 4-grams in the file:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">extracted_Ngrams = binary_file_to_Ngram_counts(file_to_analyze, 4)</pre>
<ol start="7">
<li>We list the 10 most common 4-grams of our file:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">print(extracted_Ngrams.most_common(10))</pre>
<p style="padding-left: 60px" class="mce-root">The result is as follows:</p>
<pre style="padding-left: 60px"><strong>[((0, 0, 0, 0), 24201), ((139, 240, 133, 246), 1920), ((32, 116, 111, 32), 1791), ((255, 255, 255, 255), 1663), ((108, 101, 100, 32), 1522), ((100, 32, 116, 111), 1519), ((97, 105, 108, 101), 1513), ((105, 108, 101, 100), 1513), ((70, 97, 105, 108), 1505), ((101, 100, 32, 116), 1503)]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In the literature and industry, it has been determined that the most frequent N-grams are also the most informative ones for a malware classification algorithm. For this reason, in this recipe, we will write functions to extract them for a file. We start by importing some helpful libraries for our extraction of N-grams (step 1). In particular, we import the collections library and the <kbd>ngrams</kbd> library from <kbd>nltk</kbd>. The collections library allows us to convert a list of N-grams to a frequency count of the N-grams, while the <kbd>ngrams</kbd> library allows us to take an ordered list of bytes and obtain a list of N-grams. We specify the file we would like to analyze and write a function that will read all of the bytes of a given file (steps 2 and 3). We define a few more convenience functions before we begin the extraction. In particular, we write a function to take a file's sequence of bytes and output a list of its N-grams (step 4), and a function to take a file and output the counts of its N-grams (step 5). We are now ready to pass in a file and extracts its N-grams. We do so to extract the counts of 4-grams of our file (step 6) and then display the 10 most common of them, along with their counts (step 7). We see that some of the N-gram sequences, such as (0,0,0,0) and (255,255,255,255) may not be very informative. For this reason, we will utilize feature selection methods to cut out the less informative N-grams in our next recipe.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting the best N-grams</h1>
                </header>
            
            <article>
                
<p class="mce-root">The number of different N-grams grows exponentially in N. Even for a fixed tiny N, such as N=3, there are <em>256x256x256=16,777,216</em> possible N-grams. This means that the number of N-grams features is impracticably large. Consequently, we must select a smaller subset of N-grams that will be of most value to our classifiers. In this section, we show three different methods for selecting the topmost informative N-grams.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>scikit-learn</kbd> and <kbd>nltk</kbd> packages in <kbd>pip</kbd>. The instructions are as follows:</p>
</div>
<div class="a-b-r-x">
<pre><strong>pip install sklearn nltk</strong></pre>
<p>In addition, benign and malicious files have been provided for you in the <kbd>PE Samples Dataset</kbd> folder in the root of the repository. Extract all archives named <kbd>Benign PE Samples*.7z</kbd> to a folder named <kbd>Benign PE Samples</kbd>. Extract all archives named <kbd>Malicious PE Samples*.7z</kbd> to a folder named <kbd>Malicious PE Samples</kbd>.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we show three different methods for selecting the most informative N-grams. The recipe assumes that <kbd>binaryFileToNgramCounts(file, N)</kbd> and all other helper functions from the previous recipe have been included:</p>
<ol>
<li class="mce-root">Begin by specifying the folders containing our samples, specifying our <kbd>N</kbd>, and importing modules to enumerate files:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from os import listdir<br/>from os.path import isfile, join<br/><br/>directories = ["Benign PE Samples", "Malicious PE Samples"]<br/>N = 2</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li class="mce-root">Next, we count all the N-grams from all the files:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">Ngram_counts_all_files = collections.Counter([])<br/>for dataset_path in directories:<br/>    all_samples = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]<br/>    for sample in all_samples:<br/>        file_path = join(dataset_path, sample)<br/>        Ngram_counts_all_files += binary_file_to_Ngram_counts(file_path, N)</pre>
<ol start="3">
<li class="mce-root"> We collect the <kbd>K1=1000</kbd> most frequent N-grams into a list:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">K1 = 1000<br/>K1_most_frequent_Ngrams = Ngram_counts_all_files.most_common(K1)<br/>K1_most_frequent_Ngrams_list = [x[0] for x in K1_most_frequent_Ngrams]</pre>
<ol start="4">
<li class="mce-root">A helper method, <kbd>featurize_sample</kbd>, will be used to take a sample and output the number of appearances of the most common N-grams in its byte sequence:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def featurize_sample(sample, K1_most_frequent_Ngrams_list):<br/>    """Takes a sample and produces a feature vector.<br/>    The features are the counts of the K1 N-grams we've selected.<br/>    """<br/>    K1 = len(K1_most_frequent_Ngrams_list)<br/>    feature_vector = K1 * [0]<br/>    file_Ngrams = binary_file_to_Ngram_counts(sample, N)<br/>    for i in range(K1):<br/>        feature_vector[i] = file_Ngrams[K1_most_frequent_Ngrams_list[i]]<br/>    return feature_vector</pre>
<ol start="5">
<li class="mce-root">We iterate through our directories, and use the preceding <kbd>featurize_sample</kbd> function to featurize our samples. We also create a set of labels:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">directories_with_labels = [("Benign PE Samples", 0), ("Malicious PE Samples", 1)]<br/>X = []<br/>y = []<br/>for dataset_path, label in directories_with_labels:<br/>    all_samples = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]<br/>    for sample in all_samples:<br/>        file_path = join(dataset_path, sample)<br/>        X.append(featurize_sample(file_path, K1_most_frequent_Ngrams_list))<br/>        y.append(label)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="6">
<li class="mce-root">We import the libraries we will be using for feature selection and specify how many features we would like to narrow down to:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2<br/><br/>K2 = 10</pre>
<ol start="7">
<li class="mce-root"> We perform three types of feature selections for our N-grams:</li>
</ol>
<ul>
<li style="padding-left: 30px"><strong>Frequency</strong>—selects the most frequent N-grams:<strong><br/></strong></li>
</ul>
<pre style="padding-left: 120px">X = np.asarray(X)<br/>X_top_K2_freq = X[:,:K2]</pre>
<ul>
<li style="padding-left: 30px"><strong>Mutual</strong> <strong>information</strong>—selects the N-grams ranked highest by the mutual information algorithm:</li>
</ul>
<pre style="padding-left: 120px" class="mce-root">mi_selector = SelectKBest(mutual_info_classif, k=K2)<br/>X_top_K2_mi = mi_selector.fit_transform(X, y)</pre>
<ul>
<li style="padding-left: 30px"><strong>Chi-squared</strong>—selects the N-grams ranked highest by the chi squared algorithm:<strong><br/></strong></li>
</ul>
<pre style="padding-left: 120px" class="mce-root">chi2_selector = SelectKBest(chi2, k=K2)<br/>X_top_K2_ch2 = chi2_selector.fit_transform(X, y)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>Unlike the previous recipe, in which we analyzed a single file's N-grams, in this recipe, we look at a large collection of files to understand which N-grams are the most informative features. We start by specifying the folders containing our samples, our value of N, and import some modules to enumerate files (step 1). We proceed to count <em>all</em> N-grams from <em>all</em> files in our dataset (step 2). This allows us to find the <em>globally</em> most frequent N-grams. Of these, we filter down to the <kbd>K1=1000</kbd> most frequent ones (step 3). Next, we introduce a helper method, <kbd>featurizeSample</kbd>, to be used to take a sample and output the number of appearances of the K1 most common N-grams in its byte sequence (step 4). We then iterate through our directories of files, and use the previous <kbd>featurizeSample</kbd> function to featurize our samples, as well as record their labels, as malicious or benign (step 5). The importance of the labels is that the assessment of whether an N-gram is informative depends on being able to discriminate between the malicious and benign classes based on it.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We import the <kbd>SelectKBest</kbd> library to select the best features via a score function, and the two score functions, mutual information and chi-squared (step 6). Finally, we apply the three different feature selection schemes to select the best N-grams and apply this knowledge to transform our features (step 7). In the first method, we simply select the K2 most frequent N-grams. Note that the selection of this method is often recommended in the literature, and is easier because of not requiring labels or extensive computation. In the second method, we use mutual information to narrow down the K2 features, while in the third, we use chi-squared to do so.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a static malware detector</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will see how to put together the recipes we discussed in prior sections to build a malware detector. Our malware detector will take in both features extracted from the PE header as well as features derived from N-grams.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>scikit-learn</kbd>, <kbd>nltk</kbd>, and <kbd>pefile</kbd> packages in <kbd>pip</kbd>. The instructions are as follows:</p>
<pre><strong>pip install sklearn nltk pefile</strong></pre>
<p>In addition, benign and malicious files have been provided for you in the <kbd>"PE Samples Dataset"</kbd> folder in the root of the repository. Extract all archives named <kbd>"Benign PE Samples*.7z"</kbd> to a folder named <kbd>"Benign PE Samples".</kbd> Extract all archives named <kbd>"Malicious PE Samples*.7z"</kbd> to a folder named <kbd>"Malicious PE Samples"</kbd>.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we will demonstrate a complete workflow in which we begin with raw samples, featurize them, vectorize their results, put them together, and finally train and test a classifier:</p>
<ol>
<li class="mce-root">Begin by enumerating our samples and assigning their labels:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import os<br/>from os import listdir<br/><br/>directories_with_labels = [("Benign PE Samples", 0), ("Malicious PE Samples", 1)]<br/>list_of_samples = []<br/>labels = []<br/>for dataset_path, label in directories_with_labels:<br/>    samples = [f for f in listdir(dataset_path)]<br/>    for sample in samples:<br/>        file_path = os.path.join(dataset_path, sample)<br/>        list_of_samples.append(file_path)<br/>        labels.append(label)</pre>
<ol start="2">
<li class="mce-root">We perform a stratified train-test split:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from sklearn.model_selection import train_test_split<br/><br/>samples_train, samples_test, labels_train, labels_test = train_test_split(<br/>    list_of_samples, labels, test_size=0.3, stratify=labels, random_state=11<br/>)</pre>
<ol start="3">
<li class="mce-root">We introduce convenience functions from prior sections in order to obtain features:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import collection<br/>from nltk import ngrams<br/>import numpy as np<br/>import pefile<br/><br/><br/>def read_file(file_path):<br/>    """Reads in the binary sequence of a binary file."""<br/>    with open(file_path, "rb") as binary_file:<br/>        data = binary_file.read()<br/>    return data<br/><br/><br/>def byte_sequence_to_Ngrams(byte_sequence, N):<br/>    """Creates a list of N-grams from a byte sequence."""<br/>    Ngrams = ngrams(byte_sequence, N)<br/>    return list(Ngrams)<br/><br/><br/>def binary_file_to_Ngram_counts(file, N):<br/>    """Takes a binary file and outputs the N-grams counts of its binary sequence."""<br/>    filebyte_sequence = read_file(file)<br/>    file_Ngrams = byte_sequence_to_Ngrams(filebyte_sequence, N)<br/>    return collections.Counter(file_Ngrams)<br/><br/><br/>def get_NGram_features_from_sample(sample, K1_most_frequent_Ngrams_list):<br/>    """Takes a sample and produces a feature vector.<br/>    The features are the counts of the K1 N-grams we've selected.<br/>    """<br/>    K1 = len(K1_most_frequent_Ngrams_list)<br/>    feature_vector = K1 * [0]<br/>    file_Ngrams = binary_file_to_Ngram_counts(sample, N)<br/>    for i in range(K1):<br/>        feature_vector[i] = file_Ngrams[K1_most_frequent_Ngrams_list[i]]<br/>    return feature_vector<br/><br/><br/>def preprocess_imports(list_of_DLLs):<br/>    """Normalize the naming of the imports of a PE file."""<br/>    temp = [x.decode().split(".")[0].lower() for x in list_of_DLLs]<br/>    return " ".join(temp)<br/><br/><br/>def get_imports(pe):<br/>    """Get a list of the imports of a PE file."""<br/>    list_of_imports = []<br/>    for entry in pe.DIRECTORY_ENTRY_IMPORT:<br/>        list_of_imports.append(entry.dll)<br/>    return preprocess_imports(list_of_imports)<br/><br/><br/>def get_section_names(pe):<br/>    """Gets a list of section names from a PE file."""<br/>    list_of_section_names = []<br/>    for sec in pe.sections:<br/>        normalized_name = sec.Name.decode().replace("\x00", "").lower()<br/>        list_of_section_names.append(normalized_name)<br/>    return "".join(list_of_section_names)</pre>
<ol start="4">
<li class="mce-root">We select the 100 most frequent 2-grams as our features:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">N = 2<br/>Ngram_counts_all = collections.Counter([])<br/>for sample in samples_train:<br/>    Ngram_counts_all += binary_file_to_Ngram_counts(sample, N)<br/>K1 = 100<br/>K1_most_frequent_Ngrams = Ngram_counts_all.most_common(K1)<br/>K1_most_frequent_Ngrams_list = [x[0] for x in K1_most_frequent_Ngrams]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="5">
<li class="mce-root">We extract the N-gram counts, section names, imports, and number of sections of each sample in our training test, and skip over samples whose PE header cannot be parsed:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">imports_corpus_train = []<br/>num_sections_train = []<br/>section_names_train = []<br/>Ngram_features_list_train = []<br/>y_train = []<br/>for i in range(len(samples_train)):<br/>    sample = samples_train[i]<br/>    try:<br/>        NGram_features = get_NGram_features_from_sample(<br/>            sample, K1_most_frequent_Ngrams_list<br/>        )<br/>        pe = pefile.PE(sample)<br/>        imports = get_imports(pe)<br/>        n_sections = len(pe.sections)<br/>        sec_names = get_section_names(pe)<br/>        imports_corpus_train.append(imports)<br/>        num_sections_train.append(n_sections)<br/>        section_names_train.append(sec_names)<br/>        Ngram_features_list_train.append(NGram_features)<br/>        y_train.append(labels_train[i])<br/>    except Exception as e:<br/>        print(sample + ":")<br/>        print(e)</pre>
<ol start="6">
<li class="mce-root">We use a hashing vectorizer followed by <kbd>tfidf</kbd> to convert the imports and section names, both of which are text features, into a numerical form:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer<br/>from sklearn.pipeline import Pipeline<br/><br/>imports_featurizer = Pipeline(<br/>    [<br/>       ("vect", HashingVectorizer(input="content", ngram_range=(1, 2))),<br/>        ("tfidf", TfidfTransformer(use_idf=True,)),<br/>    ]<br/>)<br/>section_names_featurizer = Pipeline(<br/>    [<br/>        ("vect", HashingVectorizer(input="content", ngram_range=(1, 2))),<br/>        ("tfidf", TfidfTransformer(use_idf=True,)),<br/>    ]<br/>)<br/>imports_corpus_train_transformed = imports_featurizer.fit_transform(<br/>    imports_corpus_train<br/>)<br/>section_names_train_transformed = section_names_featurizer.fit_transform(<br/>    section_names_train<br/>)</pre>
<ol start="7">
<li class="mce-root">We combine the vectorized features into a single array:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from scipy.sparse import hstack, csr_matrix<br/><br/>X_train = hstack(<br/>    [<br/>        Ngram_features_list_train,<br/>        imports_corpus_train_transformed,<br/>        section_names_train_transformed,<br/>        csr_matrix(num_sections_train).transpose(),<br/>    ]<br/>)</pre>
<ol start="8">
<li class="mce-root">We train a Random Forest classifier on the training set and print out its score:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from sklearn.ensemble import RandomForestClassifier<br/><br/>clf = RandomForestClassifier(n_estimators=100)<br/>clf = clf.fit(X_train, y_train)</pre>
<ol start="9">
<li class="mce-root">We collect the features of the testing set, just as we did for the training set:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">imports_corpus_test = []<br/>num_sections_test = []<br/>section_names_test = []<br/>Ngram_features_list_test = []<br/>y_test = []<br/>for i in range(len(samples_test)):<br/>    file = samples_test[i]<br/>    try:<br/>        NGram_features = get_NGram_features_from_sample(<br/>            sample, K1_most_frequent_Ngrams_list<br/>        )<br/>        pe = pefile.PE(file)<br/>        imports = get_imports(pe)<br/>        n_sections = len(pe.sections)<br/>        sec_names = get_section_names(pe)<br/>        imports_corpus_test.append(imports)<br/>        num_sections_test.append(n_sections)<br/>        section_names_test.append(sec_names)<br/>        Ngram_features_list_test.append(NGram_features)<br/>        y_test.append(labels_test[i])<br/>    except Exception as e:<br/>        print(sample + ":")<br/>        print(e)</pre>
<ol start="10">
<li class="mce-root">We apply the previously trained transformers to vectorize the text features and then test our classifier on the resulting test set:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">imports_corpus_test_transformed = imports_featurizer.transform(imports_corpus_test)<br/>section_names_test_transformed = section_names_featurizer.transform(section_names_test)<br/>X_test = hstack(<br/>    [<br/>        Ngram_features_list_test,<br/>        imports_corpus_test_transformed,<br/>        section_names_test_transformed,<br/>        csr_matrix(num_sections_test).transpose(),<br/>    ]<br/>)<br/>print(clf.score(X_test, y_test))</pre>
<div style="padding-left: 60px">The score of our classifier is as follows:</div>
<pre style="padding-left: 60px" class="p-Widget jp-RenderedText jp-mod-trusted jp-OutputArea-output"><strong>0.8859649122807017</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p class="mce-root">There are several notable new ideas in this section. We start by enumerating our samples and assigning them their respective labels (step 1). Because our dataset is imbalanced, it makes sense to use a stratified train-test split (step 2). In a stratified train-test split, a train-test split is created in which the proportion of each class is the same in the training set, testing set, and original set. This ensures that there is no possibility that our training set, for example, will consist of only one class due to a chance event. Next, we load the functions we will be using to featurize our samples. We employ our feature extraction techniques, as in previous recipes, to compute the best N-gram features (step 4) and then iterate through all of the files to extract all of the features (step 5). We then take the PE header features we obtained previously, such as section names and imports, and vectorize them using a basic NLP approach (step 6).</p>
<p class="mce-root">Having obtained all these different features, we are now ready to combine them, which we do using the <kbd>scipy</kbd> hstack, to merge the different features into one large sparse <kbd>scipy</kbd> array (step 7). We continue on to train a Random Forest classifier with default parameters (step 8) and then repeat the extraction process for our testing set (step 9). In step 10, we finally test out our trained classifier, obtaining a promising starting score. Overall, this recipe provides the foundations for a malware classifier that can be expanded into a high-powered solution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tackling class imbalance</h1>
                </header>
            
            <article>
                
<p>Often in applying machine learning to cybersecurity, we are faced with highly imbalanced datasets. For instance, it may be much easier to access a large collection of benign samples than it is to collect malicious samples. Conversely, you may be working at an enterprise that, for legal reasons, is prohibited from saving benign samples. In either case, your dataset will be highly skewed toward one class. As a consequence, naive machine learning aimed at maximizing accuracy will result in a classifier that predicts almost all samples as coming from the overrepresented class. There are several techniques that can be used to tackle the challenge of class imbalance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing the <kbd>scikit-learn</kbd> and <kbd>imbalanced-learn</kbd> pip packages. The instructions are as follows:</p>
</div>
<div class="a-b-r-x">
<pre><strong>pip install sklearn imbalanced-learn</strong></pre></div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the following steps, we will demonstrate several methods for dealing with imbalanced data:</p>
<ol start="1">
<li>Begin by loading the training and testing data, importing a decision tree, as well as some libraries we will be using to score performance:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import tree<br/>from sklearn.metrics import balanced_accuracy_score<br/>import numpy as np<br/>import scipy.sparse<br/>import collections<br/><br/>X_train = scipy.sparse.load_npz("training_data.npz")<br/>y_train = np.load("training_labels.npy")<br/>X_test = scipy.sparse.load_npz("test_data.npz")<br/>y_test = np.load("test_labels.npy")</pre>
<ol start="2">
<li>Train and test a simple Decision Tree classifier:</li>
</ol>
<pre style="padding-left: 60px">dt = tree.DecisionTreeClassifier()<br/>dt.fit(X_train, y_train)<br/>dt_pred = dt.predict(X_test)<br/>print(collections.Counter(dt_pred))<br/>print(balanced_accuracy_score(y_test, dt_pred))</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>Counter({0: 121, 1: 10})</strong><br/><strong>0.8333333333333333</strong></pre>
<p style="padding-left: 60px">Next, we test several techniques to improve performance.</p>
<ol start="3">
<li><strong>Weighting:</strong> We set the class weights of our classifier to <kbd>"balanced"</kbd> and train and test this new classifier:</li>
</ol>
<pre style="padding-left: 60px">dt_weighted = tree.DecisionTreeClassifier(class_weight="balanced")<br/>dt_weighted.fit(X_train, y_train)<br/>dt_weighted_pred = dt_weighted.predict(X_test)<br/>print(collections.Counter(dt_weighted_pred))<br/>print(balanced_accuracy_score(y_test, dt_weighted_pred))</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px" class="output_subarea output_text output_stream output_stdout"><strong>Counter({0: 114, 1: 17})</strong><br/><strong>0.9913793103448276</strong></pre>
<ol start="4">
<li><strong>Upsampling the minor class: </strong>We extract all test samples from class 0 and class 1:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.utils import resample<br/><br/>X_train_np = X_train.toarray()<br/>class_0_indices = [i for i, x in enumerate(y_train == 0) if x]<br/>class_1_indices = [i for i, x in enumerate(y_train == 1) if x]<br/>size_class_0 = sum(y_train == 0)<br/>X_train_class_0 = X_train_np[class_0_indices, :]<br/>y_train_class_0 = [0] * size_class_0<br/>X_train_class_1 = X_train_np[class_1_indices, :]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>We upsample the elements of class 1 with replacements until the number of samples of class 1 and class 0 are equal:</li>
</ol>
<pre style="padding-left: 60px">X_train_class_1_resampled = resample(<br/>    X_train_class_1, replace=True, n_samples=size_class_0<br/>)<br/>y_train_class_1_resampled = [1] * size_class_0</pre>
<ol start="6">
<li>We combine the newly upsampled samples into a single training set:</li>
</ol>
<pre style="padding-left: 60px">X_train_resampled = np.concatenate([X_train_class_0, X_train_class_1_resampled])<br/>y_train_resampled = y_train_class_0 + y_train_class_1_resampled</pre>
<ol start="7">
<li>We train and test a Random Forest classifier on our upsampled training set:</li>
</ol>
<pre style="padding-left: 60px">from scipy import sparse<br/><br/>X_train_resampled = sparse.csr_matrix(X_train_resampled)<br/>dt_resampled = tree.DecisionTreeClassifier()<br/>dt_resampled.fit(X_train_resampled, y_train_resampled)<br/>dt_resampled_pred = dt_resampled.predict(X_test)<br/>print(collections.Counter(dt_resampled_pred))<br/>print(balanced_accuracy_score(y_test, dt_resampled_pred))</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px" class="mce-root"><strong>Counter({0: 114, 1: 17})</strong><br/><strong>0.9913793103448276</strong></pre>
<ol start="8">
<li><strong>Downsampling the major class: </strong>We perform similar steps to the preceding upsampling, except this time we down-sample the major class until it is of the same size as the minor class:</li>
</ol>
<pre style="padding-left: 60px">X_train_np = X_train.toarray()<br/>class_0_indices = [i for i, x in enumerate(y_train == 0) if x]<br/>class_1_indices = [i for i, x in enumerate(y_train == 1) if x]<br/>size_class_1 = sum(y_train == 1)<br/>X_train_class_1 = X_train_np[class_1_indices, :]<br/>y_train_class_1 = [1] * size_class_1<br/>X_train_class_0 = X_train_np[class_0_indices, :]<br/>X_train_class_0_downsampled = resample(<br/>    X_train_class_0, replace=False, n_samples=size_class_1<br/>)<br/>y_train_class_0_downsampled = [0] * size_class_1</pre>
<ol start="9">
<li>We create a new training set from the downsampled data:</li>
</ol>
<pre style="padding-left: 60px">X_train_downsampled = np.concatenate([X_train_class_1, X_train_class_0_downsampled])<br/>y_train_downsampled = y_train_class_1 + y_train_class_0_downsampled</pre>
<ol start="10">
<li>We train a Random Forest classifier on this dataset:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">X_train_downsampled = sparse.csr_matrix(X_train_downsampled)<br/>dt_downsampled = tree.DecisionTreeClassifier()<br/>dt_downsampled.fit(X_train_downsampled, y_train_downsampled)<br/>dt_downsampled_pred = dt_downsampled.predict(X_test)<br/>print(collections.Counter(dt_downsampled_pred))<br/>print(balanced_accuracy_score(y_test, dt_downsampled_pred))</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px" class="mce-root"><strong>Counter({0: 100, 1: 31})</strong><br/><strong>0.9310344827586207</strong></pre>
<ol start="11">
<li><strong>Classifier including inner balancing samplers:<span> </span></strong>We utilize the imbalanced-learn package classifiers that resample subsets of data before the training estimators:</li>
</ol>
<pre style="padding-left: 60px">from imblearn.ensemble import BalancedBaggingClassifier<br/><br/>balanced_clf = BalancedBaggingClassifier(<br/>    base_estimator=tree.DecisionTreeClassifier(),<br/>    sampling_strategy="auto",<br/>    replacement=True,<br/>)<br/>balanced_clf.fit(X_train, y_train)<br/>balanced_clf_pred = balanced_clf.predict(X_test)<br/>print(collections.Counter(balanced_clf_pred))<br/>print(balanced_accuracy_score(y_test, balanced_clf_pred))</pre>
<div class="output">
<p style="padding-left: 60px" class="output_area"><span>This results in the following output:</span></p>
</div>
<pre style="padding-left: 60px"><strong>Counter({0: 113, 1: 18})</strong><br/><strong>0.9494252873563218</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>We start by loading in a predefined dataset (step 1) using the  <kbd>scipy.sparse.load_npz</kbd> loading function to load previously saved sparse matrices. Our next step is to train a basic Decision Tree model on our data (step 2). To measure performance, we utilize the balanced accuracy score, a measure that is often used in classification problems with imbalanced datasets. By definition, balanced accuracy is the average of recall obtained on each class. The best value is 1, whereas the worst value is 0.</p>
<p>In the following steps, we employ different techniques to tackle the class imbalance. Our first approach is to utilize class weights to adjust our Decision Tree to an imbalanced dataset (step 3). The balanced mode uses the values of <em>y</em> to automatically adjust weights inversely proportional to the class frequencies in the input data as <em>n_samples / (n_classes * np.bincount(y))</em>. In steps 4 to 7, we utilize upsampling to tackle class imbalance. This is the process of randomly duplicating observations from the minority class in order to reinforce the minority class's signal.</p>
<p>There are several methods for doing so, but the most common way is to simply resample with replacements as we have done. The two main concerns with upsampling are that it increases the size of the dataset and that it can lead to overfitting due to training on the same sample numerous times. In steps 8 to 10, we down-sample our major class. This simply means that we don't use all of the samples we have, but just enough so that we balance our classes.</p>
<p>The main issue with this technique is that we are forced to use a smaller training set. Our final approach, and the most sophisticated one, is to utilize a classifier that includes inner balancing samplers, namely the <kbd>BalancedBaggingClassifier</kbd> from <kbd>imbalanced-learn</kbd> (step 11). Overall, we see that every single one of our methods for tackling class imbalance increased the balanced accuracy score.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling type I and type II errors</h1>
                </header>
            
            <article>
                
<p>In many situations in machine learning, one type of error may be more important than another. For example, in a multilayered defense system, it may make sense to require a layer to have a low false alarm (low false positive) rate, at the cost of some detection rate. In this section, we provide a recipe for ensuring that the FPR does not exceed a desired limit by using thresholding.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing <kbd>scikit-learn</kbd> and <kbd>xgboost</kbd> in <kbd>pip</kbd>. The instructions are as follows:</p>
</div>
<div class="a-b-r-x">
<pre><strong>pip install sklearn xgboost</strong></pre></div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the following steps, we will load a dataset, train a classifier, and then tune a threshold to satisfy a false positive rate constraint:</p>
<ol>
<li>We load a dataset and specify that the desired FPR is at or below 1%:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>from scipy import sparse<br/>import scipy<br/><br/>X_train = scipy.sparse.load_npz("training_data.npz")<br/>y_train = np.load("training_labels.npy")<br/>X_test = scipy.sparse.load_npz("test_data.npz")<br/>y_test = np.load("test_labels.npy")<br/>desired_FPR = 0.01</pre>
<ol start="2">
<li>We write methods to calculate  <kbd>FPR</kbd> and <kbd>TPR</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.metrics import confusion_matrix<br/><br/><br/>def FPR(y_true, y_pred):<br/>    """Calculate the False Positive Rate."""<br/>    CM = confusion_matrix(y_true, y_pred)<br/>    TN = CM[0][0]<br/>    FP = CM[0][1]<br/>    return FP / (FP + TN)<br/><br/><br/>def TPR(y_true, y_pred):<br/>    """Calculate the True Positive Rate."""<br/>    CM = confusion_matrix(y_true, y_pred)<br/>    TP = CM[1][1]<br/>    FN = CM[1][0]<br/>    return TP / (TP + FN)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>We write a method to convert a vector of probabilities into a Boolean vector using thresholding:</li>
</ol>
<pre style="padding-left: 60px">def perform_thresholding(vector, threshold):<br/>    """Threshold a vector."""<br/>    return [0 if x &gt;= threshold else 1 for x in vector]</pre>
<ol start="4">
<li>We train an XGBoost model and calculate a probability prediction on the training data:</li>
</ol>
<pre style="padding-left: 60px">from xgboost import XGBClassifier<br/><br/>clf = XGBClassifier()<br/>clf.fit(X_train, y_train)<br/>clf_pred_prob = clf.predict_proba(X_train)</pre>
<ol start="5">
<li>Let's examine our prediction probability vectors:</li>
</ol>
<pre style="padding-left: 60px">print("Probabilities look like so:")<br/>print(clf_pred_prob[0:5])<br/>print()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>Probabilities look like so:</strong><br/><strong>[[0.9972162 0.0027838 ]</strong><br/><strong>[0.9985584 0.0014416 ]</strong><br/><strong>[0.9979202 0.00207978]</strong><br/><strong>[0.96858877 0.03141126]</strong><br/><strong>[0.91427565 0.08572436]]</strong></pre>
<ol start="6">
<li>We loop over 1,000 different threshold values, calculate the FPR for each, and when we satisfy our <kbd>FPR&lt;=desiredFPR</kbd>, we select that threshold:</li>
</ol>
<pre style="padding-left: 60px">M = 1000<br/>print("Fitting threshold:")<br/>for t in reversed(range(M)):<br/>    scaled_threshold = float(t) / M<br/>    thresholded_prediction = perform_thresholding(clf_pred_prob[:, 0], scaled_threshold)<br/>    print(t, FPR(y_train, thresholded_prediction), TPR(y_train, thresholded_prediction))<br/>    if FPR(y_train, thresholded_prediction) &lt;= desired_FPR:<br/>        print()<br/>        print("Selected threshold: ")<br/>        print(scaled_threshold)<br/>        break</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px"><strong>Fitting threshold:</strong><br/><strong>999 1.0 1.0</strong><br/><strong>998 0.6727272727272727 1.0</strong><br/><strong>997 0.4590909090909091 1.0</strong><br/><strong>996 0.33181818181818185 1.0</strong><br/><strong> &lt;snip&gt;</strong><br/><strong> 649 0.05454545454545454 1.0</strong><br/><strong>648 0.004545454545454545 0.7857142857142857</strong><br/><strong>Selected threshold: 0.648</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>We begin this recipe by loading in a previously featurized dataset and specifying a desired FPR constraint of 1% (step 1). The value to be used in practice depends highly on the situation and type of file being considered. There are a few considerations to follow: if the file is extremely common, but rarely malicious, such as a PDF, the desired FPR will have to be set very low, for example, 0.01%.</p>
<p>If the system is supported by additional systems that will double-check its verdict without human effort, then a high FPR might not be detrimental. Finally, a customer may have a preference, which will suggest a recommended value. We define a pair of convenience functions for FPR and TPR in step 2—these functions are very handy and reusable. Another convenience function we define is a function that will take our threshold value and use it to threshold a numerical vector (step 3).</p>
<p>In step 4, we train a model on the training data, and determine prediction probabilities on the training set as well. You can see what these look like in step 5. When a large dataset is available, using a validation set for determining the proper threshold will reduce the likelihood of overfitting. Finally, we compute the threshold to be used in future classification in order to ensure that the FPR constraint will be satisfied (step 6).</p>


            </article>

            
        </section>
    </body></html>