["```\nimport sys\nimport hashlib\n\nfilename = \"python-3.7.2-amd64.exe\"\n```", "```\nBUF_SIZE = 65536\nmd5 = hashlib.md5()\nsha256 = hashlib.sha256()\n```", "```\nwith open(filename, \"rb\") as f:\n    while True:\n        data = f.read(BUF_SIZE)\n        if not data:\n            break\n        md5.update(data)\n        sha256.update(data)\n```", "```\nprint(\"MD5: {0}\".format(md5.hexdigest()))\nprint(\"SHA256: {0}\".format(sha256.hexdigest()))\n```", "```\nMD5: ff258093f0b3953c886192dec9f52763\nSHA256: 0fe2a696f5a3e481fed795ef6896ed99157bcef273ef3c4a96f2905cbdb3aa13\n```", "```\n rule my_rule_name { condition: false }\n```", "```\n Rule my_rule_name { condition: true }\n```", "```\n Rule over_100kb { condition: filesize > 100KB }\n```", "```\n rule is_a_pdf {\n\n strings:\n   $pdf_magic = {25 50 44 46}\n\n condition:\n   $pdf_magic at 0\n }\n```", "```\n rule is_a_pdf\n {\n        strings:\n               $pdf_magic = {25 50 44 46}\n        condition:\n               $pdf_magic at 0\n }\n\n rule dummy_rule1\n {\n        condition:\n               false\n }\n\n rule dummy_rule2\n {\n        condition:\n               true\n }\n```", "```\nYara rule.yara PythonBrochure\n```", "```\nis_a_pdf target_file\ndummy_rule2 target_rule\n```", "```\npip install pefile\n```", "```\nimport pefile\n\ndesired_file = \"python-3.7.2-amd64.exe\"\npe = pefile.PE(desired_file)\n```", "```\nfor entry in pe.DIRECTORY_ENTRY_IMPORT:\n    print(entry.dll)\n    for imp in entry.imports:\n        print(\"\\t\", hex(imp.address), imp.name)\n```", "```\nfor section in pe.sections:\n    print(\n        section.Name,\n        hex(section.VirtualAddress),\n        hex(section.Misc_VirtualSize),\n        section.SizeOfRawData,\n    )\n```", "```\nprint(pe.dump_info())\n```", "```\npip install pefile\n```", "```\nimport pefile\nfrom os import listdir\nfrom os.path import isfile, join\n\ndirectories = [\"Benign PE Samples\", \"Malicious PE Samples\"]\n```", "```\ndef get_section_names(pe):\n    \"\"\"Gets a list of section names from a PE file.\"\"\"\n    list_of_section_names = []\n    for sec in pe.sections:\n        normalized_name = sec.Name.decode().replace(\"\\x00\", \"\").lower()\n        list_of_section_names.append(normalized_name)\n    return list_of_section_names\n```", "```\ndef preprocess_imports(list_of_DLLs):\n    \"\"\"Normalize the naming of the imports of a PE file.\"\"\"\n    return [x.decode().split(\".\")[0].lower() for x in list_of_DLLs]\n```", "```\ndef get_imports(pe):\n    \"\"\"Get a list of the imports of a PE file.\"\"\"\n    list_of_imports = []\n    for entry in pe.DIRECTORY_ENTRY_IMPORT:\n        list_of_imports.append(entry.dll)\n    return preprocess_imports(list_of_imports)\n```", "```\nimports_corpus = []\nnum_sections = []\nsection_names = []\nfor dataset_path in directories:\n    samples = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n    for file in samples:\n        file_path = dataset_path + \"/\" + file\n        try:\n```", "```\n            pe = pefile.PE(file_path)\n            imports = get_imports(pe)\n            n_sections = len(pe.sections)\n            sec_names = get_section_names(pe)\n            imports_corpus.append(imports)\n            num_sections.append(n_sections)\n            section_names.append(sec_names)\n```", "```\n        except Exception as e:\n            print(e)\n            print(\"Unable to obtain imports from \" + file_path)\n```", "```\npip install -U cuckoo\n```", "```\npip install PyGitHub\n```", "```\nimport os\nfrom github import Github\nimport base64\n```", "```\nusername = \"your_github_username\"\npassword = \"your_password\"\ntarget_dir = \"/path/to/JavascriptSamples/\"\ng = Github(username, password)\nrepositories = g.search_repositories(query='language:javascript')\nn = 5\ni = 0\n```", "```\nfor repo in repositories:\n    repo_name = repo.name\n    target_dir_of_repo = target_dir+\"\\\\\"+repo_name\n    print(repo_name)\n    try:\n```", "```\n        os.mkdir(target_dir_of_repo)\n        i += 1\n        contents = repo.get_contents(\"\")\n```", "```\n        while len(contents) > 1:\n            file_content = contents.pop(0)\n            if file_content.type == \"dir\":\n                contents.extend(repo.get_contents(file_content.path))\n            else:\n```", "```\n                st = str(file_content)\n                filename = st.split(\"\\\"\")[1].split(\"\\\"\")[0]\n                extension = filename.split(\".\")[-1]\n                if extension == \"js\":\n```", "```\n                    file_contents = repo.get_contents(file_content.path)\n                    file_data = base64.b64decode(file_contents.content)\n                    filename = filename.split(\"/\")[-1]\n                    file_out = open(target_dir_of_repo+\"/\"+filename, \"wb\")\n                    file_out.write(file_data)\n      except:\n        pass\n    if i==n:\n        break\n```", "```\ntarget_dir = \"/path/to/JavascriptSamples/\"\nrepositories = g.search_repositories(query='language:javascript')\n```", "```\ntarget_dir = \"/path/to/PowerShellSamples/\"\nrepositories = g.search_repositories(query='language:powershell').\n```", "```\ntarget_dir = \"/path/to/PythonSamples/\"\nrepositories = g.search_repositories(query='language:python').\n```", "```\npip install sklearn\n```", "```\nimport os\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\njavascript_path = \"/path/to/JavascriptSamples/\"\npython_path = \"/path/to/PythonSamples/\"\npowershell_path = \"/path/to/PowerShellSamples/\"\n```", "```\ncorpus = []\nlabels = []\nfile_types_and_labels = [(javascript_path, -1), (python_path, 0), (powershell_path, 1)]\nfor files_path, label in file_types_and_labels:\n    files = os.listdir(files_path)\n    for file in files:\n        file_path = files_path + \"/\" + file\n        try:\n            with open(file_path, \"r\") as myfile:\n                data = myfile.read().replace(\"\\n\", \"\")\n        except:\n            pass\n        data = str(data)\n        corpus.append(data)\n        labels.append(label)\n```", "```\nX_train, X_test, y_train, y_test = train_test_split(\n    corpus, labels, test_size=0.33, random_state=11\n)\ntext_clf = Pipeline(\n    [\n        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 3))),\n        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n        (\"rf\", RandomForestClassifier(class_weight=\"balanced\")),\n    ]\n)\n```", "```\ntext_clf.fit(X_train, y_train)\ny_test_pred = text_clf.predict(X_test)\nprint(accuracy_score(y_test, y_test_pred))\nprint(confusion_matrix(y_test, y_test_pred))\n```", "```\npip install ssdeep\n```", "```\nimport ssdeep\n\nstr1 = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\"\nstr2 = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore Magna aliqua.\"\nstr3 = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore aliqua.\"\nstr4 = \"Something completely different from the other strings.\"\n```", "```\nhash1 = ssdeep.hash(str1)\nhash2 = ssdeep.hash(str2)\nhash3 = ssdeep.hash(str3)\nhash4 = ssdeep.hash(str4)\n```", "```\nssdeep.compare(hash1, hash1)\nssdeep.compare(hash1, hash2)\nssdeep.compare(hash1, hash3)\nssdeep.compare(hash1, hash4)\n\n```", "```\n100\n39\n37\n0\n```", "```\npip install ssdeep\n```", "```\ntruncate -s +1 python-3.7.2-amd64-fake.exe\n```", "```\nhexdump -C python-3.7.2-amd64.exe |tail -5\n```", "```\n\n018ee0f0  e3 af d6 e9 05 3f b7 15  a1 c7 2a 5f b6 ae 71 1f  |.....?....*_..q.|\n018ee100  6f 46 62 1c 4f 74 f5 f5  a1 e6 91 b7 fe 90 06 3e  |oFb.Ot.........>|\n018ee110  de 57 a6 e1 83 4c 13 0d  b1 4a 3d e5 04 82 5e 35  |.W...L...J=...^5|\n018ee120  ff b2 e8 60 2d e0 db 24  c1 3d 8b 47 b3 00 00 00  |...`-..$.=.G....|\n\n```", "```\nhexdump -C python-3.7.2-amd64-fake.exe |tail -5\n```", "```\n018ee100  6f 46 62 1c 4f 74 f5 f5  a1 e6 91 b7 fe 90 06 3e  |oFb.Ot.........>|\n018ee110  de 57 a6 e1 83 4c 13 0d  b1 4a 3d e5 04 82 5e 35  |.W...L...J=...^5|\n018ee120  ff b2 e8 60 2d e0 db 24  c1 3d 8b 47 b3 00 00 00  |...`-..$.=.G....|\n018ee130  00                                                |.|\n018ee131\n```", "```\nimport ssdeep\n\nhash1 = ssdeep.hash_from_file(\"python-3.7.2-amd64.exe\")\nhash2 = ssdeep.hash_from_file(\"python-3.7.2-amd64-fake.exe\")\nssdeep.compare(hash1, hash2)\n\n```", "```\npip install nltk\n```", "```\nimport collections\nfrom nltk import ngrams\n```", "```\nfile_to_analyze = \"python-3.7.2-amd64.exe\"\n```", "```\ndef read_file(file_path):\n    \"\"\"Reads in the binary sequence of a binary file.\"\"\"\n    with open(file_path, \"rb\") as binary_file:\n        data = binary_file.read()\n    return data\n```", "```\ndef byte_sequence_to_Ngrams(byte_sequence, N):\n    \"\"\"Creates a list of N-grams from a byte sequence.\"\"\"\n    Ngrams = ngrams(byte_sequence, N)\n    return list(Ngrams)\n```", "```\ndef binary_file_to_Ngram_counts(file, N):\n    \"\"\"Takes a binary file and outputs the N-grams counts of its binary sequence.\"\"\"\n    filebyte_sequence = read_file(file)\n    file_Ngrams = byte_sequence_to_Ngrams(filebyte_sequence, N)\n    return collections.Counter(file_Ngrams)\n```", "```\nextracted_Ngrams = binary_file_to_Ngram_counts(file_to_analyze, 4)\n```", "```\nprint(extracted_Ngrams.most_common(10))\n```", "```\n[((0, 0, 0, 0), 24201), ((139, 240, 133, 246), 1920), ((32, 116, 111, 32), 1791), ((255, 255, 255, 255), 1663), ((108, 101, 100, 32), 1522), ((100, 32, 116, 111), 1519), ((97, 105, 108, 101), 1513), ((105, 108, 101, 100), 1513), ((70, 97, 105, 108), 1505), ((101, 100, 32, 116), 1503)]\n```", "```\npip install sklearn nltk\n```", "```\nfrom os import listdir\nfrom os.path import isfile, join\n\ndirectories = [\"Benign PE Samples\", \"Malicious PE Samples\"]\nN = 2\n```", "```\nNgram_counts_all_files = collections.Counter([])\nfor dataset_path in directories:\n    all_samples = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n    for sample in all_samples:\n        file_path = join(dataset_path, sample)\n        Ngram_counts_all_files += binary_file_to_Ngram_counts(file_path, N)\n```", "```\nK1 = 1000\nK1_most_frequent_Ngrams = Ngram_counts_all_files.most_common(K1)\nK1_most_frequent_Ngrams_list = [x[0] for x in K1_most_frequent_Ngrams]\n```", "```\ndef featurize_sample(sample, K1_most_frequent_Ngrams_list):\n    \"\"\"Takes a sample and produces a feature vector.\n    The features are the counts of the K1 N-grams we've selected.\n    \"\"\"\n    K1 = len(K1_most_frequent_Ngrams_list)\n    feature_vector = K1 * [0]\n    file_Ngrams = binary_file_to_Ngram_counts(sample, N)\n    for i in range(K1):\n        feature_vector[i] = file_Ngrams[K1_most_frequent_Ngrams_list[i]]\n    return feature_vector\n```", "```\ndirectories_with_labels = [(\"Benign PE Samples\", 0), (\"Malicious PE Samples\", 1)]\nX = []\ny = []\nfor dataset_path, label in directories_with_labels:\n    all_samples = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n    for sample in all_samples:\n        file_path = join(dataset_path, sample)\n        X.append(featurize_sample(file_path, K1_most_frequent_Ngrams_list))\n        y.append(label)\n```", "```\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2\n\nK2 = 10\n```", "```\nX = np.asarray(X)\nX_top_K2_freq = X[:,:K2]\n```", "```\nmi_selector = SelectKBest(mutual_info_classif, k=K2)\nX_top_K2_mi = mi_selector.fit_transform(X, y)\n```", "```\nchi2_selector = SelectKBest(chi2, k=K2)\nX_top_K2_ch2 = chi2_selector.fit_transform(X, y)\n```", "```\npip install sklearn nltk pefile\n```", "```\nimport os\nfrom os import listdir\n\ndirectories_with_labels = [(\"Benign PE Samples\", 0), (\"Malicious PE Samples\", 1)]\nlist_of_samples = []\nlabels = []\nfor dataset_path, label in directories_with_labels:\n    samples = [f for f in listdir(dataset_path)]\n    for sample in samples:\n        file_path = os.path.join(dataset_path, sample)\n        list_of_samples.append(file_path)\n        labels.append(label)\n```", "```\nfrom sklearn.model_selection import train_test_split\n\nsamples_train, samples_test, labels_train, labels_test = train_test_split(\n    list_of_samples, labels, test_size=0.3, stratify=labels, random_state=11\n)\n```", "```\nimport collection\nfrom nltk import ngrams\nimport numpy as np\nimport pefile\n\ndef read_file(file_path):\n    \"\"\"Reads in the binary sequence of a binary file.\"\"\"\n    with open(file_path, \"rb\") as binary_file:\n        data = binary_file.read()\n    return data\n\ndef byte_sequence_to_Ngrams(byte_sequence, N):\n    \"\"\"Creates a list of N-grams from a byte sequence.\"\"\"\n    Ngrams = ngrams(byte_sequence, N)\n    return list(Ngrams)\n\ndef binary_file_to_Ngram_counts(file, N):\n    \"\"\"Takes a binary file and outputs the N-grams counts of its binary sequence.\"\"\"\n    filebyte_sequence = read_file(file)\n    file_Ngrams = byte_sequence_to_Ngrams(filebyte_sequence, N)\n    return collections.Counter(file_Ngrams)\n\ndef get_NGram_features_from_sample(sample, K1_most_frequent_Ngrams_list):\n    \"\"\"Takes a sample and produces a feature vector.\n    The features are the counts of the K1 N-grams we've selected.\n    \"\"\"\n    K1 = len(K1_most_frequent_Ngrams_list)\n    feature_vector = K1 * [0]\n    file_Ngrams = binary_file_to_Ngram_counts(sample, N)\n    for i in range(K1):\n        feature_vector[i] = file_Ngrams[K1_most_frequent_Ngrams_list[i]]\n    return feature_vector\n\ndef preprocess_imports(list_of_DLLs):\n    \"\"\"Normalize the naming of the imports of a PE file.\"\"\"\n    temp = [x.decode().split(\".\")[0].lower() for x in list_of_DLLs]\n    return \" \".join(temp)\n\ndef get_imports(pe):\n    \"\"\"Get a list of the imports of a PE file.\"\"\"\n    list_of_imports = []\n    for entry in pe.DIRECTORY_ENTRY_IMPORT:\n        list_of_imports.append(entry.dll)\n    return preprocess_imports(list_of_imports)\n\ndef get_section_names(pe):\n    \"\"\"Gets a list of section names from a PE file.\"\"\"\n    list_of_section_names = []\n    for sec in pe.sections:\n        normalized_name = sec.Name.decode().replace(\"\\x00\", \"\").lower()\n        list_of_section_names.append(normalized_name)\n    return \"\".join(list_of_section_names)\n```", "```\nN = 2\nNgram_counts_all = collections.Counter([])\nfor sample in samples_train:\n    Ngram_counts_all += binary_file_to_Ngram_counts(sample, N)\nK1 = 100\nK1_most_frequent_Ngrams = Ngram_counts_all.most_common(K1)\nK1_most_frequent_Ngrams_list = [x[0] for x in K1_most_frequent_Ngrams]\n```", "```\nimports_corpus_train = []\nnum_sections_train = []\nsection_names_train = []\nNgram_features_list_train = []\ny_train = []\nfor i in range(len(samples_train)):\n    sample = samples_train[i]\n    try:\n        NGram_features = get_NGram_features_from_sample(\n            sample, K1_most_frequent_Ngrams_list\n        )\n        pe = pefile.PE(sample)\n        imports = get_imports(pe)\n        n_sections = len(pe.sections)\n        sec_names = get_section_names(pe)\n        imports_corpus_train.append(imports)\n        num_sections_train.append(n_sections)\n        section_names_train.append(sec_names)\n        Ngram_features_list_train.append(NGram_features)\n        y_train.append(labels_train[i])\n    except Exception as e:\n        print(sample + \":\")\n        print(e)\n```", "```\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\nimports_featurizer = Pipeline(\n    [\n       (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 2))),\n        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n    ]\n)\nsection_names_featurizer = Pipeline(\n    [\n        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 2))),\n        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n    ]\n)\nimports_corpus_train_transformed = imports_featurizer.fit_transform(\n    imports_corpus_train\n)\nsection_names_train_transformed = section_names_featurizer.fit_transform(\n    section_names_train\n)\n```", "```\nfrom scipy.sparse import hstack, csr_matrix\n\nX_train = hstack(\n    [\n        Ngram_features_list_train,\n        imports_corpus_train_transformed,\n        section_names_train_transformed,\n        csr_matrix(num_sections_train).transpose(),\n    ]\n)\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100)\nclf = clf.fit(X_train, y_train)\n```", "```\nimports_corpus_test = []\nnum_sections_test = []\nsection_names_test = []\nNgram_features_list_test = []\ny_test = []\nfor i in range(len(samples_test)):\n    file = samples_test[i]\n    try:\n        NGram_features = get_NGram_features_from_sample(\n            sample, K1_most_frequent_Ngrams_list\n        )\n        pe = pefile.PE(file)\n        imports = get_imports(pe)\n        n_sections = len(pe.sections)\n        sec_names = get_section_names(pe)\n        imports_corpus_test.append(imports)\n        num_sections_test.append(n_sections)\n        section_names_test.append(sec_names)\n        Ngram_features_list_test.append(NGram_features)\n        y_test.append(labels_test[i])\n    except Exception as e:\n        print(sample + \":\")\n        print(e)\n```", "```\nimports_corpus_test_transformed = imports_featurizer.transform(imports_corpus_test)\nsection_names_test_transformed = section_names_featurizer.transform(section_names_test)\nX_test = hstack(\n    [\n        Ngram_features_list_test,\n        imports_corpus_test_transformed,\n        section_names_test_transformed,\n        csr_matrix(num_sections_test).transpose(),\n    ]\n)\nprint(clf.score(X_test, y_test))\n```", "```\n0.8859649122807017\n```", "```\npip install sklearn imbalanced-learn\n```", "```\nfrom sklearn import tree\nfrom sklearn.metrics import balanced_accuracy_score\nimport numpy as np\nimport scipy.sparse\nimport collections\n\nX_train = scipy.sparse.load_npz(\"training_data.npz\")\ny_train = np.load(\"training_labels.npy\")\nX_test = scipy.sparse.load_npz(\"test_data.npz\")\ny_test = np.load(\"test_labels.npy\")\n```", "```\ndt = tree.DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ndt_pred = dt.predict(X_test)\nprint(collections.Counter(dt_pred))\nprint(balanced_accuracy_score(y_test, dt_pred))\n```", "```\nCounter({0: 121, 1: 10})\n0.8333333333333333\n```", "```\ndt_weighted = tree.DecisionTreeClassifier(class_weight=\"balanced\")\ndt_weighted.fit(X_train, y_train)\ndt_weighted_pred = dt_weighted.predict(X_test)\nprint(collections.Counter(dt_weighted_pred))\nprint(balanced_accuracy_score(y_test, dt_weighted_pred))\n```", "```\nCounter({0: 114, 1: 17})\n0.9913793103448276\n```", "```\nfrom sklearn.utils import resample\n\nX_train_np = X_train.toarray()\nclass_0_indices = [i for i, x in enumerate(y_train == 0) if x]\nclass_1_indices = [i for i, x in enumerate(y_train == 1) if x]\nsize_class_0 = sum(y_train == 0)\nX_train_class_0 = X_train_np[class_0_indices, :]\ny_train_class_0 = [0] * size_class_0\nX_train_class_1 = X_train_np[class_1_indices, :]\n```", "```\nX_train_class_1_resampled = resample(\n    X_train_class_1, replace=True, n_samples=size_class_0\n)\ny_train_class_1_resampled = [1] * size_class_0\n```", "```\nX_train_resampled = np.concatenate([X_train_class_0, X_train_class_1_resampled])\ny_train_resampled = y_train_class_0 + y_train_class_1_resampled\n```", "```\nfrom scipy import sparse\n\nX_train_resampled = sparse.csr_matrix(X_train_resampled)\ndt_resampled = tree.DecisionTreeClassifier()\ndt_resampled.fit(X_train_resampled, y_train_resampled)\ndt_resampled_pred = dt_resampled.predict(X_test)\nprint(collections.Counter(dt_resampled_pred))\nprint(balanced_accuracy_score(y_test, dt_resampled_pred))\n```", "```\nCounter({0: 114, 1: 17})\n0.9913793103448276\n```", "```\nX_train_np = X_train.toarray()\nclass_0_indices = [i for i, x in enumerate(y_train == 0) if x]\nclass_1_indices = [i for i, x in enumerate(y_train == 1) if x]\nsize_class_1 = sum(y_train == 1)\nX_train_class_1 = X_train_np[class_1_indices, :]\ny_train_class_1 = [1] * size_class_1\nX_train_class_0 = X_train_np[class_0_indices, :]\nX_train_class_0_downsampled = resample(\n    X_train_class_0, replace=False, n_samples=size_class_1\n)\ny_train_class_0_downsampled = [0] * size_class_1\n```", "```\nX_train_downsampled = np.concatenate([X_train_class_1, X_train_class_0_downsampled])\ny_train_downsampled = y_train_class_1 + y_train_class_0_downsampled\n```", "```\nX_train_downsampled = sparse.csr_matrix(X_train_downsampled)\ndt_downsampled = tree.DecisionTreeClassifier()\ndt_downsampled.fit(X_train_downsampled, y_train_downsampled)\ndt_downsampled_pred = dt_downsampled.predict(X_test)\nprint(collections.Counter(dt_downsampled_pred))\nprint(balanced_accuracy_score(y_test, dt_downsampled_pred))\n```", "```\nCounter({0: 100, 1: 31})\n0.9310344827586207\n```", "```\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nbalanced_clf = BalancedBaggingClassifier(\n    base_estimator=tree.DecisionTreeClassifier(),\n    sampling_strategy=\"auto\",\n    replacement=True,\n)\nbalanced_clf.fit(X_train, y_train)\nbalanced_clf_pred = balanced_clf.predict(X_test)\nprint(collections.Counter(balanced_clf_pred))\nprint(balanced_accuracy_score(y_test, balanced_clf_pred))\n```", "```\nCounter({0: 113, 1: 18})\n0.9494252873563218\n```", "```\npip install sklearn xgboost\n```", "```\nimport numpy as np\nfrom scipy import sparse\nimport scipy\n\nX_train = scipy.sparse.load_npz(\"training_data.npz\")\ny_train = np.load(\"training_labels.npy\")\nX_test = scipy.sparse.load_npz(\"test_data.npz\")\ny_test = np.load(\"test_labels.npy\")\ndesired_FPR = 0.01\n```", "```\nfrom sklearn.metrics import confusion_matrix\n\ndef FPR(y_true, y_pred):\n    \"\"\"Calculate the False Positive Rate.\"\"\"\n    CM = confusion_matrix(y_true, y_pred)\n    TN = CM[0][0]\n    FP = CM[0][1]\n    return FP / (FP + TN)\n\ndef TPR(y_true, y_pred):\n    \"\"\"Calculate the True Positive Rate.\"\"\"\n    CM = confusion_matrix(y_true, y_pred)\n    TP = CM[1][1]\n    FN = CM[1][0]\n    return TP / (TP + FN)\n```", "```\ndef perform_thresholding(vector, threshold):\n    \"\"\"Threshold a vector.\"\"\"\n    return [0 if x >= threshold else 1 for x in vector]\n```", "```\nfrom xgboost import XGBClassifier\n\nclf = XGBClassifier()\nclf.fit(X_train, y_train)\nclf_pred_prob = clf.predict_proba(X_train)\n```", "```\nprint(\"Probabilities look like so:\")\nprint(clf_pred_prob[0:5])\nprint()\n```", "```\nProbabilities look like so:\n[[0.9972162 0.0027838 ]\n[0.9985584 0.0014416 ]\n[0.9979202 0.00207978]\n[0.96858877 0.03141126]\n[0.91427565 0.08572436]]\n```", "```\nM = 1000\nprint(\"Fitting threshold:\")\nfor t in reversed(range(M)):\n    scaled_threshold = float(t) / M\n    thresholded_prediction = perform_thresholding(clf_pred_prob[:, 0], scaled_threshold)\n    print(t, FPR(y_train, thresholded_prediction), TPR(y_train, thresholded_prediction))\n    if FPR(y_train, thresholded_prediction) <= desired_FPR:\n        print()\n        print(\"Selected threshold: \")\n        print(scaled_threshold)\n        break\n```", "```\nFitting threshold:\n999 1.0 1.0\n998 0.6727272727272727 1.0\n997 0.4590909090909091 1.0\n996 0.33181818181818185 1.0\n <snip>\n 649 0.05454545454545454 1.0\n648 0.004545454545454545 0.7857142857142857\nSelected threshold: 0.648\n```"]