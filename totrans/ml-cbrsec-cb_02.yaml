- en: Machine Learning-Based Malware Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we begin to get serious about applying data science to cybersecurity.
    We will begin by learning how to perform static and dynamic analysis on samples.
    Building on this knowledge, we will learn how to featurize samples in order to
    construct a dataset with informative features. The highlight of the chapter is
    learning how to build a static malware detector using the featurization skills
    we have learned. Finally, you will learn how to tackle important machine learning
    challenges that occur in the domain of cybersecurity, such as class imbalance
    and **false positive rate** (**FPR**) constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter covers the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Malware static analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malware dynamic analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using machine learning to detect the file type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the similarity between two strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the similarity between two files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting N-grams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the best N-grams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a static malware detector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling class imbalance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling type I and type II errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: YARA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pefile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PyGitHub`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cuckoo Sandbox
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Toolkit** (**NLTK**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imbalanced-learn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code and datasets can be found at [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter02).
  prefs: []
  type: TYPE_NORMAL
- en: Malware static analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In static analysis, we examine a sample without executing it. The amount of
    information that can be obtained this way is large, ranging from something as
    simple as the name of the file to the more complex, such as specialized YARA signatures.
    We will be covering a selection of the large variety of features you could obtain
    by statically analyzing a sample. Despite its power and convenience, static analysis
    is no silver bullet, mainly because software can be obfuscated. For this reason,
    we will be employing dynamic analysis and other techniques in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the hash of a sample
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without delving into the intricacies of hashing, a hash is essentially a short
    and unique string signature. For example, we may hash the sequence of bytes of
    a file to obtain an essentially unique code for that file. This allows us to quickly
    compare two files to see whether they are identical.
  prefs: []
  type: TYPE_NORMAL
- en: There exist many hash procedures out there, so we will focus on the most important
    ones, namely, SHA256 and MD5\. Note that MD5 is known to exhibit vulnerabilities
    due to hash collisions—instances where two different objects have the same hash
    and, therefore, should be used with caution. In this recipe, we take an executable
    file and compute its MD5 and SHA256 hashes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparation for this recipe consists of downloading a test file, which is the
    Python executable from [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will see how to obtain the hash of a file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by importing the libraries and selecting the desired file you wish to
    hash:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the MD5 and SHA256 objects, and specify the size of the chunks
    we will be reading:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We then read in the file in chunks of 64 KB and incrementally construct our
    hashes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, print out the resulting hashes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will explain the steps that have been provided in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: In step 1, we import `hashlib`, a standard Python library for hash computation.
    We also specify the file we will be hashing—in this case, the file is `python-3.7.2-amd64.exe`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In step 2, we instantiate an `md5` object and an `sha256` object and specify
    the size of the chunks we will be reading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In step 3, we utilize the `.update(data)` method. This method allows us to compute
    the hash incrementally because it computes the hash of the concatenation. In other
    words, `hash.update(a)` followed by `hash.update(b)` is equivalent to `hash.update(a+b)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In step 4, we print out the hashes in hexadecimal digits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also verify that our computation is consistent with the hash calculations
    given by other sources, such as VirusTotal and the official Python website. The
    MD5 hash is displayed on the Python web page ([https://www.python.org/downloads/release/python-372/](https://www.python.org/downloads/release/python-372/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b284d160-12e6-4289-8b11-4c1c3f4bea6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The SHA256 hash is computed by uploading the file to VirusTotal ([https://www.virustotal.com/gui/home](https://www.virustotal.com/gui/home)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/12683aeb-2465-444a-8c15-582da8136d2d.png)'
  prefs: []
  type: TYPE_IMG
- en: YARA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'YARA is a computer language that allows a security expert to conveniently specify
    a rule that will then be used to classify all samples matching the rule. A minimal
    rule consists of a name and a condition, for example, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This rule will not match any file. Conversely, the following rule will match
    every sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A more useful example will match any file over 100 KB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Another example is checking whether a particular file is a PDF. To do so, we
    check if the magic numbers of the file correspond to the PDF. Magic numbers are
    a sequence of several bytes that occurs at the beginning of a file and indicates
    the type of file it is. In the case of a PDF, the sequence is `25 50 44 46`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's see how to run our rules against files.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparation for this recipe consists of installing YARA on your device. Instructions
    can be found at [https://yara.readthedocs.io/en/stable/](https://yara.readthedocs.io/en/stable/).
    For Windows, you need to download an executable file for YARA.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we show you how to create YARA rules and test them
    against a file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy your rules, as seen here, into a text file and name it `rules.yara`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, select a file you would like to check your rules against. Call it `target_file`.
    In a terminal, execute `Yara rules.yara target_file` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can observe, in *Step 1*, we copied several YARA rules. The first rule
    checks the magic numbers of a file to see if they match those of a PDF. The other
    two rules are trivial rules—one that matches every file, and one that matches
    no file. Then, in *Step 2*, we used the YARA program to run the rules against
    the target file. We saw from a printout that the file matched some rules but not
    others, as expected from an effective YARA ruleset.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the PE header
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Portable executable** (**PE**) files are a common Windows file type. PE files
    include the `.exe`, `.dll`, and `.sys` files. All PE files are distinguished by
    having a PE header, which is a header section of the code that instructs Windows
    on how to parse the subsequent code. The fields from the PE header are often used
    as features in the detection of malware. To easily extract the multitude of values
    of the PE header, we will utilize the `pefile` Python module. In this recipe,
    we will parse the PE header of a file, and then print out notable portions of
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `pefile` package in
    `pip`. In a terminal of your Python environment, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In addition, download the test file Python executable from [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will parse the PE header of a file, and then print
    out notable portions of it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the PE file and use it to parse the PE header of your desired file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'List the imports of the PE file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A small portion of the output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ab7af973-2df6-4626-9f0c-071d90b86067.png)'
  prefs: []
  type: TYPE_IMG
- en: 'List the sections of the PE file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/494806e9-3dab-4482-890a-e74329520e6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Print a full dump of the parsed information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'A small portion of the output is displayed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b237281e-fa9c-4bcc-ae48-672e34331f3f.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began in *step 1* by importing the `pefile` library and specifying which
    file we will be analyzing. In this case, the file was `python-3.7.2-amd64.exe`,
    though it is just as easy to analyze any other PE file. We then continued on to
    examine the DLLs being imported by the file, in order to understand which methods
    the file may be using in *Step 2*. DLLs answer this question because a DLL is
    a library of code that other applications may call upon. For example, `USER32.dll`
    is a library that contains Windows USER, a component of the Microsoft Windows
    operating system that provides core functionality for building user interfaces.
    The component allows other applications to leverage the functionality for window
    management, message passing, input processing, and standard controls. Logically
    then, if we see that a file is importing a method such as `GetCursorPos`, then
    it is likely to be looking to determine the position of the cursor. Continuing
    in *step 3*, we printed out the sections of the PE file. These provide a logical
    and physical separation to the different parts of a program, and therefore offer
    the analyst valuable information about the program. Finally, we printed out all
    of the parsed PE header information from the file in preparation for later utilizing
    it for feature engineering (*Step 4*).
  prefs: []
  type: TYPE_NORMAL
- en: Featurizing the PE header
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will extract features from the PE header to be used in building
    a `malware/benign` samples classifier. We will continue utilizing the `pefile`
    Python module.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `pefile` package in
    `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In addition, benign and malicious files have been provided for you in the `PE
    Samples Dataset` folder in the root of the repository. Extract all archives named
    `Benign PE Samples*.7z` to a folder named `Benign PE Samples`. Extract all archives
    named `Malicious PE Samples*.7z` to a folder named `Malicious PE Samples`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will collect notable portions of the PE header:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pefile` and modules for enumerating our samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function to collect the names of the sections of a file and preprocess
    them for readability and normalization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a convenience function to preprocess and standardize our imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define a function to collect the imports from a file using `pefile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we prepare to iterate through all of our files and create lists to
    store our features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to collecting the preceding features, we also collect the number
    of sections of a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In case a file''s PE header cannot be parsed, we define a try-catch clause:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, in *Step 1*, we imported the `pefile` module to enumerate the
    samples. Once that is done, we define the convenience function, as you can see
    in *Step 2*. The reason being that it often imports using varying cases (upper/lower).
    This causes the same import to appear as distinct imports.
  prefs: []
  type: TYPE_NORMAL
- en: After preprocessing the imports, we then define another function to collect
    all the imports of a file into a list. We will also define a function to collect
    the names of the sections of a file in order to standardize these names such as `.text`,
    `.rsrc`, and `.reloc` while containing distinct parts of the file (*Step 3*).
    The files are then enumerated in our folders and empty lists will be created to
    hold the features we will be extracting. The predefined functions will then collect
    the imports (*Step 4*), section names, and the number of sections of each file
    (*Steps 5* and *6*). Lastly, a try-catch clause will be defined in case a file's
    PE header cannot be parsed (*Step 7*). This can happen for many reasons. One reason
    being that the file is not actually a PE file. Another reason is that its PE header
    is intentionally or unintentionally malformed.
  prefs: []
  type: TYPE_NORMAL
- en: Malware dynamic analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike static analysis, dynamic analysis is a malware analysis technique in
    which the expert executes the sample, and then studies the sample's behavior as
    it is being run. The main advantage of dynamic analysis over static is that it
    allows you to bypass obfuscation by simply observing how a sample behaves, rather
    than trying to decipher the sample's contents and behavior. Since malware is intrinsically
    unsafe, researchers resort to executing samples in a **virtual machine** (**VM**).
    This is called **sandboxing**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most prominent tools for automating the analysis of samples in a
    VM is Cuckoo Sandbox. The initial installation of Cuckoo Sandbox is straightforward;
    simply run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You must make sure that you also have a VM that your machine can control. Configuring
    the sandbox can be a challenge, but instructions are available at [https://cuckoo.sh/docs/](https://cuckoo.sh/docs/).
  prefs: []
  type: TYPE_NORMAL
- en: We show now how to utilize Cuckoo Sandbox to obtain a dynamic analysis of a
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once your Cuckoo Sandbox is set up, and has a web interface running, follow
    these steps to gather runtime information about a sample:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up your web interface (the default location is `127.0.0.1:8000`), click
    **SUBMIT A FILE FOR ANALYSIS**, and select the sample you wish to analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/291e48cd-c544-4e1e-a6a6-db1011cdb26d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screen will appear automatically. In it, select the type of analysis
    you wish to perform on your sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/e3968f76-8e95-4f15-b910-28254321e6da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click **Analyze** to analyze the sample in your sandbox. The result should
    look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/8f250a32-58bd-4c91-82bf-2c007cfaee3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, open up the report for the sample you have analyzed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/9bb0c653-8e33-4dee-9e86-434341350844.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select the **Behavioral Analysis** tab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/01b72b50-2410-48d1-8f35-fb5aeb472c81.png)'
  prefs: []
  type: TYPE_IMG
- en: The displayed sequence of API calls, registry key changes, and other events
    can all be used as input to a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At a conceptual level, obtaining dynamic analysis results consists of running
    samples in environments that allow the analyst to collect runtime information.
    Cuckoo Sandbox is a flexible framework with prebuilt modules to do just that.
    We began our recipe for using Cuckoo Sandbox by opening up the web portal (*Step
    1*). A **command-line interface** (**CLI**) exists as well. We proceeded to submit
    a sample and select the type of analysis we wished to perform (*Steps 2* and *3*).
    These steps, too, can be performed through the Cuckoo CLI. We proceeded to examine
    the analysis report (*Step 4*). You can see at this stage how the many modules
    of Cuckoo Sandbox reflect in the final analysis output. For instance, if a module
    for capturing traffic is installed and used, then the report will contain the
    data captured in the network tab. We proceeded to focus our view of the analysis
    to behavioral analysis (*Step 5*), and in particular to observe the sequence of
    API calls. API calls are basically operations performed by the OS. This sequence
    makes up a fantastic feature set that we will utilize to detect malware in future
    recipes. Finally, note that in a production environment, it may make sense to
    create a custom-made sandbox with custom modules for data collection, as well
    as equip it with anti-VM detection software to facilitate successful analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Using machine learning to detect the file type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the techniques hackers use to sneak their malicious files into security
    systems is to obfuscate their file types. For example, a (malicious) PowerShell
    script is expected to have an extension, `.ps1`. A system administrator can aim
    to combat the execution of all PowerShell scripts on a system by preventing the
    execution of all files with the `.ps1` extension. However, the mischievous hacker
    can remove or change the extension, rendering the file's identity a mystery. Only
    by examining the contents of the file can it then be distinguished from an ordinary
    text file. For practical reasons, it is not possible for humans to examine all
    text files on a system. Consequently, it is expedient to resort to automated methods.
    In this chapter, we will demonstrate how you can use machine learning to detect
    the file type of an unknown file. Our first step is to curate a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping GitHub for files of a specific type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To curate a dataset, we will scrape GitHub for the specific file types we are
    interested in.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `PyGitHub` package in
    `pip` by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In addition, you will need GitHub account credentials.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we curate a dataset and then use it to create a classifier
    to determine the file type. For demonstration purposes, we show how to obtain
    a collection of PowerShell scripts, Python scripts, and JavaScript files by scraping
    GitHub. A collection of samples obtained in this way can be found in the accompanying
    repository as `PowerShellSamples.7z`, `PythonSamples.7z`, and `JavascriptSamples.7z`.
    First, we will write the code for the JavaScript scraper:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by importing the `PyGitHub` library in order to be able to call the GitHub
    API. We also import the `base64` module for decoding the `base64` encoded files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We must supply our credentials, and then specify a query—in this case, for
    JavaScript—to select our repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We loop over the repositories matching our criteria:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a directory for each repository matching our search criteria, and
    then read in its contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We add all directories of the repository to a queue in order to list all of
    the files contained within the directories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If we find a non-directory file, we check whether its extension is `.js`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If the extension is `.js`, we write out a copy of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Once finished, it is convenient to move all the JavaScript files into one folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To obtain PowerShell samples, run the same code, changing the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, for Python files, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by importing the `PyGitHub` library in *Step 1* in order to be able
    to conveniently call the GitHub APIs. These will allow us to scrape and explore
    the universe of repositories. We also import the `base64` module for decoding
    the `base64` encoded files that we will be downloading from GitHub. Note that
    there is a rate limit on the number of API calls a generic user can make to GitHub.
    For this reason, you will find that if you attempt to download too many files
    in a short duration, your script will not get all of the files. Our next step
    is to supply our credentials to GitHub (*step 2*), and specify that we are looking
    for repositories with JavaScript, using the `query='language:javascript'` command.
    We enumerate such repositories matching our criteria of being associated with
    JavaScript, and if they do, we search through these for files ending with `.js` and
    create local copies (steps 3 to 6). Since these files are encoded in `base64`,
    we make sure to decode them to plaintext in step 7\. Finally, we show you how
    to adjust the script in order to scrape other file types, such as Python and PowerShell
    (Step 8).
  prefs: []
  type: TYPE_NORMAL
- en: Classifying files by type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a dataset, we would like to train a classifier. Since the files
    in question are scripts, we approach the problem as an NLP problem.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `scikit-learn` package
    in `pip`. The instructions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In addition, we have supplied you with samples of each file type in the `JavascriptSamples.7z`,
    `PythonSamples.7z`, and `PowerShellSamples.7z` archives, in case you would like
    to supplement your own dataset. Extract these into separate folders for the following
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for the following can be found on [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/blob/master/Chapter02/Classifying%20Files%20by%20Type/File%20Type%20Classifier.ipynb](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/blob/master/Chapter02/Classifying%20Files%20by%20Type/File%20Type%20Classifier.ipynb).
    We build a classifier using this data to predict files as JavaScript, Python,
    or PowerShell:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by importing the necessary libraries and specifying the paths of the
    samples we will be using to train and test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we read in all of the file types. We also create an array of labels with
    -1, 0, and 1 representing the JavaScript, Python, and PowerShell scripts, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We go on to create a train-test split and a pipeline that will perform basic
    NLP on the files, followed by a random forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We fit the pipeline to the training data, and then use it to predict on the
    testing data. Finally, we print out the accuracy and the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cb75e173-58ff-41db-a345-5959d5436b5d.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging the dataset we built up in the *Scraping GitHub for files of a specific
    type* recipe, we place files in different directories, based on their file type,
    and then specify the paths in preparation for building our classifier (step 1).
    The code for this recipe assumes that the `"JavascriptSamples"` directory and
    others contain the samples, and have no subdirectories. We read in all files into
    a corpus, and record their labels (step 2). We train-test split the data and prepare
    a pipeline that will perform basic NLP on the files, followed by a random forest
    classifier (step 3). The choice of classifier here is meant for illustrative purposes,
    rather than to imply a best choice of classifier for this type of data. Finally,
    we perform the basic, but important, steps in the process of creating a machine
    learning classifier, consisting of fitting the pipeline to the training data and
    then assessing its performance on the testing set by measuring its accuracy and
    confusion matrix (step 4).
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the similarity between two strings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check whether two files are identical, we utilize standard cryptographic
    hash functions, such as SHA256 and MD5\. However, at times, we would like to also
    know to what extent two files are similar. For that purpose, we utilize similarity
    hashing algorithms. The one we will be demonstrating here is `ssdeep`.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's see how to use `ssdeep` to compare two strings. This can be useful
    to detect tampering in a text or script and also plagiarism.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparation for this recipe consists of installing the `ssdeep` package in `pip`.
    The installation is a little tricky and does not always work on Windows. Instructions
    can be found at [https://python-ssdeep.readthedocs.io/en/latest/installation.html.](https://python-ssdeep.readthedocs.io/en/latest/installation.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you only have a Windows machine and installing `ssdeep` does not work, then
    one possible solution is to run `ssdeep` on an Ubuntu VM, and then install it
    in `pip`, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Begin by importing the `ssdeep` library and creating three strings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Hash the strings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As a reference,
  prefs: []
  type: TYPE_NORMAL
- en: hash1 is `u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BSnJi:f4kPvtHMCMubyFtQ'`,
  prefs: []
  type: TYPE_NORMAL
- en: hash2 is `u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BS+EFECJi:f4kPvtHMCMubyFIsJQ'`,
  prefs: []
  type: TYPE_NORMAL
- en: hash3 is `u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BS6:f4kPvtHMCMubyF0'`, and
  prefs: []
  type: TYPE_NORMAL
- en: hash4 is `u'3:60QKZ+4CDTfDaRFKYLVL:ywKDC2mVL'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we see what kind of similarity scores the strings have:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The numerical results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic idea behind `ssdeep` is to combine a number of traditional hashes
    whose boundaries are determined by the context of the input. This collection of
    hashes can then be used to identify modified versions of known files even when
    they have been modified by insertion, modification, or deletion.
  prefs: []
  type: TYPE_NORMAL
- en: For our recipe, we began by creating a set of four test strings meant as a toy
    example to illustrate how changes in a string will affect its similarity measures
    (step 1). The first, `str1`, is simply the first sentence of Lorem Ipsum. The
    second string, `str2`, differs in the capitalization of `m` in magna. The third
    string, `str3`, is missing the word magna altogether. Finally, the fourth string
    is an entirely different string. Our next step, step 2, is to hash the strings
    using the similarity hashing `ssdeep` library. Observe that similar strings have
    visibly similar similarity hashes. This should be contrasted with traditional
    hashes, in which even a small alteration produces a completely different hash.
    Next, we derive the similarity score between the various strings using `ssdeep`
    (step 3). In particular, observe that the `ssdeep` similarity score between two
    strings is an integer ranging between 0 and 100, with 100 being identical and
    0 being dissimilar. Two identical strings will have a similarity score of 100\.
    Changing the case of one letter in our string lowered the similarity score significantly
    to 39 because the strings are relatively short. Removing a word lowered it to
    37\. And two completely different strings had a similarity of 0.
  prefs: []
  type: TYPE_NORMAL
- en: Although other, in some cases better, fuzzy hashes are available, `ssdeep` is
    still a primary choice because of its speed and being a de facto standard.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the similarity between two files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we are going to see how to apply `ssdeep` to measure the similarity between
    two binary files. The applications of this concept are many, but one in particular
    is using the similarity measure as a distance in clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparation for this recipe consists of installing the `ssdeep` package in `pip`.
    The installation is a little tricky and does not always work on Windows. Instructions
    can be found at [https://python-ssdeep.readthedocs.io/en/latest/installation.html](https://python-ssdeep.readthedocs.io/en/latest/installation.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you only have a Windows machine and it does not work, then one possible
    solution is to run `ssdeep` on an Ubuntu VM by installing `pip` with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In addition, download a test file such as the Python executable from [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following recipe, we tamper with a binary file. We then compare it to
    the original to see that `ssdeep` determines that the two files are highly similar
    but not identical:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we download the latest version of Python, `python-3.7.2-amd64.exe`.
    I am going to create a copy, rename it `python-3.7.2-amd64-fake.exe`, and add
    a null byte at the end:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `hexdump`, I can verify that the operation was successful by looking
    at the file before and after:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The same can be verified with a second file using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, I will hash the two files using `ssdeep` and compare the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The output to the preceding code is `99`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This scenario simulates tampering with a file and then utilizing similarity
    hashing to detect the existence of tampering, as well as measuring the size of
    the delta. We begin with a vanilla Python executable and then tamper with it by
    adding a null byte at the end (step 1). In real life, a hacker may take a legitimate
    program and insert malicious code into the sample. We double-checked that the
    tempering was successful and examined its nature using a `hexdump` in step 2\.
    We then ran a similarity computation using similarity hashing on the original
    and tempered file, to observe that a minor alteration took place (step 3). Utilizing
    only standard hashing, we would have no idea how the two files are related, other
    than to conclude that they are not the same file. Knowing how to compare files
    allows us to cluster malware and benign files in machine learning algorithms,
    as well as group them into families.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting N-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In standard quantitative analysis of text, N-grams are sequences of N tokens
    (for example, words or characters). For instance, given the text *The quick brown
    fox jumped over the lazy dog,* if our tokens are words, then the 1-grams are *the*,
    *quick*, *brown*, *fox*, *jumped*, *over*, *the*, *lazy*, and *dog*. The 2-grams
    are *the quick*, *quick brown*, *brown fox*, and so on. The 3-grams are *the quick
    brown*, *quick brown fox*, *brown fox jumped*, and so on. Just like the local
    statistics of the text allowed us to build a Markov chain to perform statistical
    predictions and text generation from a corpus, N-grams allow us to model the local
    statistical properties of our corpus. Our ultimate goal is to utilize the counts
    of N-grams to help us predict whether a sample is malicious or benign. In this
    recipe, we demonstrate how to extract N-gram counts from a sample.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `nltk` package in `pip`.
    The instructions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In addition, download a test file, such as the Python executable from [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will enumerate all the 4-grams of a sample file
    and select the 50 most frequent ones:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing the `collections` library to facilitate counting and
    the `ngrams` library from `nltk` to ease extraction of N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify which file we would like to analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a convenience function to read in a file''s bytes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We write a convenience function to take a byte sequence and obtain N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We write a function to take a file and obtain its count of N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify that our desired value is N=4 and obtain the counts of all 4-grams
    in the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We list the 10 most common 4-grams of our file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the literature and industry, it has been determined that the most frequent
    N-grams are also the most informative ones for a malware classification algorithm.
    For this reason, in this recipe, we will write functions to extract them for a
    file. We start by importing some helpful libraries for our extraction of N-grams
    (step 1). In particular, we import the collections library and the `ngrams` library
    from `nltk`. The collections library allows us to convert a list of N-grams to
    a frequency count of the N-grams, while the `ngrams` library allows us to take
    an ordered list of bytes and obtain a list of N-grams. We specify the file we
    would like to analyze and write a function that will read all of the bytes of
    a given file (steps 2 and 3). We define a few more convenience functions before
    we begin the extraction. In particular, we write a function to take a file's sequence
    of bytes and output a list of its N-grams (step 4), and a function to take a file
    and output the counts of its N-grams (step 5). We are now ready to pass in a file
    and extracts its N-grams. We do so to extract the counts of 4-grams of our file
    (step 6) and then display the 10 most common of them, along with their counts
    (step 7). We see that some of the N-gram sequences, such as (0,0,0,0) and (255,255,255,255)
    may not be very informative. For this reason, we will utilize feature selection
    methods to cut out the less informative N-grams in our next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best N-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of different N-grams grows exponentially in N. Even for a fixed tiny
    N, such as N=3, there are *256x256x256=16,777,216* possible N-grams. This means
    that the number of N-grams features is impracticably large. Consequently, we must
    select a smaller subset of N-grams that will be of most value to our classifiers.
    In this section, we show three different methods for selecting the topmost informative
    N-grams.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `scikit-learn` and `nltk`
    packages in `pip`. The instructions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: In addition, benign and malicious files have been provided for you in the `PE
    Samples Dataset` folder in the root of the repository. Extract all archives named
    `Benign PE Samples*.7z` to a folder named `Benign PE Samples`. Extract all archives
    named `Malicious PE Samples*.7z` to a folder named `Malicious PE Samples`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we show three different methods for selecting the most
    informative N-grams. The recipe assumes that `binaryFileToNgramCounts(file, N)`
    and all other helper functions from the previous recipe have been included:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by specifying the folders containing our samples, specifying our `N`,
    and importing modules to enumerate files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we count all the N-grams from all the files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We collect the `K1=1000` most frequent N-grams into a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'A helper method, `featurize_sample`, will be used to take a sample and output
    the number of appearances of the most common N-grams in its byte sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We iterate through our directories, and use the preceding `featurize_sample`
    function to featurize our samples. We also create a set of labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We import the libraries we will be using for feature selection and specify
    how many features we would like to narrow down to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform three types of feature selections for our N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Frequency**—selects the most frequent N-grams:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '**Mutual** **information**—selects the N-grams ranked highest by the mutual
    information algorithm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '**Chi-squared**—selects the N-grams ranked highest by the chi squared algorithm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike the previous recipe, in which we analyzed a single file's N-grams, in
    this recipe, we look at a large collection of files to understand which N-grams
    are the most informative features. We start by specifying the folders containing
    our samples, our value of N, and import some modules to enumerate files (step
    1). We proceed to count *all* N-grams from *all* files in our dataset (step 2).
    This allows us to find the *globally* most frequent N-grams. Of these, we filter
    down to the `K1=1000` most frequent ones (step 3). Next, we introduce a helper
    method, `featurizeSample`, to be used to take a sample and output the number of
    appearances of the K1 most common N-grams in its byte sequence (step 4). We then
    iterate through our directories of files, and use the previous `featurizeSample`
    function to featurize our samples, as well as record their labels, as malicious
    or benign (step 5). The importance of the labels is that the assessment of whether
    an N-gram is informative depends on being able to discriminate between the malicious
    and benign classes based on it.
  prefs: []
  type: TYPE_NORMAL
- en: We import the `SelectKBest` library to select the best features via a score
    function, and the two score functions, mutual information and chi-squared (step
    6). Finally, we apply the three different feature selection schemes to select
    the best N-grams and apply this knowledge to transform our features (step 7).
    In the first method, we simply select the K2 most frequent N-grams. Note that
    the selection of this method is often recommended in the literature, and is easier
    because of not requiring labels or extensive computation. In the second method,
    we use mutual information to narrow down the K2 features, while in the third,
    we use chi-squared to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Building a static malware detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to put together the recipes we discussed in
    prior sections to build a malware detector. Our malware detector will take in
    both features extracted from the PE header as well as features derived from N-grams.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `scikit-learn`, `nltk`,
    and `pefile` packages in `pip`. The instructions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: In addition, benign and malicious files have been provided for you in the `"PE
    Samples Dataset"` folder in the root of the repository. Extract all archives named
    `"Benign PE Samples*.7z"` to a folder named `"Benign PE Samples".` Extract all
    archives named `"Malicious PE Samples*.7z"` to a folder named `"Malicious PE Samples"`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will demonstrate a complete workflow in which we
    begin with raw samples, featurize them, vectorize their results, put them together,
    and finally train and test a classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by enumerating our samples and assigning their labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform a stratified train-test split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We introduce convenience functions from prior sections in order to obtain features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'We select the 100 most frequent 2-grams as our features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract the N-gram counts, section names, imports, and number of sections
    of each sample in our training test, and skip over samples whose PE header cannot
    be parsed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a hashing vectorizer followed by `tfidf` to convert the imports and
    section names, both of which are text features, into a numerical form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We combine the vectorized features into a single array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We train a Random Forest classifier on the training set and print out its score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We collect the features of the testing set, just as we did for the training
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the previously trained transformers to vectorize the text features
    and then test our classifier on the resulting test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The score of our classifier is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several notable new ideas in this section. We start by enumerating
    our samples and assigning them their respective labels (step 1). Because our dataset
    is imbalanced, it makes sense to use a stratified train-test split (step 2). In
    a stratified train-test split, a train-test split is created in which the proportion
    of each class is the same in the training set, testing set, and original set.
    This ensures that there is no possibility that our training set, for example,
    will consist of only one class due to a chance event. Next, we load the functions
    we will be using to featurize our samples. We employ our feature extraction techniques,
    as in previous recipes, to compute the best N-gram features (step 4) and then
    iterate through all of the files to extract all of the features (step 5). We then
    take the PE header features we obtained previously, such as section names and
    imports, and vectorize them using a basic NLP approach (step 6).
  prefs: []
  type: TYPE_NORMAL
- en: Having obtained all these different features, we are now ready to combine them,
    which we do using the `scipy` hstack, to merge the different features into one
    large sparse `scipy` array (step 7). We continue on to train a Random Forest classifier
    with default parameters (step 8) and then repeat the extraction process for our
    testing set (step 9). In step 10, we finally test out our trained classifier,
    obtaining a promising starting score. Overall, this recipe provides the foundations
    for a malware classifier that can be expanded into a high-powered solution.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling class imbalance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often in applying machine learning to cybersecurity, we are faced with highly
    imbalanced datasets. For instance, it may be much easier to access a large collection
    of benign samples than it is to collect malicious samples. Conversely, you may
    be working at an enterprise that, for legal reasons, is prohibited from saving
    benign samples. In either case, your dataset will be highly skewed toward one
    class. As a consequence, naive machine learning aimed at maximizing accuracy will
    result in a classifier that predicts almost all samples as coming from the overrepresented
    class. There are several techniques that can be used to tackle the challenge of
    class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing the `scikit-learn` and `imbalanced-learn`
    pip packages. The instructions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will demonstrate several methods for dealing with
    imbalanced data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by loading the training and testing data, importing a decision tree,
    as well as some libraries we will be using to score performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Train and test a simple Decision Tree classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Next, we test several techniques to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighting:** We set the class weights of our classifier to `"balanced"` and
    train and test this new classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '**Upsampling the minor class: **We extract all test samples from class 0 and
    class 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'We upsample the elements of class 1 with replacements until the number of samples
    of class 1 and class 0 are equal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We combine the newly upsampled samples into a single training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'We train and test a Random Forest classifier on our upsampled training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '**Downsampling the major class: **We perform similar steps to the preceding
    upsampling, except this time we down-sample the major class until it is of the
    same size as the minor class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a new training set from the downsampled data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'We train a Random Forest classifier on this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '**Classifier including inner balancing samplers: **We utilize the imbalanced-learn
    package classifiers that resample subsets of data before the training estimators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by loading in a predefined dataset (step 1) using the  `scipy.sparse.load_npz`
    loading function to load previously saved sparse matrices. Our next step is to
    train a basic Decision Tree model on our data (step 2). To measure performance,
    we utilize the balanced accuracy score, a measure that is often used in classification
    problems with imbalanced datasets. By definition, balanced accuracy is the average
    of recall obtained on each class. The best value is 1, whereas the worst value
    is 0.
  prefs: []
  type: TYPE_NORMAL
- en: In the following steps, we employ different techniques to tackle the class imbalance.
    Our first approach is to utilize class weights to adjust our Decision Tree to
    an imbalanced dataset (step 3). The balanced mode uses the values of *y* to automatically
    adjust weights inversely proportional to the class frequencies in the input data
    as *n_samples / (n_classes * np.bincount(y))*. In steps 4 to 7, we utilize upsampling
    to tackle class imbalance. This is the process of randomly duplicating observations
    from the minority class in order to reinforce the minority class's signal.
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods for doing so, but the most common way is to simply
    resample with replacements as we have done. The two main concerns with upsampling
    are that it increases the size of the dataset and that it can lead to overfitting
    due to training on the same sample numerous times. In steps 8 to 10, we down-sample
    our major class. This simply means that we don't use all of the samples we have,
    but just enough so that we balance our classes.
  prefs: []
  type: TYPE_NORMAL
- en: The main issue with this technique is that we are forced to use a smaller training
    set. Our final approach, and the most sophisticated one, is to utilize a classifier
    that includes inner balancing samplers, namely the `BalancedBaggingClassifier`
    from `imbalanced-learn` (step 11). Overall, we see that every single one of our
    methods for tackling class imbalance increased the balanced accuracy score.
  prefs: []
  type: TYPE_NORMAL
- en: Handling type I and type II errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many situations in machine learning, one type of error may be more important
    than another. For example, in a multilayered defense system, it may make sense
    to require a layer to have a low false alarm (low false positive) rate, at the
    cost of some detection rate. In this section, we provide a recipe for ensuring
    that the FPR does not exceed a desired limit by using thresholding.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe consists of installing `scikit-learn` and `xgboost`
    in `pip`. The instructions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will load a dataset, train a classifier, and then
    tune a threshold to satisfy a false positive rate constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We load a dataset and specify that the desired FPR is at or below 1%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'We write methods to calculate  `FPR` and `TPR`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'We write a method to convert a vector of probabilities into a Boolean vector
    using thresholding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'We train an XGBoost model and calculate a probability prediction on the training
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s examine our prediction probability vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'We loop over 1,000 different threshold values, calculate the FPR for each,
    and when we satisfy our `FPR<=desiredFPR`, we select that threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin this recipe by loading in a previously featurized dataset and specifying
    a desired FPR constraint of 1% (step 1). The value to be used in practice depends
    highly on the situation and type of file being considered. There are a few considerations
    to follow: if the file is extremely common, but rarely malicious, such as a PDF,
    the desired FPR will have to be set very low, for example, 0.01%.'
  prefs: []
  type: TYPE_NORMAL
- en: If the system is supported by additional systems that will double-check its
    verdict without human effort, then a high FPR might not be detrimental. Finally,
    a customer may have a preference, which will suggest a recommended value. We define
    a pair of convenience functions for FPR and TPR in step 2—these functions are
    very handy and reusable. Another convenience function we define is a function
    that will take our threshold value and use it to threshold a numerical vector
    (step 3).
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, we train a model on the training data, and determine prediction probabilities
    on the training set as well. You can see what these look like in step 5\. When
    a large dataset is available, using a validation set for determining the proper
    threshold will reduce the likelihood of overfitting. Finally, we compute the threshold
    to be used in future classification in order to ensure that the FPR constraint
    will be satisfied (step 6).
  prefs: []
  type: TYPE_NORMAL
