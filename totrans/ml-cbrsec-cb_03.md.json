["```\npip install sklearn\n```", "```\nimport os\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n```", "```\njs_path = \"path\\\\to\\\\JavascriptSamples\"\nobfuscated_js_path = \"path\\\\to\\\\ObfuscatedJavascriptSamples\"\n\ncorpus = []\nlabels = []\nfile_types_and_labels = [(js_path, 0), (obfuscated_js_path, 1)]\n```", "```\nfor files_path, label in file_types_and_labels:\n    files = os.listdir(files_path)\n    for file in files:\n        file_path = files_path + \"/\" + file\n        try:\n            with open(file_path, \"r\") as myfile:\n                data = myfile.read().replace(\"\\n\", \"\")\n                data = str(data)\n                corpus.append(data)\n                labels.append(label)\n        except:\n            pass\n```", "```\nX_train, X_test, y_train, y_test = train_test_split(\n    corpus, labels, test_size=0.33, random_state=42\n)\ntext_clf = Pipeline(\n    [\n        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 3))),\n        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n        (\"rf\", RandomForestClassifier(class_weight=\"balanced\")),\n    ]\n)\n```", "```\ntext_clf.fit(X_train, y_train)\ny_test_pred = text_clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_test_pred))\nprint(confusion_matrix(y_test, y_test_pred))\n```", "```\n PDFiD 0.2.5 PythonBrochure.pdf\n\n PDF Header: %PDF-1.6\n obj                 1096\n endobj              1095\n stream              1061\n endstream           1061\n xref                   0\n trailer                0\n startxref              2\n /Page                 32\n /Encrypt               0\n /ObjStm               43\n /JS                    0\n /JavaScript            0\n /AA                    1\n /OpenAction            0\n /AcroForm              1\n /JBIG2Decode           0\n /RichMedia             0\n /Launch                0\n /EmbeddedFile          0\n /XFA                   0\n /URI                   0\n /Colors > 2^24         0\n```", "```\nfrom IPython.utils import io\n```", "```\ndef PDF_to_FV(file_path):\n    \"\"\"Featurize a PDF file using pdfid.\"\"\"\n```", "```\n     with io.capture_output() as captured:\n         %run -i pdfid $file_path\n     out = captured.stdout\n```", "```\n    out1 = out.split(\"\\n\")[2:-2]\n    return [int(x.split()[-1]) for x in out1]\n```", "```\nfrom os import listdir\n\nPDFs_path = \"PDFSamples\\\\\"\n```", "```\nX = []\nfiles = listdir(PDFs_path)\nfor file in files:\n    file_path = PDFs_path + file\n    X.append(PDF_to_FV(file_path))\n```", "```\n [1096, 1095, 1061, 1061, 0, 0, 2, 32, 0, 43, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n```", "```\npip install nltk\n```", "```\nfrom os import listdir\nfrom nltk import ngrams\nimport hashlib\n\ndirectories = [\"Benign PE Samples\", \"Malicious PE Samples\"]\nN = 2\n```", "```\ndef read_file(file_path):\n    \"\"\"Reads in the binary sequence of a binary file.\"\"\"\n    with open(file_path, \"rb\") as binary_file:\n        data = binary_file.read()\n    return data\n\ndef byte_sequence_to_Ngrams(byte_sequence, N):\n    \"\"\"Creates a list of N-grams from a byte sequence.\"\"\"\n    return ngrams(byte_sequence, N)\n```", "```\ndef hash_input(inp):\n    \"\"\"Compute the MD5 hash of an input.\"\"\"\n    return int(hashlib.md5(inp).hexdigest(), 16)\n\ndef make_ngram_hashable(Ngram):\n    \"\"\"Convert N-gram into bytes to be hashable.\"\"\"\n    return bytes(Ngram)\n```", "```\ndef hash_file_Ngrams_into_dictionary(file_Ngrams, T):\n    \"\"\"Hashes N-grams in a list and then keeps track of the counts in a dictionary.\"\"\"\n    for Ngram in file_Ngrams:\n        hashable_Ngram = make_ngram_hashable(Ngram)\n        hashed_and_reduced = hash_input(hashable_Ngram) % B\n        T[hashed_and_reduced] = T.get(hashed_and_reduced, 0) + 1\n```", "```\nB = 65521\nT = {}\n```", "```\nfor dataset_path in directories:\n    samples = [f for f in listdir(dataset_path)]\n    for file in samples:\n        file_path = dataset_path + \"/\" + file\n        file_byte_sequence = read_file(file_path)\n        file_Ngrams = byte_sequence_to_Ngrams(file_byte_sequence, N)\n        hash_file_Ngrams_into_dictionary(file_Ngrams, T)\n```", "```\nK1 = 1000\nimport heapq\n\nK1_most_common_Ngrams_Using_Hash_Grams = heapq.nlargest(K1, T)\n```", "```\ndef featurize_sample(file, K1_most_common_Ngrams_Using_Hash_Grams):\n    \"\"\"Takes a sample and produces a feature vector.\n    The features are the counts of the K1 N-grams we've selected.\n    \"\"\"\n    K1 = len(K1_most_common_Ngrams_Using_Hash_Grams)\n    fv = K1 * [0]\n    file_byte_sequence = read_file(file_path)\n    file_Ngrams = byte_sequence_to_Ngrams(file_byte_sequence, N)\n    for Ngram in file_Ngrams:\n        hashable_Ngram = make_ngram_hashable(Ngram)\n        hashed_and_reduced = hash_input(hashable_Ngram) % B\n        if hashed_and_reduced in K1_most_common_Ngrams_Using_Hash_Grams:\n            index = K1_most_common_Ngrams_Using_Hash_Grams.index(hashed_and_reduced)\n            fv[index] += 1\n    return fv\n```", "```\nX = []\nfor dataset_path in directories:\n    samples = [f for f in listdir(dataset_path)]\n    for file in samples:\n        file_path = dataset_path + \"/\" + file\n        X.append(featurize_sample(file_path, K1_most_common_Ngrams_Using_Hash_Grams))\n```", "```\npip install sklearn nltk xgboost\n```", "```\nimport numpy as np\nimport os\nimport json\n\ndirectories_with_labels = [(\"DA Logs Benign\", 0), (\"DA Logs Malware\", 1)]\n```", "```\ndef get_API_class_method_type_from_log(log):\n    \"\"\"Parses out API calls from behavioral logs.\"\"\"\n    API_data_sequence = []\n    with open(log) as log_file:\n        json_log = json.load(log_file)\n        api_calls_array = \"[\" + json_log[\"api_calls\"] + \"]\"\n```", "```\n        api_calls = json.loads(api_calls_array)\n        for api_call in api_calls:\n            data = api_call[\"class\"] + \":\" + api_call[\"method\"] + \":\" + api_call[\"type\"]\n            API_data_sequence.append(data)\n    return API_data_sequence\n```", "```\ndata_corpus = []\nlabels = []\nfor directory, label in directories_with_labels:\n    logs = os.listdir(directory)\n    for log_path in logs:\n        file_path = directory + \"/\" + log_path\n        try:\n            data_corpus.append(get_API_class_method_type_from_log(file_path))\n            labels.append(label)\n        except:\n            pass\n```", "```\nprint(data_corpus[0])\n\n['android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content', 'android.app.ContextImpl:registerReceiver:binder', 'android.app.ContextImpl:registerReceiver:binder', 'android.os.SystemProperties:get:content', 'android.os.SystemProperties:get:content']\n```", "```\nfrom sklearn.model_selection import train_test_split\n\ncorpus_train, corpus_test, y_train, y_test = train_test_split(\n    data_corpus, labels, test_size=0.2, random_state=11\n)\n```", "```\nimport collections\nfrom nltk import ngrams\nimport numpy as np\n\ndef read_file(file_path):\n    \"\"\"Reads in the binary sequence of a binary file.\"\"\"\n    with open(file_path, \"rb\") as binary_file:\n        data = binary_file.read()\n    return data\n\ndef text_to_Ngrams(text, n):\n    \"\"\"Produces a list of N-grams from a text.\"\"\"\n    Ngrams = ngrams(text, n)\n    return list(Ngrams)\n\ndef get_Ngram_counts(text, N):\n    \"\"\"Get a frequency count of N-grams in a text.\"\"\"\n    Ngrams = text_to_Ngrams(text, N)\n    return collections.Counter(Ngrams)\n```", "```\nN = 4\ntotal_Ngram_count = collections.Counter([])\nfor file in corpus_train:\n    total_Ngram_count += get_Ngram_counts(file, N)\n```", "```\nK1 = 3000\nK1_most_frequent_Ngrams = total_Ngram_count.most_common(K1)\nK1_most_frequent_Ngrams_list = [x[0] for x in K1_most_frequent_Ngrams]\n\n[('java.lang.reflect.Method:invoke:reflection', 'java.lang.reflect.Method:invoke:reflection', 'java.lang.reflect.Method:invoke:reflection', 'java.lang.reflect.Method:invoke:reflection'),\n\n('java.io.FileInputStream:read:runtime', 'java.io.FileInputStream:read:runtime', 'java.io.FileInputStream:read:runtime', 'java.io.FileInputStream:read:runtime'),\n\n <snip>\n\n ('android.os.SystemProperties:get:content',   'android.os.SystemProperties:get:content',   'android.os.SystemProperties:get:content',   'javax.crypto.spec.SecretKeySpec:javax.crypto.spec.SecretKeySpec:crypto')\n```", "```\ndef featurize_sample(file, Ngrams_list):\n    \"\"\"Takes a sample and produces a feature vector.\n    The features are the counts of the K1 N-grams we've selected.\n    \"\"\"\n    K1 = len(Ngrams_list)\n    feature_vector = K1 * [0]\n    fileNgrams = get_Ngram_counts(file, N)\n    for i in range(K1):\n        feature_vector[i] = fileNgrams[Ngrams_list[i]]\n    return feature_vector\n```", "```\nX_train = []\nfor sample in corpus_train:\n    X_train.append(featurize_sample(sample, K1_most_frequent_Ngrams_list))\nX_train = np.asarray(X_train)\nX_test = []\nfor sample in corpus_test:\n    X_test.append(featurize_sample(sample, K1_most_frequent_Ngrams_list))\nX_test = np.asarray(X_test)\n```", "```\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\n\nK2 = 500\nmi_pipeline = Pipeline(\n    [\n        (\"mutual_information\", SelectKBest(mutual_info_classif, k=K2)),\n        (\"xgb\", XGBClassifier()),\n    ]\n)\n```", "```\nmi_pipeline.fit(X_train, y_train)\nprint(\"Training accuracy:\")\nprint(mi_pipeline.score(X_train, y_train))\nprint(\"Testing accuracy:\")\nprint(mi_pipeline.score(X_test, y_test))\n```", "```\nTraining accuracy:\n0.8149428743235118\nTesting accuracy:\n0.8033674082982561\n```", "```\npip install keras tensorflow tqdm\n```", "```\nimport numpy as np\nfrom tqdm import tqdm\n```", "```\ndef embed_bytes(byte):\n    binary_string = \"{0:08b}\".format(byte)\n    vec = np.zeros(8)\n    for i in range(8):\n        if binary_string[i] == \"1\":\n            vec[i] = float(1) / 16\n        else:\n            vec[i] = -float(1) / 16\n    return vec\n```", "```\nimport os\nfrom os import listdir\n\ndirectories_with_labels = [(\"Benign PE Samples\", 0), (\"Malicious PE Samples\", 1)]\nlist_of_samples = []\nlabels = []\nfor dataset_path, label in directories_with_labels:\n    samples = [f for f in listdir(dataset_path)]\n    for file in samples:\n        file_path = os.path.join(dataset_path, file)\n        list_of_samples.append(file_path)\n        labels.append(label)\n```", "```\ndef read_file(file_path):\n    \"\"\"Read the binary sequence of a file.\"\"\"\n    with open(file_path, \"rb\") as binary_file:\n        return binary_file.read()\n```", "```\nmax_size = 15000\nnum_samples = len(list_of_samples)\nX = np.zeros((num_samples, 8, max_size))\nY = np.asarray(labels)\nfile_num = 0\nfor file in tqdm(list_of_samples):\n    sample_byte_sequence = read_file(file)\n    for i in range(min(max_size, len(sample_byte_sequence))):\n        X[file_num, :, i] = embed_bytes(sample_byte_sequence[i])\n    file_num += 1\n```", "```\nfrom keras import optimizers\n\nmy_opt = optimizers.SGD(lr=0.01, decay=1e-5, nesterov=True)\n```", "```\n from keras import Input\n from keras.layers import Conv1D, Activation, multiply, GlobalMaxPool1D, Dense\n from keras import Model\n\n inputs = Input(shape=(8, maxSize))\n conv1 = Conv1D(kernel_size=(128), filters=32, strides=(128), padding='same')(inputs)\n conv2 = Conv1D(kernel_size=(128), filters=32, strides=(128), padding='same')(inputs)\n a = Activation('sigmoid', name='sigmoid')(conv2)\n mul = multiply([conv1, a])\n b = Activation('relu', name='relu')(mul)\n p = GlobalMaxPool1D()(b)\n d = Dense(16)(p)\n predictions = Dense(1, activation='sigmoid')(d)\n model = Model(inputs=inputs, outputs=predictions)\n\n```", "```\nmodel.compile(optimizer=my_opt, loss=\"binary_crossentropy\", metrics=[\"acc\"])\nbatch_size = 16\nnum_batches = int(num_samples / batch_size)\n```", "```\nfor batch_num in tqdm(range(num_batches)):\n    batch = X[batch_num * batch_size : (batch_num + 1) * batch_size]\n    model.train_on_batch(\n        batch, Y[batch_num * batch_size : (batch_num + 1) * batch_size]\n    )\n```", "```\nimport os\n\nfiles_path = \"Benign PE Samples UPX/\"\nfiles = os.listdir(files_path)\nfile_paths = [files_path+x for x in files]\n```", "```\nfrom subprocess import Popen, PIPE\n\ncmd = \"upx.exe\"\nfor path in file_paths:\n    cmd2 = cmd+\" \\\"\"+path+\"\\\"\"\n    res = Popen(cmd2, stdout=PIPE).communicate()\n    print(res)\n```", "```\n    if \"error\" in str(res[0]):\n        print(path)\n        os.remove(path)\n```", "```\npip install sklearn nltk\n```", "```\nimport os\nfrom os import listdir\n\ndirectories_with_labels = [\n    (\"Benign PE Samples\", 0),\n    (\"Benign PE Samples UPX\", 1),\n    (\"Benign PE Samples Amber\", 2),\n]\nlist_of_samples = []\nlabels = []\nfor dataset_path, label in directories_with_labels:\n    samples = [f for f in listdir(dataset_path)]\n    for file in samples:\n        file_path = os.path.join(dataset_path, file)\n        list_of_samples.append(file_path)\n        labels.append(label)\n```", "```\nfrom sklearn.model_selection import train_test_split\n\nsamples_train, samples_test, labels_train, labels_test = train_test_split(\n    list_of_samples, labels, test_size=0.3, stratify=labels, random_state=11\n)\n```", "```\nimport collections\nfrom nltk import ngrams\nimport numpy as np\n```", "```\ndef read_file(file_path):\n    \"\"\"Reads in the binary sequence of a binary file.\"\"\"\n    with open(file_path, \"rb\") as binary_file:\n        data = binary_file.read()\n    return data\n\ndef byte_sequence_to_Ngrams(byte_sequence, N):\n    \"\"\"Creates a list of N-grams from a byte sequence.\"\"\"\n    Ngrams = ngrams(byte_sequence, N)\n    return list(Ngrams)\n\ndef extract_Ngram_counts(file, N):\n    \"\"\"Takes a binary file and outputs the N-grams counts of its binary sequence.\"\"\"\n    filebyte_sequence = read_file(file)\n    file_Ngrams = byte_sequence_to_Ngrams(filebyte_sequence, N)\n    return collections.Counter(file_Ngrams)\n\ndef featurize_sample(sample, K1_most_frequent_Ngrams_list):\n    \"\"\"Takes a sample and produces a feature vector.\n    The features are the counts of the K1 N-grams we've selected.\n    \"\"\"\n    K1 = len(K1_most_frequent_Ngrams_list)\n    feature_vector = K1 * [0]\n    file_Ngrams = extract_Ngram_counts(sample, N)\n    for i in range(K1):\n        feature_vector[i] = file_Ngrams[K1_most_frequent_Ngrams_list[i]]\n    return feature_vector\n```", "```\nN = 2\ntotal_Ngram_count = collections.Counter([])\nfor file in samples_train:\n    total_Ngram_count += extract_Ngram_counts(file, N)\nK1 = 100\nK1_most_common_Ngrams = total_Ngram_count.most_common(K1)\nK1_most_common_Ngrams_list = [x[0] for x in K1_most_common_Ngrams]\n```", "```\nNgram_features_list_train = []\ny_train = []\nfor i in range(len(samples_train)):\n    file = samples_train[i]\n    NGram_features = featurize_sample(file, K1_most_common_Ngrams_list)\n    Ngram_features_list_train.append(NGram_features)\n    y_train.append(labels_train[i])\nX_train = Ngram_features_list_train\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100)\nclf = clf.fit(X_train, y_train)\n```", "```\nNgram_features_list_test = []\ny_test = []\nfor i in range(len(samples_test)):\n    file = samples_test[i]\n    NGram_features = featurize_sample(file, K1_most_common_Ngrams_list)\n    Ngram_features_list_test.append(NGram_features)\n    y_test.append(labels_test[i])\nX_test = Ngram_features_list_test\n```", "```\ny_pred = clf.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)\n```", "```\npip install pandas keras tensorflow sklearn\n```", "```\n\"MalGAN_input/samplesIn.csv\" \n```", "```\nimport os\nimport pandas as pd\nfrom keras.models import load_model\nimport MalGAN_utils\nimport MalGAN_gen_adv_examples\n```", "```\nsave_path = \"MalGAN_output\"\nmodel_path = \"MalGAN_input/malconv.h5\"\nlog_path = \"MalGAN_output/adversarial_log.csv\"\npad_percent = 0.1\nthreshold = 0.6\nstep_size = 0.01\nlimit = 0.\ninput_samples = \"MalGAN_input/samplesIn.csv\"\n```", "```\nMalGAN_utils.limit_gpu_memory(limit)\n```", "```\ndf = pd.read_csv(input_samples, header=None)\nfn_list = df[0].values\n```", "```\nmodel = load_model(model_path)\n```", "```\nadv_samples, log = MalGAN_gen_adv_examples.gen_adv_samples(model, fn_list, pad_percent, step_size, threshold)\n```", "```\nlog.save(log_path)\nfor fn, adv in zip(fn_list, adv_samples):\n    _fn = fn.split('/')[-1]\n    dst = os.path.join(save_path, _fn)\n    print(dst)\n    with open(dst, 'wb') as f:\n        f.write(adv)\n```", "```\npip install matplotlib statsmodels scipy\n```", "```\nmonth0 = {\"Trojan\": 24, \"CryptoMiner\": 11, \"Other\": 36, \"Worm\": 29}\nmonth1 = {\"Trojan\": 28, \"CryptoMiner\": 25, \"Other\": 22, \"Worm\": 25}\nmonth2 = {\"Trojan\": 18, \"CryptoMiner\": 36, \"Other\": 41, \"Worm\": 5}\nmonth3 = {\"CryptoMiner\": 18, \"Trojan\": 33, \"Other\": 44, \"Worm\": 5}\nmonths = [month0, month1, month2, month3]\n```", "```\ntrojan_time_series = []\ncrypto_miner_time_series = []\nworm_time_series = []\nother_time_series = []\nfor month in months:\n    trojan_time_series.append(month[\"Trojan\"])\n    crypto_miner_time_series.append(month[\"CryptoMiner\"])\n    worm_time_series.append(month[\"Worm\"])\n    other_time_series.append(month[\"Other\"])\n```", "```\nfrom statsmodels.tsa.arima_model import ARMA\n```", "```\nts_model = ARMA(trojan_time_series, order=(0, 1))\nmodel_fit_to_data = ts_model.fit(disp=True)\ny_Trojan = model_fit_to_data.predict(len(trojan_time_series), len(trojan_time_series))\nprint(\"Trojan prediction for following month: \" + str(y_Trojan[0]) + \"%\")\n```", "```\nTrojan prediction for following month: 21.699999876315772%\n```", "```\nts_model = ARMA(crypto_miner_time_series, order=(0, 1))\nmodel_fit_to_data = ts_model.fit(disp=True)\ny_CryptoMiner = model_fit_to_data.predict(\n    len(crypto_miner_time_series), len(crypto_miner_time_series)\n)\nprint(\"CryptoMiner prediction for following month: \" + str(y_CryptoMiner[0]) + \"%\")\n```", "```\nCryptoMiner prediction for following month: 24.09999979660618%\n```", "```\nts_model = ARMA(worm_time_series, order=(0, 1))\nmodel_fit_to_data = ts_model.fit(disp=True)\ny_Worm = model_fit_to_data.predict(len(worm_time_series), len(worm_time_series))\nprint(\"Worm prediction for following month: \" + str(y_Worm[0]) + \"%\")\n```", "```\nWorm prediction for following month: 14.666665384131406%\n```", "```\nts_model = ARMA(other_time_series, order=(0, 1))\nmodel_fit_to_data = ts_model.fit(disp=True)\ny_Other = model_fit_to_data.predict(len(other_time_series), len(other_time_series))\nprint(\"Other prediction for following month: \" + str(y_Other[0]) + \"%\")\n```", "```\nOther prediction for following month: 27.400000645620793%\n```"]