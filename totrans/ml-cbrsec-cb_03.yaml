- en: Advanced Malware Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be covering more advanced concepts for malware analysis.
    In the previous chapter, we covered general methods for attacking malware classification.
    Here, we will discuss more specific approaches and cutting-edge technologies.
    In particular, we will cover how to approach obfuscated and packed malware, how
    to scale up the collection of N-gram features, and how to use deep learning to
    detect and even create malware.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter comprises the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting obfuscated JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Featurizing PDF files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting N-grams quickly using the hash-gram algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a dynamic malware classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MalConv – end-to-end deep learning for malicious PE detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using packers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assembling a packed sample dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classifier for packers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MalGAN – creating evasive malware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking malware drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical prerequisites for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UPX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statsmodels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code and datasets may be found at [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter03](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter03).
  prefs: []
  type: TYPE_NORMAL
- en: Detecting obfuscated JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to use machine learning to detect when a JavaScript
    file is obfuscated. Doing so can serve to create a binary feature, obfuscated
    or not, to be used in benign/malicious classification, and can serve also as a
    prerequisite step to deobfuscating the scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing the `scikit-learn` package
    in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In addition, obfuscated and non-obfuscated JavaScript files have been provided
    for you in the repository. Extract `JavascriptSamplesNotObfuscated.7z` to a folder
    named `JavaScript Samples`. Extract `JavascriptSamplesObfuscated.7z` to a folder
    named `JavaScript Samples Obfuscated`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will demonstrate how a binary classifier can detect
    obfuscated JavaScript files:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by importing the libraries we will be needing to process the JavaScript''s
    content, prepare the dataset, classify it, and measure the performance of our
    classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the paths of our obfuscated and non-obfuscated JavaScript files and
    assign the two types of file different labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We then read our files into a corpus and prepare labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We split our dataset into a training and testing set, and prepare a pipeline
    to perform basic NLP, followed by a random forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we fit our pipeline to the training data, predict the testing data,
    and then print out our results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy and confusion matrix is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/93e19b60-0a56-4f9c-b550-f3c1de40da7d.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by importing standard Python libraries to analyze the files and set
    up machine learning pipelines (*Step 1*). In *Steps 2* and *3*, we collect the
    non-obfuscated and obfuscated JavaScript files into arrays and assign them their
    respective labels. This is preparation for our binary classification problem.
    Note that the main challenge in producing this classifier is producing a large
    and useful dataset. Ideas for solving this hurdle include collecting a large number
    of JavaScript samples and then using different tools to obfuscate these. Consequently,
    your classifier will likely be able to avoid overfitting to one type of obfuscation.
    Having collected the data, we separate it into training and testing subsets (*Step
    4*). In addition, we set up a pipeline to apply NLP methods to the JavaScript code
    itself, and then train a classifier (*Step 4*). Finally, we measure the performance
    of our classifier in *Step 5*. You will notice that besides the challenge of constructing
    an appropriate dataset, the recipe is similar to the one we used to detect the
    file type.
  prefs: []
  type: TYPE_NORMAL
- en: Featurizing PDF files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to featurize PDF files in order to use them
    for machine learning. The tool we will be utilizing is the `PDFiD` Python script
    designed by *Didier Stevens* ([https://blog.didierstevens.com/](https://blog.didierstevens.com/)).
    Stevens selected a list of 20 features that are commonly found in malicious files,
    including whether the PDF file contains JavaScript or launches an automatic action.
    It is suspicious to find these features in a file, hence, the appearance of these
    can be indicative of malicious behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, the tool scans through a PDF file, and counts the number of occurrences
    of each of the ~20 features. A run of the tool appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The requisite files for this recipe are in the `pdfid` and `PDFSamples` folders
    included in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will featurize a collection of PDF files using
    the `PDFiD` script:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the tool and place all accompanying code in the same directory as featurizing
    PDF `Files.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import IPython''s `io` module so as to capture the output of an external script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to featurize a PDF:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `pdfid` against a file and capture the output of the operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, parse the output so that it is a numerical vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `listdir` to enumerate the files of a folder and specify where you have
    placed your collection of PDFs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through each file in the directory, featurize it, and then collect
    all the feature vectors into `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start our preparation by downloading the `PDFiD` tool and placing our PDF
    files in a convenient location for analysis (*Step 1*). Note that the tool is
    free and simple to use. Continuing, we import the very useful IPython''s `io`
    module in order to capture the results of an external program, namely, `PDFiD`
    (*Step 2*). In the following steps, *Step 3* and *Step 5*, we define a function
    PDF to FV that takes a PDF file and featurizes it. In particular, it utilizes
    the `PDFiD` tool, and then parses its output into a convenient form. When we run
    on the `PDFSamples`\`PythonBrochure.pdf` file, our functions output the following
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we are able to featurize a single PDF file, why not featurize all of
    our PDF files to make these amenable to machine learning (*Steps 6* and *7*).
    In particular, in *Step 6*, we provide a path containing the PDF files we would
    like to featurize, and, in *Step 7*, we perform the actual featurization of the
    files.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting N-grams quickly using the hash-gram algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we demonstrate a technique for extracting the most frequent
    N-grams quickly and memory-efficiently. This allows us to make the challenges
    that come with the immense number of N-grams easier. The technique is called **Hash-Grams**,
    and relies on hashing the N-grams as they are extracted. A property of N-grams
    is that they follow a power law that ensures that hash collisions have an insignificant
    impact on the quality of the features thus obtained.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `nltk` in `pip`. The command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In addition, benign and malicious files have been provided for you in the `PE
    Samples Dataset` folder in the root of the repository. Extract all archives named
    `Benign PE Samples*.7z` to a folder named `Benign PE Samples`, and extract all
    archives named `Malicious PE Samples*.7z` to a folder named `Malicious PE Samples`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will demonstrate how the hash-gram algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by specifying the folders containing our samples, the parameter N, and
    importing a library for hashing and a library to extract N-grams from a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a function to read in the bytes of a file and turn these into N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will want to hash the N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `hash_file_Ngrams_into_dictionary` function takes an N-gram, hashes it,
    and then increments the count in the dictionary for the hash. The reduction module
    B (%B) ensures that there can be no more than `B` keys in the dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify a value for B, the largest prime number smaller than 2^16, and create
    an empty dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We iterate over our files and count their hashed N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We select the most frequent `K1=1000` using `heapq`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the top-hashed N-grams have been selected, these make up the feature set.
    In order to featurize a sample, one iterates over its N-grams, hashes, and reduces
    them, and, if the result is one of the selected top-hashed N-grams, increments
    the feature vector at that index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we featurize our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The initial steps in the hash-gram recipe are similar to the ordinary extraction
    of N-grams. First, we prepare by specifying the folders containing our samples,
    our value of N (as in N-grams). In addition, we import a hashing library, which
    is an action different from the ordinary extraction of N-grams (*Step 1*). Continuing
    our preparation, we define a function to read in all the bytes of a file (as opposed
    to reading in its content) and turn these into N-grams (*Step 2*). We define a
    function to compute the MD5 hash of an N-gram and return the result as a hexadecimal
    number. Additionally, we define a function to convert an N-gram to its byte constituents
    in order to be able to hash it (*Step 3*).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a function to iterate through the hashed N-grams of a file,
    reduce these to modulo B, and then increase the count in the dictionary for the
    reduced hash (*Step 4*). The parameter B controls the limit on how many different
    keys the dictionary will have. By hashing, we are able to randomize the buckets
    that count the N-grams. Now, as we are about to run our functions, it's time to
    specify the value of B. We select the value for B to be the largest prime number
    smaller than 2^16 (*Step 5*).
  prefs: []
  type: TYPE_NORMAL
- en: It is standard to select a prime to ensure that the number of hash collisions
    is minimal. We now iterate through our directory of files and apply the functions
    we have defined previously to each file (*Step 6*). The result is a large dictionary, *T*,
    that contains counts of hashed N-grams. This dictionary is not too big, and we
    easily select the top K1 most common reduced hashes of N-grams from it (*Step
    7*). By doing so, the probability is high that we select the top most frequent
    N-grams, although there may be more than K1 due to hash collisions. At this point,
    we have our feature set, which is N-grams that get mapped by hashing to the K1
    hashed N-grams we have selected. We now featurize our dataset (*Steps 8* and *9*).
    In particular, we iterate through our files, computing their N-grams. If an N-gram
    has a reduced hash that is one of the K1 selected ones, we consider it to be a
    frequent N-gram, and use it as part of our feature set.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the hash-grams algorithm will not always be faster,
    but it is expected to be whenever the datasets under consideration are large.
    In many cases, in a situation where a naive approach to extracting N-grams leads
    to memory error, hash-grams is able to terminate successfully.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For additional details on the hash-gram algorithm, see [https://www.edwardraff.com/publications/hash-grams-faster.pdf](https://www.edwardraff.com/publications/hash-grams-faster.pdf)[.](https://www.edwardraff.com/publications/hash-grams-faster.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Building a dynamic malware classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In certain situations, there is a considerable advantage to being able to detect
    malware based on its behavior. In particular, it is much more difficult for a
    malware to hide its intentions when it is being analyzed in a dynamic situation.
    For this reason, classifiers that operate on dynamic information can be much more
    accurate than their static counterparts. In this section, we provide a recipe
    for a dynamic malware classifier. The dataset we use is part of a VirusShare repository
    from android applications. The dynamic analysis was performed by Johannes Thon
    on several LG Nexus 5 devices with Android API 23, (over 4,000 malicious apps
    were dynamically analyzed on the LG Nexus 5 device farm (API 23), and over 4,300
    benign apps were dynamically analyzed on the LG Nexus 5 device farm (API 23) by
    goorax, used under CC BY / unmodified from the original).
  prefs: []
  type: TYPE_NORMAL
- en: Our approach will be to use N-grams on the sequence of API calls.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn`, `nltk`, and
    `xgboost` in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In addition, benign and malicious dynamic analysis files have been provided
    for you in the repository. Extract all archives named `DA Logs Benign*.7z` to
    a folder named `DA Logs Benign`, and extract all archives named `DA Logs Malware*.7z`
    to a folder named `DA Logs Malicious`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following steps, we demonstrate how a classifier can detect malware based
    on an observed sequence of API calls.
  prefs: []
  type: TYPE_NORMAL
- en: Our logs are in JSON format, so we begin by importing the JSON library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a function to parse the JSON logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We choose to extract the class, method, and type of the API call:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We read our logs into a corpus and collect their labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a look at what the data in our corpus looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We proceed to perform a train-test split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Our approach is to use N-grams, so we load our N-gram extraction functions,
    with a slight modification for the current data format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify N=4 and collect all N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we narrow down to the `K1 = 3000` most frequent N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then write a method to featurize a sample into a vector of N-gram counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply this function to featurize our training and testing samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We use mutual information to further narrow the K1=3000 most frequent N-grams
    to K2=500 most informative N-grams. We then set up a pipeline to subsequently
    run an XGBoost classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We train our pipeline and evaluate its accuracy on the training and testing
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output gives us the training and testing accuracies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we perform something exciting, namely, classification of malware
    and benign samples based on their runtime behavior. Our first three steps are
    to define a function to read in and parse the JSON logs that contain information
    about the samples runtime behavior. As an aside, JSON is a useful file format
    whenever your data might have a variable number of attributes. We make the strategic
    choice to extract the API call class, method, and content. Other features are
    available as well, such as the time at which the API call was made and what arguments
    were called. The trade-off is that the dataset will be larger and these features
    might cause a slowdown or overfit. Investigation is recommended as regards selecting
    additional features for a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: With our function defined, we proceed with performing parsing and collecting
    all of our parsed data in one place (*Step 4*). In *Step 5*, we take a peek at
    our corpus. We see a sample of the quadruples of API calls that make up our data.
    Next is the standard step of performing a training-testing split. In *Steps 7*
    and *8*, we load our N-gram extraction functions and use these to extract N-grams
    from our dataset. These extraction methods are similar to the ones used for binary
    files, but adjusted for the text format at hand. Initially, we collect the K1=3000
    most frequent N-grams in order to reduce the computational load. By increasing
    the numbers K1 and, later on, K2, we can expect the accuracy of our classifier
    to improve, but the memory and computational requirements to increase (*Step 9*).
    In *Step 10*, we define a function to featurize the samples into their N-gram
    feature vectors, and then, in *Step 11*, we apply this function to featurize our
    training and testing samples. We would like to narrow down our feature set further.
    We choose to use mutual information to select the K2=500 most informative N-grams
    from the `K1=3000` most frequent ones (*Step 12*)—there are many options, as discussed
    in the recipe on selecting the best N-grams.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, an alternative choice would have been to use chi-squared. In addition,
    other classifiers aside from XGBoost can be chosen. Finally, we see that the accuracy
    obtained suggests that the approach of using N-grams on the sequence of API calls
    to be promising.
  prefs: []
  type: TYPE_NORMAL
- en: MalConv – end-to-end deep learning for malicious PE detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the new developments in static malware detection has been the use of
    deep learning for end-to-end machine learning for malware detection. In this setting,
    we completely skip all feature engineering; we need not have any knowledge of
    the PE header or other features that may be indicative of PE malware. We simply
    feed a stream of raw bytes into our neural network and train. This idea was first
    suggested in [https://arxiv.org/pdf/1710.09435.pdf](https://arxiv.org/pdf/1710.09435.pdf).
    This architecture has come to be known as **MalConv**, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c9386f51-629c-45b5-90e8-3a6dbd951146.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing a number of packages in `pip`,
    namely, `keras`, `tensorflow`, and `tqdm`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In addition, benign and malicious files have been provided for you in the `PE
    Samples Dataset` folder in the root of the repository. Extract all archives named
    `Benign PE Samples*.7z` to a folder named `Benign PE Samples`, and extract all
    archives named `Malicious PE Samples*.7z` to a folder named `Malicious PE Samples`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we detail how to train MalConv on raw PE files:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import `numpy` for vector operations and `tqdm` to keep track of progress
    in our loops:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to embed a byte as a vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the locations of your raw PE samples and create a list of their labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a convenience function to read in the byte sequence of a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Set a maximum length, `maxSize`, of bytes to read in per sample, embed all
    the bytes of the samples, and gather the result in X:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare an optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Utilize the Keras functional API to set up the deep neural network architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model and choose a batch size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model on batches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by importing `numpy` and `tqdm` (*Step 1*), a package that allows you
    to keep track of progress in a loop by showing a percentage progress bar. As part
    of feeding the raw bytes of a file into our deep neural network, we use a simple
    embedding of bytes in an 8-dimensional space, in which each bit of the byte corresponds
    to a coordinate of the vector (*Step 2*). A bit equal to 1 means that the corresponding
    coordinate is set to 1/16, whereas a bit value of 0 corresponds to a coordinate
    equal to -1/16\. For example, 10010001 is embedded as the vector (1/16, -1/16,
    -1/16, 1/16, -1/16, -1/16, -1/16, 1/16). Other ways to perform embeddings, such
    as ones that are trained along with the neural network, are possible.
  prefs: []
  type: TYPE_NORMAL
- en: The MalConv architecture makes a simple, but computationally fast, choice. In
    *Step 3*, we list our samples and their labels, and, in *Step 4*, we define a
    function to read the bytes of a file. Note the `rb` setting in place of `r`, so
    as to read the file as a byte sequence. In *Step 5*, we use `tqdm` to track the
    progress of the loop. For each file, we read in the byte sequence and embed each
    byte into an 8-dimensional space. We then gather all of these into *X*. If the
    number of bytes exceeds `maxSize=15000`, then we stop. If the number of bytes
    is smaller than maxSize, then the bytes are assumed to be 0's. The `maxSize` parameter,
    which controls how many bytes we read per file, can be tuned according to memory
    capacity, the amount of computation available, and the size of the samples. In
    the following steps (*Steps 6* and *7*), we define a standard optimizer, namely,
    a stochastic gradient descent with a selection of parameters, and define the architecture
    of our neural network to match closely with that of MalConv. Note that we have
    used the Keras functional API here, which allows us to create non-trivial, input-output
    relations in our model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that better architectures and choices of parameters are an open
    area of research. Continuing, we are now free to select a batch size and begin
    training (*Steps 8* and *9*). The batch size is an important parameter that can
    affect both speed and stability of the learning process. For our purposes, we
    have made a simple choice. We feed in a batch at a time, and train our neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling packed malware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Packing is the compression or encryption of an executable file, distinguished
    from ordinary compression in that it is typically decompressed during runtime,
    in memory, as opposed to being decompressed to disk, prior to execution. Packers
    pose an obfuscation challenge to analysts.
  prefs: []
  type: TYPE_NORMAL
- en: A packer called VMProtect, for example, protects its content from analyst eyes
    by executing in a virtual environment with a unique architecture, making it a
    great challenge for anyone to analyze the software.
  prefs: []
  type: TYPE_NORMAL
- en: Amber is a reflective PE packer for bypassing security products and mitigations.
    It can pack regularly compiled PE files into reflective payloads that can load
    and execute themselves like a shellcode. It enables stealthy in-memory payload
    deployment that can be used to bypass anti-virus, firewall, IDS, IPS products,
    and application whitelisting mitigations. The most commonly used packer is UPX.
  prefs: []
  type: TYPE_NORMAL
- en: Since packing obfuscates code, it can often result in a decrease in the performance
    of a machine learning classifier. By determining which packer was used to pack
    an executable, we can then utilize the same packer to unpack the code, that is,
    revert the code to its original, non-obfuscated version. Then, it becomes simpler
    for both antivirus and machine learning to detect whether the file is malicious.
  prefs: []
  type: TYPE_NORMAL
- en: Using packers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will show how to obtain a packer, namely UPX, and how to
    use it. The purpose of having a collection of packers is, firstly, to perform
    data augmentation as will be detailed in the remainder of the recipe, and, secondly,
    to be able to unpack a sample once the packer used to pack it is determined.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are no packages required for the following recipe. You may find `upx.exe`
    in the `Packers` folder of the repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, you will utilize the UPX packer to pack a file:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and unarchive the latest version of UPX from [https://github.com/upx/upx/releases/](https://github.com/upx/upx/releases/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/b1b91e18-66dd-4bf4-bb13-75826c3680b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Execute `upx.exe` against the file you wish to pack by running `upx.exe` and
    `foofile.exe`. The result of a successful packing appears as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/6695ad88-cfd4-4a68-8887-e3b997d847ca.png)'
  prefs: []
  type: TYPE_IMG
- en: The file remains an executable, unlike in the case of archives, which become
    zipped.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, using a packer is very simple. One of the benefits of most packers
    is that they reduce the size of the file, in addition to obfuscating its content.
    Many hackers utilize custom-made packers. The advantage of these is that they
    are difficult to unpack. From the standpoint of detecting malicious files, a file
    that is packed using a custom packer is highly suspicious.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling a packed sample dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One obvious way in which to assemble a dataset for a packer classifier is to
    collect samples that have been packed and whose packing has been labeled. Another
    fruitful way in which to assemble packed samples is to collect a large dataset
    of files and then pack these yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are no packages required for the following recipe. You may find `upx.exe`
    in the `Packers` folder of the repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will use UPX to pack a directory of files.
  prefs: []
  type: TYPE_NORMAL
- en: Place `upx.exe` in a directory, `A`, and place a collection of samples in a
    directory, `B`, in `A`. For this example, `B` is Benign PE Samples UPX.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'List the files of directory `B`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `upx` against each file in `B`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever an error occurs in packing, remove the original sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first two steps are preparation for running our UPX packer. In *Step 3*,
    we use a subprocess to call an external command, namely UPX, in Python. As we
    pack our samples (*Step 4*), whenever an error occurs, we remove the sample, as
    it cannot be packed successfully. This ensures that our directory contains nothing
    but packed samples, so that we can feed in clean and organized data to our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Building a classifier for packers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having assembled the labeled data, consisting of packed samples in directories
    labeled according to the packer, we are ready to train a classifier to determine
    whether a sample was packed, and, if so, by which packer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn` and `nltk` in
    `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, packed and non-packed files have been provided for you in the
    repository. In this recipe, three types of samples are used: unpacked, UPX packed,
    and Amber packed. Extract all archives named `Benign PE Samples*.7z` from `PE
    Samples Dataset` in the root of the repository to a folder named `Benign PE Samples`,
    extract `Benign PE Samples UPX.7z` to a folder named `Benign PE Samples UPX`,
    and extract `Benign PE Samples Amber.7z` to a folder named `Benign PE Samples
    Amber`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, you will build a classifier to determine which packer was used
    to pack a file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read in the names of the files to be analyzed along with their labels, corresponding
    to the packer used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a train-test split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the imports needed to extract N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the functions to be used in extracting N-grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass through the data, and select the N-grams you wish to utilize as your features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Featurize the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a random forest model on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Featurize the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Utilize the trained classifier to predict on the testing set, and assess the
    performance using a confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9f4cfd98-7261-4d18-b696-8831f90b68f6.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start simply by organizing our data and labels into arrays (*Step 1*). In
    particular, we read in our samples and give them the label corresponding to the
    packer with which they have been packed. In *Step 2*, we train-test split our
    data. We are now ready to featurize our data, so we import the requisite libraries
    for N-gram extraction, as well as define our N-gram functions (*Steps 3* and *4*),
    which are discussed in other recipes, and, making a simplifying choice of *N=2*
    and the *K1=100* most frequent N-grams as our features, featurize our data (*Steps
    5* and *6*). Different values of N and other methods of selecting the most informative
    N-grams can yield superior results, while increasing the need for computational
    resources. Having featurized the data, we train-test split it (*Step 7*) and then
    train a random forest classifier (a simple first choice) on the data (*Step 8*).
    Judging by the confusion matrix in *Step 9*, we see that a machine learning classifier
    performs very accurately on this type of problem.
  prefs: []
  type: TYPE_NORMAL
- en: MalGAN – creating evasive malware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using **Generative Adversarial Networks** (**GANs**), we can create adversarial
    malware samples to train and improve our detection methodology, as well as to
    identify gaps before an adversary does. The code here is based on **j40903272/MalConv-keras**.
    The adversarial malware samples are malware samples that have been modified by
    padding them with a small, but carefully calculated, sequence of bytes, selected
    so as to fool the neural network (in this case, MalConv) being used to classify
    the samples.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing the `pandas`, `keras`, `tensorflow`,
    and `scikit-learn` packages in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The associated code and resource files for `MalGan` have been included in the
    repository for this book, in the `MalGan` directory. In addition, assemble a collection
    of PE samples and then place their paths in the first column of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: In the second column, type in these samples' verdicts (1 for benign and 0 for
    malicious).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, you will learn how to create adversarial malware:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin by importing the code for MalGAN, as well as some utility libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the input and output paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Set whether you''d like to use a GPU for adversarial sample generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the csv file containing the names and labels of your samples into a
    data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the pre-computed MalConv model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the **Fast Gradient Step Method** (**FGSM**) to generate adversarial malware:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Save a log of the results and write the samples to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by importing all the MalGAN code that we will be using (*Step 1*).
    We must specify a few arguments (*Step 2*), to be explained now. The `savePath`
    parameter is the location into which the adversarial examples will be saved. The
    `modelPath` variable is the path to the pre-computed weights of MalConv. The `logPath`
    parameter is where data pertaining to the application of the **Fast Gradient Signed
    Method** (**FGSM**) to the sample is recorded. For example, a log file may appear
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **filename** | **original score** | **file length** | **pad length** | **loss**
    | **predict score** |'
  prefs: []
  type: TYPE_TB
- en: '| 0778...b916 | 0.001140 | 235 | 23 | 1 | 0.912 |'
  prefs: []
  type: TYPE_TB
- en: 'Observe that the original score is close to `0`, indicating that the original
    sample is considered malicious by MalConv. After selecting which bytes to use
    to pad, the final prediction score is close to `1`, indicating that the modified
    sample is now considered benign. The `padPercent` parameter determines how many
    bytes are appended to the end of a sample. The `threshold` parameter determines
    how certain the neural network should be in the adversarial example being benign
    for it to be written to disk. `stepSize` is a parameter used in the FGSM. That''s
    all for parameters at this stage. We still have another choice to make, which
    is whether to use a CPU or GPU (*Step 3*). We make the choice of using a CPU in
    this recipe for simplicity. Obviously, a GPU would make computations faster. The
    `limit` parameter here indicates how much GPU to use in the computation, and is
    set to `0`. In the next step, *Step 4*, we read in the `.csv` file pointed to
    by the `inputSamples` parameter. This input log takes a format such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2b5137a1658c...8 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0778a070b28...6 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Here, in the first column, the path of a sample is given, and, in the second
    column, a label is provided (`1` for benign and `0` for malicious). We now load
    the precomputed MalGAN model (*Step 5*), generate adversarial malware samples
    (*Step 6*), and then save them onto disk (*Step 7*).
  prefs: []
  type: TYPE_NORMAL
- en: Tracking malware drift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The distribution of malware is ever-changing. Not only are new samples released,
    but new types of viruses as well. For example, cryptojackers are a relatively
    recent breed of malware unknown until the advent of cryptocurrency. Interestingly,
    from a machine learning perspective, it's not only the types and distribution
    of malware that are evolving, but also their definitions, something known as **concept
    drift**. To be more specific, a 15 year-old virus is likely no longer executable
    in the systems currently in use. Consequently, it cannot harm a user, and is therefore
    no longer an instance of malware.
  prefs: []
  type: TYPE_NORMAL
- en: By tracking the drift of malware, and even predicting it, an organization is
    better able to channel its resources to the correct type of defense, inoculating
    itself from future threats.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing the `matplotlib`, `statsmodels`,
    and `scipy` packages in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, you will use a regression on time series to predict the distribution
    of malware based on historical data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Collect historical data on the distribution of malware in your domain of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the data into a separate time series for each class of malware:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the time series for Trojan:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/61629ac3-ea5d-45c7-b8d7-ffe33e2b62fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph shows the time series for CryptoMiners:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f79a4bd3-3510-4acd-bab4-aeb75c6987cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph shows the time series for Worms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/db09ce4d-c553-465a-8b5c-d3e482b2c8c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph shows the time series for other types of malware:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8980a23c-40c6-49c7-bbb8-43fbf12e4af7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Import the moving average from `statsmodels`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Predict the following month's distribution based on the time series using the
    moving average.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The result for Trojans is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the same method for Cryptominers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of Worms, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'For other types of Malware, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For instructive purposes, we produce a toy dataset representing the percentage
    of each type of malware in time (*Step 1*). With a larger amount of historical
    data, such a dataset can indicate where to channel your resources in the domain
    of security. We collect the data in one place and produce visualization plots
    (*Step 2*). We would like to perform simple forecasting, so we import ARMA, which
    stands for *autoregressive–moving-average model*, and is a generalization of the
    moving-average model. For simplicity, we specialize ARMA to **moving average**
    (**MA**). In *Step 4*, we employ MA to make a prediction on how the percentages
    of malware will evolve to the next time period. With a larger dataset, it is prudent
    to attempt different models, as well as create a train-test split that accounts
    for time. This will allow you to find the most explanatory model, in other words,
    the model that produces the most accurate time forecasts.
  prefs: []
  type: TYPE_NORMAL
