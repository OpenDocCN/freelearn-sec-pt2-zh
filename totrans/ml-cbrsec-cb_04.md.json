["```\npip install tweepy markovify\n```", "```\nimport json\nimport tweepy\n\nCONSUMER_API_KEY = \"fill me in\"\nCONSUMER_API_SECRET_KEY = \"fill me in\"\nACCESS_TOKEN = \"fill me in\"\nACCESS_TOKEN_SECRET = \"fill me in\"\n\nauth = tweepy.OAuthHandler(CONSUMER_API_KEY, CONSUMER_API_SECRET_KEY)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\napi = tweepy.API(\n    auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True\n)\n```", "```\nuser_id = \"elonmusk\"\n```", "```\ncount = 200\nuser_tweets = api.user_timeline(screen_name=user_id, count=count, tweet_mode=\"extended\")\n```", "```\ntweet_corpus = []\nfor tweet in user_tweets:\n    tweet_corpus.append(tweet.full_text)\ntweets_text = \". \".join(tweet_corpus)\n```", "```\nimport re\n\ndef replace_URLs(string, new_URL):\n    \"\"\"Replaces all URLs in a string with a custom URL.\"\"\"\n    modified_string = re.sub(\n        \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n        \" \" + new_URL + \" \",\n        string,\n    )\n    return modified_string\n```", "```\nphishing_link = \"https://urlzs.com/u8ZB\"\nprocessed_tweets_text = replace_URLs(tweets_text, phishing_link)\n```", "```\nimport markovify\n\nmarkov_model = markovify.Text(processed_tweets_text)\n```", "```\nnum_phishing_tweets_desired = 5\nnum_phishing_tweets_so_far = 0\ngenerated_tweets = []\nwhile num_phishing_tweets_so_far < num_phishing_tweets_desired:\n    tweet = markov_model.make_short_sentence(140)\n    if phishing_link in tweet and tweet not in generated_tweets:\n        generated_tweets.append(tweet)\n        num_phishing_tweets_so_far += 1\n```", "```\nuser = api.get_user(user_id)\nfor friend in user.friends():\n    print(friend.screen_name)\n```", "```\nwonderofscience\nSpaceComCC\nAFSpace\nLiv_Boeree\nshivon\nTeslarati\nneiltyson\nSciGuySpace\nwlopwangling\nBerger_SN\npewdiepie\nCathieDWood\nlexfridman\nccsakuras\n4thFromOurStar\nTheOnion\nBBCScienceNews\nsciencemagazine\nNatureNews\nTheStoicEmperor\n```", "```\npip install torch librosa\n```", "```\nimport math\nfrom torch.autograd import Variable\nfrom voice_impersonation_utils import *\nfrom voice_impersonation_model import *\n```", "```\ninput_files = \"voice_impersonation_input/\"\ncontent_file = input_files + \"male_voice.wav\"\nstyle_file = input_files + \"Eleanor_Roosevelt.wav\"\n```", "```\naudio_content, sampling_rate = wav2spectrum(content_file)\naudio_style, sampling_rate = wav2spectrum(style_file)\naudio_content_torch = torch.from_numpy(audio_content)[None, None, :, :]\naudio_style_torch = torch.from_numpy(audio_style)[None, None, :, :]\n```", "```\nvoice_impersonation_model = RandomCNN()\nvoice_impersonation_model.eval()\n```", "```\naudio_content_variable = Variable(audio_content_torch, requires_grad=False).float()\naudio_style_variable = Variable(audio_style_torch, requires_grad=False).float()\naudio_content = voice_impersonation_model(audio_content_variable)\naudio_style = voice_impersonation_model(audio_style_variable)\n\nlearning_rate = 0.003\naudio_G_var = Variable(\n    torch.randn(audio_content_torch.shape) * 1e-3, requires_grad=True\n)\nopt = torch.optim.Adam([audio_G_var])\n```", "```\nstyle_param = 1\ncontent_param = 5e2\n\nnum_epochs = 500\nprint_frequency = 50\n```", "```\nfor epoch in range(1, num_epochs + 1):\n    opt.zero_grad()\n    audio_G = voice_impersonation_model(audio_G_var)\n\n    content_loss = content_param * compute_content_loss(audio_content, audio_G)\n    style_loss = style_param * compute_layer_style_loss(audio_style, audio_G)\n    loss = content_loss + style_loss\n    loss.backward()\n    opt.step()\n```", "```\n    if epoch % print_frequency == 0:\n        print(\"epoch: \"+str(epoch))\n        print(\"content loss: \"+str(content_loss.item()))\n        print(\"style loss: \"+str(style_loss.item()))\n        print(\"loss: \"+str(loss.item()))\n\ngen_spectrum = audio_G_var.cpu().data.numpy().squeeze()\noutput_audio_name = \"Eleanor_saying_there_was_a_change_now.wav\"\nspectrum2wav(gen_spectrum, sampling_rate, output_audio_name)\n```", "```\npip install speechrecognition\n```", "```\nimport speech_recognition\n\nlist_of_audio_files = [\"Eleanor_Roosevelt.wav\", \"Comey.wav\"]\nkeywords = [\"Twitter\", \"Linkedin\", \"Facebook\", \"Instagram\", \"password\", \"FBI\"]\n```", "```\ndef transcribe_audio_file_to_text(audio_file):\n    \"\"\"Takes an audio file and produces a text transcription.\"\"\"\n    recognizer = speech_recognition.Recognizer()\n    with speech_recognition.AudioFile(audio_file) as audio_source:\n        audio = recognizer.record(audio_source)\n        return recognizer.recognize_google(audio)\n```", "```\naudio_corpus = {}\nfor audio_file in list_of_audio_files:\n    audio_corpus[transcribe_audio_file_to_text(audio_file)] = audio_file\n\nprint(audio_corpus)\n```", "```\n{\"I'm very glad to be able to take part in this celebration dim sum Direct on human rights day\": 'Eleanor_Roosevelt.wav', \"have you met read recently that I'm on Twitter I am not a tweeter I am there to listen to read especially what's being said about the FBI and its mission\": 'Comey.wav'}\n```", "```\nfor keyword in keywords:\n    for transcription in audio_corpus:\n        if keyword in transcription:\n            print(\n                \"keyword \"\n                + keyword\n                + \" found in audio \"\n                + '\"'\n                + audio_corpus[transcription]\n                + '\"'\n            )\n```", "```\nkeyword Twitter found in audio \"Comey.wav\"\nkeyword FBI found in audio \"Comey.wav\"\n```", "```\npip install face_recognition opencv-python\n```", "```\nimport face_recognition\n```", "```\nknown_image = face_recognition.load_image_file(\"trump_official_portrait.jpg\")\n```", "```\nunknown_image = face_recognition.load_image_file(\"trump_and_others.jpg\")\n```", "```\ntrump_encoding = face_recognition.face_encodings(known_image)[0]\n```", "```\nunknown_faces = face_recognition.face_encodings(unknown_image)\n```", "```\nmatches = face_recognition.compare_faces(unknown_faces, trump_encoding)\nprint(matches)\n```", "```\n[False, False, False, True]\n```", "```\nface_locations = face_recognition.face_locations(unknown_image)\ntrump_face_location = face_locations[3]\n```", "```\nimport cv2\nunknown_image_cv2 = cv2.imread(\"trump_and_others.jpg\")\n```", "```\n(top, right, bottom, left) = trump_face_location\ncv2.rectangle(unknown_image_cv2, (left, top), (right, bottom), (0, 0, 255), 2)\n```", "```\ncv2.rectangle(unknown_image_cv2, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\nfont = cv2.FONT_HERSHEY_DUPLEX\ncv2.putText(unknown_image_cv2, \"Trump\", (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n```", "```\ncv2.namedWindow('image', cv2.WINDOW_NORMAL)\ncv2.imshow('image',unknown_image_cv2)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```", "```\npip install opencv-python dlib scipy\n```", "```\nimport os\nimport cv2\nimport numpy as np\nfrom deepfake_face_detection import select_face\nfrom deepfake_face_swap import (\n    warp_image_2d,\n    warp_image_3d,\n    mask_from_points,\n    apply_mask,\n    correct_colours,\n    transformation_from_points,\n    ProcessFace,\n)\n```", "```\ncontent_image = \"deepfake_input/author.jpg\"\ntarget_image = \"deepfake_input/gymnast.jpg\"\nresult_image_path = \"deepfake_results/author_gymnast.jpg\"\n```", "```\ncontent_img = cv2.imread(content_image)\ndestination_img = cv2.imread(target_image)\ncontent_img_points, content_img_shape, content_img_face = select_face(content_img)\ndestination_img_points, destination_img_shape, destination_img_face = select_face(\n    destination_img\n)\n```", "```\nresult_image = ProcessFace(\n    content_img_points, content_img_face, destination_img_points, destination_img_face\n)\n```", "```\nx, y, w, h = destination_img_shape\ndestination_img_copy = destination_img.copy()\ndestination_img_copy[y : y + h, x : x + w] = result_image\nresult_image = destination_img_copy\ncv2.imwrite(result_image_path, result_image)\n```", "```\npip install keras tensorflow pillow\n```", "```\nfrom mesonet_classifiers import *\nfrom keras.preprocessing.image import ImageDataGenerator\n```", "```\nMesoNet_classifier = Meso4()\nMesoNet_classifier.load(\"mesonet_weights/Meso4_DF\")\n```", "```\nimage_data_generator = ImageDataGenerator(rescale=1.0 / 255)\ndata_generator = image_data_generator.flow_from_directory(\n    \"\", classes=[\"mesonet_test_images\"]\n)\n```", "```\nFound 3 images belonging to 1 classes.\n```", "```\nnum_to_label = {1: \"real\", 0: \"fake\"}\n```", "```\nX, y = data_generator.next()\nprobabilistic_predictions = MesoNet_classifier.predict(X)\npredictions = [num_to_label[round(x[0])] for x in probabilistic_predictions]\nprint(predictions)\n```", "```\n['real', 'fake', 'fake']\n```", "```\n         x = Input(shape = (IMGWIDTH, IMGWIDTH, 3))\n         x1 = Conv2D(8, (3, 3), padding='same', activation = 'relu')(x)\n         x1 = BatchNormalization()(x1)\n         x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n\n         x2 = Conv2D(8, (5, 5), padding='same', activation = 'relu')(x1)\n         x2 = BatchNormalization()(x2)\n         x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n\n         x3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n         x3 = BatchNormalization()(x3)\n         x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n\n         x4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n         x4 = BatchNormalization()(x4)\n         x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n\n         y = Flatten()(x4)\n         y = Dropout(0.5)(y)\n         y = Dense(16)(y)\n         y = LeakyReLU(alpha=0.1)(y)\n         y = Dropout(0.5)(y)\n         y = Dense(1, activation = 'sigmoid')(y)\n```", "```\npip install -r requirements.txt\n```", "```\nPython application.py \n```", "```\npip install ibm-watson\n```", "```\nfrom ibm_watson import PersonalityInsightsV3\nfrom datetime import date\n\nv = str(date.today())\napi_key = \"fill me in\"\n```", "```\npersonality_insights_service = PersonalityInsightsV3(version=v, iam_apikey=api_key)\n```", "```\ntweets_file = \"ElonMuskTweets.txt\"\n```", "```\nwith open(tweets_file) as input_file:\n    profile = personality_insights_service.profile(\n        input_file.read(),\n        \"application/json\",\n        raw_scores=False,\n        consumption_preferences=True,\n    ).get_result()\n```", "```\nimport json\n\nprint(json.dumps(profile, indent=2))\n\n{ \"word_count\": 2463, \"processed_language\": \"en\", \"personality\": [ { \"trait_id\": \"big5_openness\", \"name\": \"Openness\", \"category\": \"personality\", \"percentile\": 0.7417085532819794, \"significant\": true, \"children\": [ { \"trait_id\": \"facet_adventurousness\", \"name\": \"Adventurousness\", \"category\": \"personality\", \"percentile\": 0.9589655282562557, \"significant\": true }, { \"trait_id\": \"facet_artistic_interests\", \"name\": \"Artistic interests\", \"category\": \"personality\", \"percentile\": 0.44854779978198406, \"significant\": true }, { \"trait_id\": \"facet_emotionality\", \"name\": \"Emotionality\", \"category\": \"personality\", \"percentile\": 0.0533351337262023, \"significant\": true },\n <snip>\n \"consumption_preference_id\": \"consumption_preferences_books_financial_investing\", \"name\": \"Likely to read financial investment books\", \"score\": 0.0 }, { \"consumption_preference_id\": \"consumption_preferences_books_autobiographies\", \"name\": \"Likely to read autobiographical books\", \"score\": 1.0 } ] }, { \"consumption_preference_category_id\": \"consumption_preferences_volunteering\", \"name\": \"Volunteering Preferences\", \"consumption_preferences\": [ { \"consumption_preference_id\": \"consumption_preferences_volunteer\", \"name\": \"Likely to volunteer for social causes\", \"score\": 0.0 } ] } ], \"warnings\": [] }\n```", "```\n global linkedin_username\n global linkedin_password\n linkedin_username = \"\"\n linkedin_password = \"\"\n global facebook_username\n global facebook_password\n facebook_username = \"\"\n facebook_password = \"\"\n global twitter_username\n global twitter_password\n twitter_username = \"FILL ME IN\"\n twitter_password = \"FILL ME IN\"\n global instagram_username\n global instagram_password\n instagram_username = \"\"\n instagram_password = \"\"\n global google_username\n global google_password\n google_username = \"\"\n google_password = \"\"\n global vk_username\n global vk_password\n```", "```\n Python social_mapper.py -f imagefolder -I ./Input-Examples/imagefolder -m fast -tw \n```", "```\npip install keras tensorflow\n```", "```\nwith open(\"airport_reviews_short.csv\", encoding=\"utf-8\") as fp:\n    reviews_text = fp.read()\n```", "```\nchars_list = sorted(list(set(reviews_text)))\nchar_to_index_dict = {\n    character: chars_list.index(character) for character in chars_list\n}\n```", "```\n{' ': 0, '!': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '-': 6, '.': 7, '/': 8, '2': 9, '5': 10, '<': 11, '>': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'L': 23, 'M': 24, 'O': 25, 'R': 26, 'S': 27, 'T': 28, 'U': 29, 'W': 30, 'a': 31, 'b': 32, 'c': 33, 'd': 34, 'e': 35, 'f': 36, 'g': 37, 'h': 38, 'i': 39, 'j': 40, 'k': 41, 'l': 42, 'm': 43, 'n': 44, 'o': 45, 'p': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54}\n```", "```\nimport keras\nfrom keras import layers\n\nmax_length = 40\nrnn = keras.models.Sequential()\nrnn.add(\n    layers.LSTM(1024, input_shape=(max_length, len(chars_list)), return_sequences=True)\n)\nrnn.add(layers.LSTM(1024, input_shape=(max_length, len(chars_list))))\nrnn.add(layers.Dense(len(chars_list), activation=\"softmax\"))\n```", "```\noptimizer = keras.optimizers.SGD(lr=0.01, decay=1e-6, nesterov=True)\nrnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n```", "```\nimport numpy as np\n\ndef text_to_vector(input_txt, max_length):\n    \"\"\"Reads in the text and vectorizes it.\n    X will consist of consecutive sequences of characters. \n    Y will consist of the next character.\n    \"\"\"\n    sentences = []\n    next_characters = []\n    for i in range(0, len(input_txt) - max_length):\n        sentences.append(input_txt[i : i + max_length])\n        next_characters.append(input_txt[i + max_length])\n    X = np.zeros((len(sentences), max_length, len(chars_list)))\n    y = np.zeros((len(sentences), len(chars_list)))\n    for i, sentence in enumerate(sentences):\n        for t, char in enumerate(sentence):\n            X[i, t, char_to_index_dict[char]] = 1\n            y[i, char_to_index_dict[next_characters[i]]] = 1\n    return [X, y]\n```", "```\nX, y = text_to_vector(reviews_text, max_length)\nrnn.fit(X, y, batch_size=256, epochs=1)\n```", "```\nrnn.save_weights(\"weights.hdf5\")\n```", "```\npip install keras tensorflow\n```", "```\nimport keras\nfrom keras import layers\n```", "```\nchar_indices = dict((char, chars.index(char)) for char in chars) \n```", "```\ntext = open(\"seed_text.txt\").read()\nmax_length = 40\n```", "```\nrnn = keras.models.Sequential()\nrnn.add(\n    layers.LSTM(1024, input_shape=(max_length, len(chars_list)), return_sequences=True)\n)\nrnn.add(layers.LSTM(1024, input_shape=(max_length, len(chars_list))))\nrnn.add(layers.Dense(len(chars_list), activation=\"softmax\"))\nrnn.load_weights(\"weights.hdf5\")\noptimizer = keras.optimizers.SGD(lr=0.01, decay=1e-6, nesterov=True)\nrnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n```", "```\nimport numpy as np\n\ndef sample_next_char(preds):\n    \"\"\"Samples the subsequent character based on a probability distribution.\"\"\"\n    return np.random.choice(chars_list, p=preds)\n```", "```\nimport sys\n\nstart_index = np.random.randint(0, len(text) - max_length - 1)\ngenerated_text = text[start_index : start_index + max_length]\nsys.stdout.write(generated_text)\nsentence_length = 1000\nfor i in range(sentence_length):\n    vec_so_far = np.zeros((1, max_length, len(chars_list)))\nfor t, char in enumerate(generated_text):\n    vec_so_far[0, t, char_to_index_dict[char]] = 1.0\npreds = rnn.predict(vec_so_far)[0]\nnext_char = sample_next_char(preds)\ngenerated_text += next_char\ngenerated_text = generated_text[1:]\nsys.stdout.write(next_char)\nsys.stdout.flush()\nprint(generated_text)\n```", "```\npip install pandas sklearn\n```", "```\nimport pandas as pd\n\ncolumns = [\n    \"text\",\n    \"language\",\n    \"thread_title\",\n    \"spam_score\",\n    \"replies_count\",\n    \"participants_count\",\n    \"likes\",\n    \"comments\",\n    \"shares\",\n    \"type\",\n]\ndf = pd.read_csv(\"fake_news_dataset.csv\", usecols=columns)\n```", "```\ndf = df[df[\"language\"] == \"english\"]\ndf = df.dropna()\ndf = df.drop(\"language\", axis=1\n```", "```\nfeatures = 0\nfeature_map = {}\n\ndef add_feature(name):\n    \"\"\"Adds a feature to the dictionary of features.\"\"\"\n    if name not in feature_map:\n        global features\n        feature_map[name] = features\n        features += 1\n```", "```\nadd_feature(\"fake\")\nadd_feature(\"real\")\n```", "```\ndef article_type(row):\n    \"\"\"Binarizes target into fake or real.\"\"\"\n    if row[\"type\"] == \"fake\":\n        return feature_map[\"fake\"]\n    else:\n        return feature_map[\"real\"]\n```", "```\ndf[\"type\"] = df.apply(article_type, axis=1)\n```", "```\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df)\n```", "```\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_text = TfidfVectorizer()\nvectorizer_title = TfidfVectorizer()\n```", "```\nvectorized_text = vectorizer_text.fit_transform(df_train.pop(\"text\").values)\nvectorized_title = vectorizer_title.fit_transform(df_train.pop(\"thread_title\").values\n```", "```\nfrom scipy import sparse\n\nspam_score_train = sparse.csr_matrix(df_train[\"spam_score\"].values).transpose()\nreplies_count_train = sparse.csr_matrix(df_train[\"replies_count\"].values).transpose()\nparticipants_count_train = sparse.csr_matrix(\n    df_train[\"participants_count\"].values\n).transpose()\nlikes_train = sparse.csr_matrix(df_train[\"likes\"].values).transpose()\ncomments_train = sparse.csr_matrix(df_train[\"comments\"].values).transpose()\nshares_train = sparse.csr_matrix(df_train[\"shares\"].values).transpose()\n```", "```\nfrom scipy.sparse import hstack\n\nX_train = hstack(\n    [\n        vectorized_text,\n        vectorized_title,\n        spam_score_train,\n        replies_count_train,\n        participants_count_train,\n        likes_train,\n        comments_train,\n        shares_train,\n    ]\n)\ny_train = df_train.pop(\"type\").values\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n```", "```\nvectorized_text_test = vectorizer_text.transform(df_test.pop(\"text\").values)\nvectorized_title_test = vectorizer_title.transform(df_test.pop(\"thread_title\").values)\n```", "```\nspam_score_test = sparse.csr_matrix(df_test[\"spam_score\"].values).transpose()\nreplies_count_test = sparse.csr_matrix(df_test[\"replies_count\"].values).transpose()\nparticipants_count_test = sparse.csr_matrix(\n    df_test[\"participants_count\"].values\n).transpose()\nlikes_test = sparse.csr_matrix(df_test[\"likes\"].values).transpose()\ncomments_test = sparse.csr_matrix(df_test[\"comments\"].values).transpose()\nshares_test = sparse.csr_matrix(df_test[\"shares\"].values).transpose()\nX_test = hstack(\n    [\n        vectorized_text_test,\n        vectorized_title_test,\n        spam_score_test,\n        replies_count_test,\n        participants_count_test,\n        likes_test,\n        comments_test,\n        shares_test,\n    ]\n)\ny_test = df_test.pop(\"type\").values\n```", "```\nclf.score(X_test, y_test)\n```", "```\n0.9977324263038548\n```"]