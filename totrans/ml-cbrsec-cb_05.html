<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Penetration Testing Using Machine Learning</h1>
                </header>
            
            <article>
                
<p><span>A penetration test, aka a pen test, is an authorized simulated cyberattack on an information system, designed to elicit security vulnerabilities. In this chapter, we will be covering a wide selection of machine learning-technologies for penetration testing and security countermeasures. We'll begin by cracking a simple CAPTCHA system. We'll cover the automatic discovery of software vulnerabilities using deep learning, using fuzzing and code gadgets. We'll demonstrate enhancements to Metasploit, as well as covering how to assess the robustness of machine learning systems to adversarial attacks. Finally, we'll cover more specialized topics, such as deanonymizing Tor traffic, recognizing unauthorized access via keystroke dynamics, and detecting malicious URLs.<br/></span></p>
<p class="mce-root">This chapter covers the following recipes:</p>
<ul>
<li class="mce-root">CAPTCHA breaker</li>
<li class="mce-root"><span>Neural network-assisted fuzzing</span></li>
<li class="mce-root">DeepExploit</li>
<li class="mce-root">Web server vulnerability scanner using machine learning (GyoiThon)</li>
<li class="mce-root">Deanonymizing Tor using machine learning</li>
<li class="mce-root"><span><strong>Internet of Things</strong> (</span><strong>IoT</strong>) device type identification using machine learning</li>
<li class="mce-root">Keystroke dynamics</li>
<li class="mce-root">Malicious URL detector</li>
<li class="mce-root">Deep-pwning</li>
<li class="mce-root">Deep learning-based system for the automatic detection of software vulnerabilities (VulDeePecker)</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be using the following:</p>
<ul>
<li>TensorFlow</li>
<li>Keras</li>
<li>OpenCV</li>
<li>Google API Client</li>
<li>Censys</li>
<li>NetworkX</li>
<li>Tldextract</li>
<li>dpkt</li>
<li>NumPy</li>
<li>SciPy</li>
<li>Xlib</li>
<li>Gensim</li>
</ul>
<p><span>The code and datasets can be found at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter05">https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter05</a></span>.<span><a href="https://github.com/emmanueltsukerman/MLforCSCookbook"/></span><span><a href="https://github.com/emmanueltsukerman/MLforCSCookbook"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CAPTCHA breaker</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">A <strong>CAPTCHA</strong> is a system intended to prevent automated access or scraping. It does so by asking questions that are meant to recognize when the user is a human and when the user is a program. You have probably seen countless variations of the following screenshot:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/2d535842-1285-4150-aa14-470cc388ae91.png" style="width:10.00em;height:4.42em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">Sometimes, the request is to insert a code, sometimes it is to select some objects, for example, storefronts or traffic lights in a series of images, and sometimes the CAPTCHA is a math question. In this chapter, we are going to break a simple CAPTCHA system, called <span class="packt_screen">Really Simple CAPTCHA</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1019 image-border" src="assets/3e6d42c3-d51a-4506-b5f0-f88abef38a1c.png" style="width:111.17em;height:72.83em;"/></p>
<p class="mce-root">Despite its simplicity, <strong>Really Simple CAPTCHA</strong> is still widely used. Most importantly, it will illustrate how to approach breaking other, more complicated, CAPTCHA systems.<br/>
The first step will be to process the CAPTCHA dataset so that it is convenient for machine learning. The most naive approach to the problem is likely to fail. Namely, constructing a supervised classifier that takes a four-character CAPTCHA and classifies it into one of the <em>(26+10)^4 = 1,679,616</em> possible classes (26 letters and 10 digits, taken to the fourth power due to the number of possible combinations of four in such a sequence) would require a huge amount of data and computation. Instead, we train a classifier on individual characters, cut the CAPTCHA into individual characters, and then perform classification four times. Here, again, there is a catch, and that is that it is not that easy to precisely crop the characters. Using OpenCV functionality and additional considerations, this recipe will solve this challenge.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Processing a CAPTCHA dataset</h1>
                </header>
            
            <article>
                
<p>In this recipe, we'll perform the first part of creating a CAPTCHA breaker, in which we process a CAPTCHA dataset to make it amenable to training a machine learning model.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>The preparation for this recipe consists of installing a number of packages in <kbd>pip</kbd>. The instructions are as follows:</p>
</div>
</div>
</div>
</div>
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<pre><strong>pip install opencv-python imutils</strong></pre></div>
</div>
</div>
</div>
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>In addition, a collection of CAPTCHAs has been included for your convenience in <kbd>captcha_images.7z</kbd>. To use these, simply extract the archive into a <kbd>captcha_images</kbd> folder.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we'll process a CAPTCHA dataset to make it amenable to training a machine learning model:</p>
<ol>
<li class="mce-root">Collect a large corpus of CAPTCHAs.</li>
<li class="mce-root">Our next goal is to process the CAPTCHAs, specify where the CAPTCHA images are stored and then enumerate all CAPTCHAs in the specified folder:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import os<br/><br/>captcha_images_folder = "captcha_images"<br/>captchas = [<br/>    os.path.join(captcha_images_folder, f) for f in os.listdir(captcha_images_folder)<br/>]</pre>
<ol start="3">
<li class="mce-root"> Define a function that will take the image of a CAPTCHA and produce a grayscale version, as well as a thresholded (that is, black and white) version of the CAPTCHA's image:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import cv2<br/><br/>def preprocess_CAPTCHA(img):<br/>    """Takes a CAPTCHA image and thresholds it."""<br/>    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>    gray_with_border = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)<br/>    preprocessed = cv2.threshold(<br/>        gray_with_border, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU<br/>    )[1]<br/>    return gray_with_border, preprocessed</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="4">
<li class="mce-root"> Define a function that will take the path of a CAPTCHA and use it to store the text label of that CAPTCHA:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def get_CAPTCHA_label(path_to_file):<br/>    """Get the CAPTCHA text from the file name."""<br/>    filename = os.path.basename(path_to_file)<br/>    label = filename.split(".")[0]<br/>    return label</pre>
<ol start="5">
<li class="mce-root">Define a function that will take the contours of the CAPTCHA, which we will compute, and then determine their bounding rectangles, in preparation for cropping the CAPTCHA into individual characters:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def find_bounding_rectangles_of_contours(contours):<br/>    """Determines the bounding rectangles of the contours of the cropped letters."""<br/>    letter_bounding_rectangles = []<br/>    for contour in contours:<br/>        (x, y, w, h) = cv2.boundingRect(contour)<br/>        if w / h &gt; 1.25:<br/>            half_width = int(w / 2)<br/>            letter_bounding_rectangles.append((x, y, half_width, h))<br/>            letter_bounding_rectangles.append((x + half_width, y, half_width, h))<br/>        else:<br/>            letter_bounding_rectangles.append((x, y, w, h))<br/>    return letter_bounding_rectangles</pre>
<ol start="6">
<li class="mce-root">Define a function that will take the path of a CAPTCHA, read it in as an image, and then preprocess it using the functions we have defined:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def CAPTCHA_to_gray_scale_and_bounding_rectangles(captcha_image_file):<br/>    """Take a CAPTCHA and output a grayscale version as well as the bounding rectangles of its cropped letters."""<br/>    image = cv2.imread(captcha_image_file)<br/>    gray, preprocessed = preprocess_CAPTCHA(image)<br/>    contours = cv2.findContours(<br/>        preprocessed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE<br/>    )<br/>    contours = contours[0]<br/>    letter_bounding_rectangles = find_bounding_rectangles_of_contours(contours)<br/>    letter_bounding_rectangles = sorted(letter_bounding_rectangles, key=lambda x: x[0])<br/>    return gray, letter_bounding_rectangles</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="7">
<li class="mce-root">Define another helper function to take the bounding rectangles of contours of letters and produce character images from these:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def bounding_rectangle_to_letter_image(letter_bounding_box, grayscaled):<br/>    """Obtains the letter defined by a bounding box."""<br/>    x, y, w, h = letter_bounding_box<br/>    letter_image = grayscaled[y - 2 : y + h + 2, x - 2 : x + w + 2]<br/>    return letter_image</pre>
<ol start="8">
<li class="mce-root">Define one last helper function to perform the cropping of a CAPTCHA and then save each cropped character:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">captcha_processing_output_folder = "extracted_letter_images"<br/>character_counts = {}<br/><br/>def crop_bounding_rectangles_and_save_to_file(<br/>    letter_bounding_rectangles, gray, captcha_label<br/>):<br/>    """Saves the individual letters of a CAPTCHA."""<br/>    for letter_bounding_rectangle, current_letter in zip(<br/>        letter_bounding_rectangles, captcha_label<br/>    ):<br/>        letter_image = bounding_rectangle_to_letter_image(<br/>            letter_bounding_rectangle, gray<br/>        )<br/><br/>        save_path = os.path.join(captcha_processing_output_folder, current_letter)<br/>        if not os.path.exists(save_path):<br/>            os.makedirs(save_path)<br/><br/>        character_count = character_counts.get(current_letter, 1)<br/><br/>        p = os.path.join(save_path, str(character_count) + ".png")<br/>        cv2.imwrite(p, letter_image)<br/><br/>        character_counts[current_letter] = character_count + 1</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li class="mce-root">Loop through all of the CAPTCHAs, preprocess them, find the character contours, and then save the corresponding characters:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import imutils<br/>import numpy as np<br/><br/>for captcha_image_file in captchas:<br/>    captcha_label = get_CAPTCHA_label(captcha_image_file)<br/>    gray, letter_bounding_rectangles = CAPTCHA_to_gray_scale_and_bounding_rectangles(<br/>        captcha_image_file<br/>    )<br/>    if len(letter_bounding_rectangles) != 4:<br/>        continue<br/>    crop_bounding_rectangles_and_save_to_file(<br/>        letter_bounding_rectangles, gray, captcha_label<br/>    )</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>Our starting point is to collect a large corpus of CAPTCHAs (<em>step 1</em>). You can find these in <kbd>captcha_images.7z</kbd>. Alternatively, since Really Simple CAPTCHA's code is available online, you can modify it to generate a large number of CAPTCHAs. Additional ideas include utilizing bots to scrape CAPTCHAs. Next, in <em>step 2</em>, we specify where the CAPTCHA images are stored and then enumerate all CAPTCHAs in the specified folder. Our goal is to begin processing these. In <em>step 3</em>, we define a function to threshold and grayscale the CAPTCHA images. This allows us to reduce the computation, as well as making it easier to determine where one character starts and where the next one ends. We then define a function to obtain the label of a CAPTCHA (<em>step 4</em>). Continuing, to prepare for processing, we define a utility function that takes the contours of the CAPTCHA and uses them to determine each character's bounding rectangles. Once a bounding rectangle is found, it is easy to crop the character in order to isolate it (<em>step 5</em>). Next, in <em>step 6</em>, we combine the functions we have defined thus far into one convenient function. We also define an additional function, to actually crop the characters. Putting the above together, in <em>step 8</em>, we write a function that will perform the preceding steps, and then save the resulting isolated character, as well as keeping count of how many of each character has been saved. This is helpful for naming, as well as accounting. We are now in a position to perform the cropping, so, in <em>step 9</em>, we iterate through all the CAPTCHAs and, using our utility functions, crop individual characters. Note that the <kbd>if</kbd> statement is meant to skip any incorrectly cropped CAPTCHAs.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>At the conclusion of the recipe, your output folder, <kbd>extracted_letter_images</kbd>, should have a folder for most letters and digits, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/23403397-e788-4dde-9d75-6ca1739d3a20.png"/></p>
<p class="mce-root"/>
<p class="mce-root">The reason not all characters and digits are represented is that the CAPTCHAs do not contain the digit 1 and letter I, as the two are easily confused. Similarly for 0 and O.<br/>
Inside each folder, you will have a large collection of instances of that letter or digit, cropped and processed from the initial CAPTCHAs:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1012 image-border" src="assets/21529bd2-1d19-4f7b-9556-2da0eb14c722.png" style="width:61.25em;height:41.83em;"/></p>
<p>This concludes the preprocessing step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a CAPTCHA solver neural network</h1>
                </header>
            
            <article>
                
<p>Now that our data is nicely processed, we can train a neural network to perform CAPTCHA prediction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe consists of installing a number of packages in pip. The instructions are as follows:</p>
</div>
</div>
</div>
</div>
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<pre><strong>pip install opencv-python imutils sklearn keras tensorflow</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we'll train a neural network to solve Really Simple CAPTCHA's CAPTCHAs:</p>
<ol>
<li class="mce-root">Specify the folder where the extracted letter images are located:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">captcha_processing_output_folder = "extracted_letter_images"</pre>
<ol start="2">
<li class="mce-root">Import OpenCV and imutils for image manipulation:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import cv2<br/>import imutils</pre>
<ol start="3">
<li class="mce-root">Define a helper function to resize an image to a given size:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def resize_image_to_dimensions(image, desired_width, desired_height):<br/>    """Resizes an image to the desired dimensions."""<br/>    (h, w) = image.shape[:2]<br/>    if w &gt; h:<br/>        image = imutils.resize(image, width=desired_width)<br/>    else:<br/>        image = imutils.resize(image, height=desired_height)<br/>    pad_width = int((desired_width - image.shape[1]) / 2.0)<br/>    pad_height = int((desired_height - image.shape[0]) / 2.0)<br/>    image_with_border = cv2.copyMakeBorder(<br/>        image, pad_height, pad_height, pad_width, pad_width, cv2.BORDER_REPLICATE<br/>    )<br/>    image_with_border_resized = cv2.resize(<br/>        image_with_border, (desired_width, desired_height)<br/>    )<br/>    return image_with_border_resized</pre>
<ol start="4">
<li class="mce-root">Prepare to read in the images:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def read_image(image_file_path):<br/>    """Read in an image file."""<br/>    img = cv2.imread(image_file_path)<br/>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>    img = resize_image_to_dimensions(img, 20, 20)<br/>    img = np.expand_dims(img, axis=2)<br/>    return img</pre>
<ol start="5">
<li class="mce-root">Read in each letter image and record its label:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import numpy as np<br/>import os<br/>from imutils import paths<br/><br/>images = []<br/>labels = []<br/><br/>for image_file_path in imutils.paths.list_images(captcha_processing_output_folder):<br/>    image_file = read_image(image_file_path)<br/>    label = image_file_path.split(os.path.sep)[-2]<br/>    images.append(image_file)<br/>    labels.append(label)</pre>
<ol start="6">
<li class="mce-root">Normalize all images, that is, rescale the pixel values to 0-1 and convert labels to a NumPy array:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">images = np.array(images, dtype="float") / 255.0<br/>labels = np.array(labels)</pre>
<ol start="7">
<li>Create a train-test split:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from sklearn.model_selection import train_test_split<br/><br/>(X_train, X_test, y_train, y_test) = train_test_split(<br/>    images, labels, test_size=0.3, random_state=11<br/>)</pre>
<ol start="8">
<li>Import <kbd>LabelBinarizer</kbd> in order to encode the labels:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from sklearn.preprocessing import LabelBinarizer<br/><br/>label_binarizer = LabelBinarizer().fit(y_train)<br/>y_train = label_binarizer.transform(y_train)<br/>y_test = label_binarizer.transform(y_test)</pre>
<ol start="9">
<li>Define a neural network architecture:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from keras.models import Sequential<br/>from keras.layers.convolutional import Conv2D, MaxPooling2D<br/>from keras.layers.core import Flatten, Dense<br/><br/>num_classes = 32<br/>NN_model = Sequential()<br/>NN_model.add(<br/>    Conv2D(20, (5, 5), padding="same", input_shape=(20, 20, 1), activation="relu")<br/>)<br/>NN_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))<br/>NN_model.add(Conv2D(50, (5, 5), padding="same", activation="relu"))<br/>NN_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))<br/>NN_model.add(Flatten())<br/>NN_model.add(Dense(512, activation="relu"))<br/>NN_model.add(Dense(num_classes, activation="softmax"))<br/>NN_model.compile(<br/>    loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"]<br/>)<br/>NN_model.summary()</pre>
<ol start="10">
<li>Fit the neural network to the training data:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">NN_model.fit(<br/>    X_train,<br/>    y_train,<br/>    validation_data=(X_test, y_test),<br/>    batch_size=16,<br/>    epochs=5,<br/>    verbose=1,<br/>)</pre>
<ol start="11">
<li>Select a CAPTCHA instance you would like to break:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">CAPTCHA = "captcha_images\\NZH2.png"</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="12">
<li>We'll import all of the functions we used to process images in the previous recipe, namely, <kbd>find_bounding_rectangles_of_contours</kbd>, <kbd>preprocess_CAPTCHA</kbd>, <kbd>get_CAPTCHA_label</kbd>, and <kbd>CAPTCHA_to_grayscale_and_bounding_rectangles</kbd>.</li>
<li>Process the CAPTCHA image as we did in the previous recipe:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">captcha_label = get_CAPTCHA_label(CAPTCHA)<br/>gray, letter_bounding_rectangles = CAPTCHA_to_gray_scale_and_bounding_rectangles(<br/>    CAPTCHA<br/>)<br/>predictions = []</pre>
<ol start="14">
<li>Read in each cropped letter and use the neural network to predict the label:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">for letter_bounding_rectangle in letter_bounding_rectangles:<br/>    x, y, w, h = letter_bounding_rectangle<br/>    letter_image = gray[y - 2 : y + h + 2, x - 2 : x + w + 2]<br/>    letter_image = resize_image_to_dimensions(letter_image, 20, 20)<br/>    letter_image = np.expand_dims(letter_image, axis=2)<br/>    letter_image = np.expand_dims(letter_image, axis=0)<br/>    prediction = NN_model.predict(letter_image)<br/>    letter = label_binarizer.inverse_transform(prediction)[0]<br/>    predictions.append(letter)</pre>
<ol start="15">
<li>Print out the prediction:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">predicted_captcha_text = "".join(predictions)<br/>print("Predicted CAPTCHA text is: {}".format(predicted_captcha_text))<br/>print("CAPTCHA text is: {}".format(CAPTCHA.split("\\")[-1].split(".")[0]))<br/><br/>Predicted CAPTCHA text is: NZH2<br/>CAPTCHA text is: NZH2</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>Having completed our preprocessing of CAPTCHAs in the previous recipe, we are now ready to utilize these to train a CAPTCHA breaker. We start by setting a variable to the path of all of our individual characters extracted from CAPTCHAs. We import the image manipulation libraries we will be using (<em>step 2</em>) and then define a function to resize an image in <em>step 3</em>. This is a relatively standard method for character recognition, which allows training to proceed faster, and memory utilization to be reduced. In <em>step 4</em>, we define a convenience function to read in files as NumPy arrays, for training purposes, and then, in <em>step 5</em>, we iterate through all the letters and record their labels. Next, we normalize all of the images (<em>step 6</em>), another standard computer vision trick. We now create a train-test split in preparation for fitting our classifier (<em>step 7</em>) and then utilize label binarizers to encode our labels (<em>step 8</em>). This is necessary since the labels are the characters, which may not be numerical. In <em>step 9</em>, we define the architecture of our neural network. The architecture stated is relatively common, and offers both precision and speed. We fit our neural network to the training set in <em>step 10</em>. Other parameters can enhance the performance of the network. The hard work is now finished. We now proceed to demonstrate how the CAPTCHA breaker works. In <em>step 11</em>, we choose a singleton instance to demonstrate the efficacy of our CAPTCHA breaker. In steps 12-14, we pass this image through our pipeline and produce predicted text for this CAPTCHA. Finally, we verify that the prediction is correct (<em>step 15</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network-assisted fuzzing</h1>
                </header>
            
            <article>
                
<p>Fuzzing is a software vulnerability detection method wherein a large number of random inputs are fed into a program in search of ones that will cause a crash, unwanted information leak, or other unintended behavior. In automated fuzzing, a program generates these inputs. Generally, automated fuzzers suffer from the shortcoming that they tend to get stuck trying redundant inputs. For this reason, AI-based fuzzers have recently been developed. In this recipe, we'll employ NEUZZ, a neural network-based fuzzer by She et al. (see <a href="https://arxiv.org/abs/1807.05620">https://arxiv.org/abs/1807.05620</a>), to find unknown vulnerabilities in software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>The following recipe requires an Ubuntu 16.04 or 18.04 virtual or physical machine. On this device, run the following:</p>
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<pre><strong>pip install keras</strong></pre>
<p>Extract <kbd>neuzz-modified.7z</kbd> to a folder of your choosing.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following steps, we provide a recipe for using NEUZZ to find crash-causing inputs to the readelf Unix tool:</p>
<ol>
<li class="mce-root"> Build neuzz using the following:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>gcc -O3 -funroll-loops ./neuzz.c -o neuzz</strong></pre>
<div style="padding-left: 60px" class="mce-root packt_infobox">If you receive warnings, that's okay.</div>
<p style="padding-left: 60px" class="mce-root">2. Install the libraries needed for 32-bit binaries:</p>
<pre style="padding-left: 60px" class="mce-root"><strong> sudo dpkg --add-architecture i386</strong><br/><strong> sudo apt-get update</strong><br/><strong> sudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386 lib32z1</strong></pre>
<ol start="3">
<li class="mce-root">As root, set the CPU scaling algorithm and core dump notification:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong> cd /sys/devices/system/cpu</strong><br/><strong> echo performance | tee cpu*/cpufreq/scaling_governor</strong><br/><strong> echo core &gt;/proc/sys/kernel/core_pattern</strong></pre>
<ol start="4">
<li class="mce-root">Copy <kbd>neuzz</kbd>, <kbd>nn.py</kbd>, and <kbd>afl-showmap</kbd> to <kbd>programs/readelf</kbd>:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong> cp /path_to_neuzz/neuzz /path_to_neuzz/programs/readelf</strong><br/><strong> cp /path_to_neuzz/nn.py /path_to_neuzz/programs/readelf</strong><br/><strong> cp /path_to_neuzz/afl-showmap /path_to_neuzz/programs/readelf</strong></pre>
<ol start="5">
<li class="mce-root">Provide all files with executable permission:</li>
</ol>
<pre style="padding-left: 60px"><strong>chmod +x /path_to_neuzz/programs/readelf/neuzz</strong><br/><strong>chmod +x /path_to_neuzz/programs/readelf/nn.py</strong><br/><strong>chmod +x /path_to_neuzz/programs/readelf/afl-showmap</strong><br/><strong>chmod +x /path_to_neuzz/programs/readelf/readelf</strong></pre>
<ol start="6">
<li class="mce-root">Open a Terminal to start the neural network module:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>cd /path_to_neuzz/programs/readelf</strong><br/><strong>python nn.py ./readelf -a</strong></pre>
<ol start="7">
<li class="mce-root">Open another Terminal and start NEUZZ:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong> ./neuzz -i neuzz_in -o seeds -l 7507 ./readelf -a @@</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px" class="mce-root">Here is a snippet from running the commands:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/9675c7a1-2b44-43e5-8fd8-9816e4176517.png"/></p>
<ol start="8">
<li class="mce-root">Test the crashes that NEUZZ has collected by running the following:</li>
</ol>
<pre style="padding-left: 60px"><strong> ./readelf -a crash/file_name</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1013 image-border" src="assets/d367d900-4a9f-47ff-906c-d598bb32bc9c.png" style="width:48.33em;height:27.00em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>Most popular fuzzers, while effective in some limited situations, often get stuck in a loop. Gradient-based methods, such as the one discussed here, are promising but do not clearly apply to the problem, because real-world program behaviors are not necessarily smooth functions (for example, they can be discontinuous). The idea behind NEUZZ is to approximate the program's behavior as a smooth function using neural networks. Then, it is possible to apply gradient methods to improve fuzzing efficiency. We start our recipe by compiling NEUZZ (<em>step 1)</em>. The <kbd>funroll-loops</kbd> flag causes the compiler to unroll loops whose number of iterations can be determined at compile time or upon entry to the loop. As a result, the code is larger, and may run faster, although not necessarily. Continuing to setup NEUZZ, we add in 32-bit support (<em>step 2</em>). We set the CPU scaling algorithm and core dump notification (<em>step 3</em>); the CPU frequency scaling is a setting that enables the OS to save power by scaling the CPU frequency up or down. In the next two steps, we simply place the files in a convenient location and allow permissions to execute them. We are done setting up NEUZZ. We can now use it to find inputs that cause programs to crash. In <em>step 6</em> and <em>step 7</em>, we begin the search for crashes using our neural network. After waiting a sufficient amount of time for <em>step 6</em> and <em><span>step </span>7</em> to gather enough inputs to cause the readelf tool to crash, we execute one of these inputs (<em>step 8</em>) to see the result. Indeed, we see that the input resulted in readelf crashing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DeepExploit</h1>
                </header>
            
            <article>
                
<p><strong>DeepExploit</strong> is a penetration testing tool that elevates Metasploit to a whole new level by leveraging AI. Its key features are as follows: </p>
<ul>
<li><strong>Deep penetration</strong>: If DeepExploit successfully exploits the target, it will automatically execute the exploit to other internal servers as well. </li>
<li><strong>Learning</strong>: DeepExploit is a reinforcement learning system, akin to AlphaGo.</li>
</ul>
<p>Using DeepExploit to pentest your security systems will take you a long way toward keeping your systems secure. In this recipe, we will set up and run DeepExploit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<p>You will now be guided through the steps required to install <kbd>DeepExploit</kbd>:</p>
<ol>
<li>Download and set up Kali Linux. You can find VM images online at <a href="https://www.offensive-security.com/kali-linux-vm-vmware-virtualbox-image-download/">https://www.offensive-security.com/kali-linux-vm-vmware-virtualbox-image-download/</a><span>. </span>The following steps all take place in your Kali Linux box.</li>
</ol>
<ol start="2">
<li> Install Git by running the following in a Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt install git</strong></pre>
<ol start="3">
<li> Install Python by running the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt install python3-pip</strong></pre>
<ol start="4">
<li>Clone the <kbd>git</kbd> repository. In a Terminal, run the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/emmanueltsukerman/machine_learning_security.git</strong></pre>
<ol start="5">
<li>Open the <kbd>DeepExploit</kbd> directory:</li>
</ol>
<p style="padding-left: 60px">In a Terminal, run the following:</p>
<pre style="padding-left: 60px"><strong>cd machine_learning_security/DeepExploit</strong></pre>
<ol start="6">
<li>Install the prerequisite packages for <kbd>DeepExploit</kbd>.</li>
</ol>
<p style="padding-left: 60px">In a Terminal, run the following:</p>
<pre style="padding-left: 60px"><strong>pip3 install -r requirements.txt</strong></pre></div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will use <kbd>DeepExploit</kbd> to compromise a victim virtual machine.</p>
<ol>
<li>Download a <kbd>Metasploitable2</kbd> VM image.</li>
</ol>
<div class="packt_infobox">Details can be found at <a href="https://metasploit.help.rapid7.com/docs/metasploitable-2">https://metasploit.help.rapid7.com/docs/metasploitable-2</a>.</div>
<p class="mce-root"/>
<ol start="2">
<li>Run a <kbd>Metasploitable2</kbd> instance on a VM.</li>
<li>Obtain the IP address of your <kbd>Metasploitable2</kbd>.</li>
<li>The next step is to set up DeepExploit's configurations.</li>
<li>In a Terminal, run <kbd>ifconfig</kbd> to obtain your <span>Kali Linux</span>'s IP. Edit <kbd>config.ini</kbd> (for example, using <kbd>vim</kbd>) by setting <kbd>server_host</kbd> under <kbd>[common]</kbd> to your Kali Linux IP.</li>
<li>Set the values of <kbd>proxy_host</kbd> and <kbd>proxy_port</kbd> in <kbd>config.ini</kbd> to those in <kbd>proxychains.conf</kbd>.</li>
<li>In the Terminal, run <kbd>cat /etc/proxychains.conf</kbd> and find the value next to <kbd>socks4</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong> ...snip...</strong><br/><strong> [ProxyList]</strong><br/><strong> ...snip...</strong><br/><strong> socks4  127.0.0.1 9050</strong></pre>
<ol start="8">
<li>Then, set the values of <kbd>proxy_host</kbd> and <kbd>proxy_port</kbd> in <kbd>config.ini</kbd> equal to these values:</li>
</ol>
<pre style="padding-left: 60px"><strong> vim config.ini</strong><br/><strong> ...snip...</strong><br/><strong> proxy_host      : 127.0.0.1</strong><br/><strong> proxy_port      : 9050</strong></pre>
<ol start="9">
<li>Launch Metasploit in the Terminal by running <kbd>msfconsole</kbd>.</li>
<li>Launch an RPC server on Metasploit. Where indicated, type in your Kali Linux's IP:</li>
</ol>
<pre style="padding-left: 60px"><strong>msf&gt; load msgrpc ServerHost="kali linux ip" ServerPort=55553 User=test Pass=test1234.</strong></pre>
<p style="padding-left: 60px">You should see the following: </p>
<pre style="padding-left: 60px"><strong>[*] MSGRPC Service: "kali linux ip":55553</strong><br/><strong>[*] MSGRPC Username: test</strong><br/><strong>[*] MSGRPC Password: test1234</strong><br/><strong>[*] Successfully loaded plugin: msgrpc</strong></pre>
<p class="mce-root"/>
<ol start="11">
<li>In a Terminal of your Kali Linux machine, run <kbd>python3 DeepExploit.py -t "Metasploitable2 ip" -m train</kbd> to train <kbd>DeepExploit</kbd>. The beginning of the training should look as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1020 image-border" src="assets/b2fccab6-e290-47a0-b863-7b447bb02774.png" style="width:154.58em;height:70.58em;"/></p>
<p style="padding-left: 60px">Whenever <kbd>DeepExploit</kbd> finds a vulnerability, you will see a <kbd>BINGO!!!</kbd> notification, as in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1021 image-border" src="assets/b63f382c-1202-4166-9c8c-fb3fc19f2119.png" style="width:154.25em;height:71.25em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">Upon conclusion of the training, the learning is saved. You can see the completion screen here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1022 image-border" src="assets/e312ea67-43e8-4c15-bb3f-1e50298d61ea.png" style="width:154.25em;height:71.58em;"/></p>
<ol start="12">
<li>Test <kbd>Metasploitable2</kbd> for vulnerabilities using <kbd>DeepExploit</kbd>. In a Terminal, run <kbd>python DeepExploit.py -t "Metasploitable2 ip" -m test</kbd>.</li>
<li>Check the report of the pen test as shown:</li>
</ol>
<pre style="padding-left: 60px"><strong>firefox report/DeepExploit_test_report.html</strong></pre>
<p style="padding-left: 60px">We'll get the following as the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/840efb2f-5ed7-433e-8226-ef3474486359.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>This recipe requires a large amount of preparation and configuration. The initial steps are to set up a victim virtual machine (<em>steps 1 and 2</em>). In <em>step 3</em>, we determine the IP address of the victim VM. Note that the credentials of <kbd>Metasploitable2</kbd> are <kbd>msfadmin/msfadmin</kbd>. You can use the credentials to log in and then use <kbd>ifconfig</kbd> to obtain your <kbd>Metasploitable2</kbd> IP. If you are using a Kali Linux VM and a <kbd>Metasploitable2</kbd> VM on the same host, make sure that the two can communicate. For instance, put both VMs on a Host-Only Adapter and ping from your Kali Linux machine to the Metasploitable2 machine. Proceeding, we now configure <kbd>DeepExploit</kbd> so we can target the victim VM (<em>steps 4</em>-<em>8</em>). In <em>steps 9</em> and <em>10</em>, we open up Metasploit, which is used as a submodule by <kbd>DeepExploit</kbd>. Metasploit is a major penetration testing framework. Having finished all the preparation, we can now start to train our model. In <em>step 11</em>, we train <kbd>DeepExploit</kbd> on the <kbd>Metasploitable2</kbd> VM. The model utilizes the <strong>Asynchronous Actor-Critic Agents</strong> (<kbd>A3C</kbd>) algorithm, released by Google's DeepMind group a few years back, famous for outperforming the <kbd>deep Q-network</kbd> (<kbd>DQN</kbd>) approach. Next, we test our model (<em>step 12</em>) and print out the results of its analysis in a report (<em>step 13</em>). As you can see from the long report, a large number of vulnerabilities were found by <kbd>DeepExploit</kbd>. Speaking from a high level, the application of reinforcement learning to penetration testing suggests that extremely efficient automated penetration testing is on the horizon.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Web server vulnerability scanner using machine learning (GyoiThon)</h1>
                </header>
            
            <article>
                
<p><strong>GyoiThon</strong> is an intelligence-gathering tool for web servers. It executes remote access to a target web server and identifies products operated on the server, such as the <strong>Content Management System</strong> (<strong>CMS</strong>), web server software, framework, and programming language. In addition, it can execute exploit modules for the identified products using Metasploit.          </p>
<p>Some of the main features of GyoiThon are as follows:</p>
<ul>
<li><strong>Remote access/Fully automatic</strong>: GyoiThon can automatically gather information on a target web server using only remote access. You only execute GyoiThon once for your operation.</li>
<li><strong>Non-destructive test</strong>:<em><strong> </strong></em>GyoiThon can gather information on the target web server using only normal access. A feature permits GyoiThon to access abnormally, such as by sending exploit modules.</li>
<li><strong>Gathering varied information</strong>: GyoiThon has a number of intelligence gathering engines such as a web crawler, the Google Custom Search API, Censys, an explorer of default contents, and the examination of cloud services. By analyzing gathered information using string pattern matching and machine learning, GyoiThon can identify a product/version/CVE number operated on the target web server, HTML comments/debug messages, login pages, and other information.</li>
<li><strong>Examination of real vulnerabilities</strong>: GyoiThon can execute exploit modules on identified products using Metasploit. As a result, it can determine the real vulnerabilities of the target web server.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>You will now be guided through the steps for installing and running GyoiThon:</p>
<ol>
<li>Download and set up Kali Linux. You can find VM images online at <a href="https://www.offensive-security.com/kali-linux-vm-vmware-virtualbox-image-download/">https://www.offensive-security.com/kali-linux-vm-vmware-virtualbox-image-download/</a><span>. </span>The following steps all take place in your Kali Linux box.</li>
</ol>
<ol start="2">
<li> Install <kbd>git</kbd> in the Terminal by running the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt install git</strong></pre>
<ol start="3">
<li>Install <kbd>python</kbd> in the Terminal by running the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt install python3-pip</strong></pre>
<ol start="4">
<li>Clone the Git repository into your Linux box in the Terminal by running the following command:</li>
</ol>
<div class="a-b-r-x">
<pre style="padding-left: 60px"><strong>git clone https://github.com/gyoisamurai/GyoiThon.git</strong></pre>
<ol start="5">
<li>Open the <kbd>GyoiThon</kbd> directory in the Terminal by running the following command:</li>
</ol>
</div>
<pre style="padding-left: 60px"><strong>cd GyoiThon</strong></pre>
<ol start="6">
<li>Install the prerequisites for DeepExploit in the Terminal by running the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip3 install -r requirements.txt</strong></pre>
<ol start="7">
<li>(Optional.) Substitute the <kbd>Gyoi_CveExplorerNVD</kbd> file in modules with the one available in the repository for this book. In some cases, the original code has malfunctioned and the modified code available in the repository for this book may address this problem.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will use DeepExploit to compromise a victim virtual machine:</p>
<ol>
<li>Download a <kbd>Metasploitable2</kbd> VM image.</li>
<li>Run a <kbd>Metasploitable2</kbd> instance on a VM.</li>
<li>Obtain the IP address of your <kbd>Metasploitable2</kbd>.</li>
<li>In your Kali Linux machine, you should be able to see your <kbd>Metasploitable2's</kbd> website instance by typing <kbd>Metasploitable2's ip address:80</kbd> into a web browser:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1023 image-border" src="assets/71464351-549d-4a10-9759-c96489711565.png" style="width:106.58em;height:71.50em;"/></div>
<ol start="5">
<li>In a Terminal, run <kbd>ifconfig</kbd> to obtain your Kali Linux's IP. Edit <kbd>config.ini</kbd> (for example, using <kbd>vim</kbd>) by setting <kbd>proxy</kbd> to <kbd>empty</kbd>, <kbd>server host</kbd> to your <kbd>Kali Linux IP</kbd>, <kbd>LHOST</kbd> to your <kbd>Metasploitable2 IP</kbd>, and <kbd>LPORT</kbd> to <kbd>80</kbd>.</li>
<li> Open the host file and add the <kbd>Metasploitable2</kbd> web server address by typing in <kbd>http:Metasploitable2 ip:80/</kbd>.</li>
<li>In a Terminal of your Kali Linux machine, run <kbd>python3 Gyoithon.py</kbd> to begin the attack. </li>
<li>Upon the conclusion of the attack, check the report of the pen test in the folder report:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1024 image-border" src="assets/efb44d52-8217-4d81-beae-0e222e914e69.png" style="width:126.83em;height:24.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p><em>Steps 1-3</em> are no different than in the recipe for DeepExploit, where we prepared a victim VM. The credentials of <kbd>Metasploitable2</kbd> are <kbd>msfadmin/msfadmin</kbd>. You can use the credentials to log in and then use <kbd>ifconfig</kbd> to obtain your <kbd>Metasploitable2</kbd> IP. If you are using a Kali Linux VM and <kbd>Metasploitable2</kbd> VM on the same host, make sure that the two can communicate. For instance, put both VMs on a <span class="packt_screen">Host-only Adapter</span> and ping from your Kali Linux machine to the Metasploitable2 machine. Next, we verify that the environment has been properly set up by checking that we are able to access the victim VM's web page in <em>step 4</em>. In <em>step 5</em> and <em>step 6</em>, we configure GyoiThon in preparation for our pen test. Having finished setting up our environments, we are now ready to perform the pen test. In <em>step 7</em>, we utilize GyoiThon to search for vulnerabilities. We then output a full report of the vulnerabilities detected (<em>step 8</em>). Looking at the report, we can see that GyoiThon was able to find a large number of vulnerabilities. Having now determined the vulnerabilities of the victim box, we can go ahead and exploit these, using, for example, Metasploit, to hack the victim box.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deanonymizing Tor using machine learning</h1>
                </header>
            
            <article>
                
<p>Tor <span>is</span> <span>a</span> <span>free, open source software for enabling anonymous communication. In addition, websites accessible only when using the Tor browser exist, and are part of the <strong>dark web</strong> ecosystem – the name given to the part of the internet that is hidden from the average user.</span> <span>In this recipe, we will deanonymize Tor traffic by</span> c<span>ollect</span><span>ing</span> <span>enough features and information from individual sessions</span> <span>to be able to</span> <span>identify the activity of anonymized users.</span> <span>This recipe utilizes the</span> <strong>conmarap/website-fingerprinting</strong> repository.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>You will now be guided through the steps needed to set up Tor and the Lynx web browser:</p>
<ol>
<li>Set up an Ubuntu VM.</li>
<li>Install <kbd>git</kbd> in the Terminal by running the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt install git</strong></pre>
<ol start="3">
<li><span>Clone the code repository i</span>n the Terminal by running <span>the following command</span>:</li>
</ol>
<div class="a-b-r-x">
<pre style="padding-left: 60px"><strong>git clone https://github.com/conmarap/website-fingerprinting</strong></pre></div>
<div class="a-b-r-x">
<ol start="4">
<li>Install <kbd>tor</kbd> and <kbd>lynx</kbd><span> i</span>n the Terminal by running <span>the following command</span>:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt install tor lynx</strong></pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p class="mce-root">This recipe consists of three parts. The first part consists of the data collection of Tor traffic. The second consists of training a classifier on this data. And the final part consists of using the classifier to predict the type of traffic being observed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting data</h1>
                </header>
            
            <article>
                
<p>The following steps need to be followed for data collection:</p>
<ol>
<li>List the classes of traffic you wish to classify in <kbd>config.json</kbd>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5c3ccb28-410d-41c7-bb0b-f8f7c8577d14.png" style="width:32.92em;height:11.58em;"/></p>
<ol start="2">
<li>Collect an additional data point for one of the classes, say <kbd>duckduckgo.com</kbd>, in a Terminal, from the website-fingerprinting directory by running the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>./pcaps/capture.sh duckduckgo.com</strong></pre>
<ol start="3">
<li>Open another Terminal, and run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>torsocks lynx https://duckduckgo.com</strong><a href="https://duckduckgo.com/"/></pre>
<p style="padding-left: 60px">Your two Terminals should look as follows, at this point:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ecfa12db-50fc-48f7-b06e-fa36a7cbdbdb.png"/></p>
<ol start="4">
<li>Once you have finished the browsing session, end the capture by pressing <em>Q</em> twice.</li>
</ol>
<p>When a sufficient amount of training data has been gathered, we are ready to train a classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training</h1>
                </header>
            
            <article>
                
<p><span>To train a classifier on the data, r</span><span>un the following script using Python:</span></p>
<pre><strong>python gather_and_train.py</strong></pre>
<p>The result is a file classifier: <kbd>nb.dmp</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting</h1>
                </header>
            
            <article>
                
<p>Let's use the classifier to predict the type of traffic being observed:</p>
<ol>
<li>To predict a new instance of traffic, collect the <kbd>pcap</kbd> file.</li>
<li>
<p>Using Python, run the <kbd>predict.py</kbd> script with the <kbd>pcap</kbd> file as an argument:</p>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3ed74252-6025-48bc-b427-eea3b88220cd.png"/></p>
<p>The clustering by the author looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/63bfbc53-5589-47b6-a81a-a8a28fe1d895.png"/></p>
<p>The preceding diagram shows that the features do indeed differentiate between the type of traffic, despite it being anonymous.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>We start constructing our classifier by creating a catalog of all of the websites we wish to profile (<em>step 1</em>). The more there are, the more likely the target is to visit one of these. On the other hand, the fewer there are, the smaller the training dataset will have to be. In <em>steps 2</em>-<em>4</em>, we perform the steps required to collect a data point for our classifier. Specifically, we do so by visiting one of the websites defined in <em>step 1</em> and then capture the packets for that visit. By repeating these steps for different browsing sessions, we are able to construct a robust dataset. In <em>step 5</em>, we train a classifier on our data, which we have collected up until now. We are now ready to test out our classifier. In <em>step 6</em>, we visit a website and collect its <kbd>pcap</kbd>, just as we did when collecting our training data. We then employ the classifier to classify this visit (<em>step 7</em>). We see that it did, indeed, correctly determine which web page the user visited, despite the user using Tor.</p>
<p>In summary, in this recipe, scikit-learn was used to write a k-nearest neighbors classifier that would classify Tor <kbd>pcap</kbd> files. In practice, traffic is never as <em>clean</em>, so accuracy is likely to decrease on a real dataset of the same size. However, an entity with large amounts of resources can create a very accurate classifier. This means that it is entirely possible to use a method like this to accurately compromise anonymized users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">IoT device type identification using machine learning</h1>
                </header>
            
            <article>
                
<p>With the advent of IoT, the attack surfaces on any given target have increased exponentially. With new technology comes new risks, and, in the case of IoT, one such risk to an organization is the addition of a malicious IoT device connected to the organization's network. It is essential to be able to tell when such a device has been added to a network and to understand its nature. In this recipe, we'll build a machine learning model to classify network IoT devices by type.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>The preparation for this recipe consists of installing the <kbd>sklearn</kbd>, <kbd>pandas</kbd>, and <kbd>xgboost</kbd> packages in <kbd>pip</kbd>. The instructions are as follows:</p>
<pre><strong>pip install pandas sklearn xgboost</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>A dataset has been provided for you in the <kbd>iot_train.csv</kbd> and <kbd>iot_test.csv</kbd> files.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>In the following steps, we'll train and test a classifier on IoT network information:</p>
<ol>
<li>Import <kbd>pandas</kbd> and <kbd>os</kbd> and read in the training and testing data:</li>
</ol>
<pre style="padding-left: 60px">import pandas as pd<br/>import os<br/><br/>training_data = pd.read_csv("iot_devices_train.csv")<br/>testing_data = pd.read_csv("iot_devices_test.csv")</pre>
<p style="padding-left: 60px">The data contains 298 features, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9c50983f-be84-4f46-9db7-6d554bbccf6e.png"/></p>
<p class="mce-root"/>
<ol start="2">
<li>Create a training and testing dataset, where the target is the device category:</li>
</ol>
<pre style="padding-left: 60px">X_train, y_train = (<br/>    training_data.loc[:, training_data.columns != "device_category"].values,<br/>    training_data["device_category"],<br/>)<br/>X_test, y_test = (<br/>    testing_data.loc[:, testing_data.columns != "device_category"].values,<br/>    testing_data["device_category"],<br/>)</pre>
<p style="padding-left: 60px">The device categories are security camera, TV, smoke detector, thermostat, water sensor, watch, baby monitor, motion sensor, lights, and socket.</p>
<ol start="3">
<li> Encode the class categories into numerical form:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import preprocessing<br/><br/>le = preprocessing.LabelEncoder()<br/>le.fit(training_data["device_category"].unique())<br/>y_train_encoded = le.transform(y_train)<br/>y_test_encoded = le.transform(y_test)</pre>
<ol start="4">
<li>Instantiate an <kbd>xgboost</kbd> classifier:</li>
</ol>
<pre style="padding-left: 60px">from xgboost import XGBClassifier<br/><br/>model = XGBClassifier()</pre>
<ol start="5">
<li>Train and test the <kbd>xgboost</kbd> classifier:</li>
</ol>
<pre style="padding-left: 60px">model.fit(X_train, y_train_encoded)<br/>model.score(X_test, y_test_encoded)</pre>
<p style="padding-left: 60px">The output is as follows:</p>
<pre style="padding-left: 60px"><strong>0.6622222222222223</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>An important motivation for this recipe is that we can't rely on the IP address as an identifier of the device, since this value can be spoofed. Consequently, we would like to analyze the traffic's high-level data, <span>that is</span>, the metadata and traffic statistics, rather than content, to determine whether the device belongs to the network. We begin by reading in the training and testing datasets. We go on to featurize these and perform a quick data exploration step by observing the classification labels (<em>step 2</em>). To feed these into our classifier, we convert these categorical labels into numerical ones to be used to train our machine learning classifier (<em>step 3</em>). Having featurized the data in <em>step 4</em> and <em>step 5</em>, we instantiate, train and test an <kbd>xgboost</kbd> classifier, obtaining a score of <kbd>0.66</kbd> on the testing set. There are 10 categories of IoT devices in the associated data. The baseline of randomly guessing between the 10 would yield an accuracy of 0.1. The <kbd>XGBoost</kbd> classifier trained here attains an accuracy of 0.66, suggesting that it is indeed a promising approach to classify IoT devices successfully based on high-level traffic data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keystroke dynamics</h1>
                </header>
            
            <article>
                
<p>Keystroke dynamics, <span>aka</span> typing biometrics, <span>is the study of</span> recognizing a person by the way they type. One important use case is recognizing which user is logging in using a given credential, for example, who is logging in as root? Another use case is recognizing when a different user has typed a sequence of keystrokes. In this recipe, we'll show how to use a machine learning-based keystroke dynamics algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This recipe will require a Linux virtual or real machine. In preparation, do the following:</p>
<ol>
<li>Install <kbd>git</kbd> on your device.</li>
</ol>
<p style="padding-left: 60px">In a Terminal, run the following command:</p>
<pre style="padding-left: 60px"><strong>sudo apt install git</strong></pre>
<ol start="2">
<li>Clone the <kbd>git</kbd> repository containing the code for the keystroke dynamics algorithm:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/emmanueltsukerman/keystroke_dynamics.git</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the following steps, we'll train the model on two users' typing patterns and then use the model to recognize one of the user's typing patterns. The recipe should be run on a Linux virtual or real machine:</p>
<ol>
<li> Run <kbd>example.py</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>python example.py</strong></pre>
<ol start="2">
<li>Train on the keystrokes of user 1 by selecting option 1 and then typing in the text:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/209f8806-a477-4839-b1e1-221d511f8a53.png" style="width:43.58em;height:19.83em;"/></p>
<ol start="3">
<li>Run <kbd>example.py</kbd> and train on the keystrokes of user 2 by selecting option 1 and then having user 2 type in the text:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b69bbb11-e7fb-4448-b4c9-fb5081e45a08.png"/></p>
<ol start="4">
<li>Run <kbd>example.py</kbd> and, this time, select option 2.</li>
<li>Have one of the users type the text again. The algorithm will match the keyboard dynamics to the most similar typist from the training data:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/04dfcca1-d472-4c2a-a11e-b4c06a74d71f.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Analyzing keystroke dynamics utilizes the rhythm and pace at which a user types on a keyboard to verify that individual's identity. We begin by setting up some baselines. In <em>step 1</em> and <em>step 2</em>, we set up the keystroke dynamics system to learn the typing pattern of the first user. We then do the same for the second user (<em>step 3</em>). This establishes our <em>normal</em> users, as well as their typing patterns. In <em>step 4</em> and <em>step 5</em>, we utilize our trained model (trained in <em>steps 1</em>-<em>3</em>), to determine who the current user is. As you can see, the classifier outputs a similarity score and a prediction of who the current user is from its catalog of saved users. This allows us to detect unauthorized users, as well as to simply keep track of system usage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Malicious URL detector</h1>
                </header>
            
            <article>
                
<p>Malicious URLs cause billions of dollars of damage every year by hosting spam, malware, and exploits, as well as stealing information. Traditionally, defenses against these have relied on blacklists and whitelists – lists of URLs that are considered malicious, and lists of URLs that are considered safe. However, blacklists suffer from a lack of generality and an inability to defend against previously unseen malicious URLs. To remedy the situation, machine learning techniques have been developed. In this recipe, we'll run a malicious URL detector using character-level recurrent neural networks with Keras. The code is based on <a href="https://github.com/chen0040/keras-malicious-url-detector">https://github.com/chen0040/keras-malicious-url-detector</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>The preparation for this recipe consists of installing a number of packages in <kbd>pip</kbd>. The instructions are as follows:</p>
</div>
</div>
</div>
</div>
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<pre><strong>pip install keras tensorflow sklearn pandas matplotlib</strong></pre>
<p>In addition, clone the following <kbd>git</kbd> repository:</p>
<pre><strong>git clone https://github.com/emmanueltsukerman/keras-malicious-url-detector.git</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<ol>
<li>Train the bidirectional LSTM model:</li>
</ol>
<pre style="padding-left: 60px"><strong>python bidirectional_lstm_train.py</strong></pre>
<p style="padding-left: 60px">The training screen should look something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/84819f8a-6cdb-44bc-92b1-ddf7aaae7375.png"/></p>
<ol start="2">
<li>Test the classifier:</li>
</ol>
<pre style="padding-left: 60px"><strong>python bidirectional_lstm_predict.py</strong></pre>
<p style="padding-left: 60px">The testing screen should look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1184 image-border" src="assets/9c1798ae-4ed4-4b4c-9241-67f07a4fd2cb.png" style="width:158.00em;height:54.08em;"/></p>
<p style="padding-left: 60px">Finally, you can see the results under the <kbd>reports</kbd> folder:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/44212e52-6f14-44fd-864b-1937f0dbf77b.png" style="width:31.25em;height:23.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>This is a relatively simple recipe but serves as a good starting point for a more high-powered malicious URL detector. The dataset consists of URLs with the labels 0 and 1, depending on whether they are malicious or benign:</p>
<pre>http://google.com,0<br/>http://facebook.com,0<br/>http://youtube.com,0<br/>http://yahoo.com,0<br/>http://baidu.com,0<br/>http://wikipedia.org,0<br/>http://qq.com,0<br/>http://linkedin.com,0<br/>http://live.com,0<br/>http://twitter.com,0<br/>http://amazon.com,0<br/>http://taobao.com,0<br/>http://blogspot.com,0<br/><br/>&lt;snip&gt;<br/>http://360.cn,0 <br/>http://go.com,0 <br/>http://bbc.co.uk,0<br/>http://xhamster.com,0</pre>
<p>In <em>step 1</em>, we train a bidirectional LSTM model. By digging deeper into the code, you can adjust the network to your needs. Having trained our model, it is important to assess its performance and perform some sanity checks. We do so in <em>step 2</em>, the testing step, consisting of displaying the results of the classifier on a random selection of 20 URLs. In general, a bidirectional LSTM is a recurrent neural network architecture that shows great promise, due to its ability to remember information and analyze data from both beginning to end, and end to beginning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep-pwning</h1>
                </header>
            
            <article>
                
<p>Deep-pwning is a framework for evaluating the robustness of machine learning tools against adversarial attacks. It has become widely known in the data science community that naive machine learning models, such as deep neural networks trained with the sole aim of classifying images, are very easily fooled.</p>
<p>The following diagram shows Explaining and Harnessing Adversarial Examples, I. J. Goodfellow et al:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span><img src="assets/9ac0d85b-0396-4dcd-af64-85b1d4e362f1.png" style="width:41.33em;height:13.92em;"/><br/></span></span></div>
<p>Cybersecurity being an adversarial field of battle, a machine learning model used to secure from attackers ought to be robust against adversaries. As a consequence, it is important to not only report the usual performance metrics, such as accuracy, precision, and recall, but also to have some measure of the adversarial robustness of the model. The deep-pwning framework is a simple toolkit for doing so.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<p>In preparation for this recipe, follow these steps:</p>
<ol>
<li>Install <kbd>git</kbd> on your device.</li>
</ol>
</div>
<ol start="2">
<li><span>Download or clone the repository using Git by using the following:</span></li>
</ol>
<div class="a-b-r">
<pre style="padding-left: 60px"><strong>git clone https://github.com/emmanueltsukerman/deep-pwning.git</strong></pre>
<ol start="3">
<li>Install the requirements for the repo.</li>
</ol>
<p style="padding-left: 60px">In a Terminal, go to the root directory of your repository and run the following command:</p>
<pre style="padding-left: 60px"><strong><span>pip install -r requirements.txt</span></strong></pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>In the following steps, you will utilize deep-pwning to attack LeNet5 on the MNIST digits dataset:</p>
<ol>
<li>From the directory down, run the MNIST driver using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>python mnist_driver.py –restore_checkpoint</strong></pre>
<p style="padding-left: 60px">The result should appear like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/850ff85a-8a0e-4458-80f8-9b13406ba064.png"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>In <em>step 1</em>, we create a large dataset of adversarial samples; namely, 150,000 adversarial samples are created, almost all of which are able to fool LeNet5 on digits. To examine these adversarial samples, unpickle the pickle in the output directory, like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1185 image-border" src="assets/bb8fd2b2-dd3f-4dbf-906c-6c8e2af2fdf7.png" style="width:35.75em;height:20.25em;"/></p>
<p>Under <kbd>utils</kbd>, a file named <kbd>mnist_read_pickle.py</kbd> takes as an argument the <kbd>pickle</kbd> file. Running it displays one of the adversarial samples. The following image tricks LeNet5 into thinking that it is seeing the number 1:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d48b2dcf-0f92-4bb1-a4a8-fdf33014de85.png" style="width:13.08em;height:13.08em;"/></p>
<p>The deep-pwning framework is designed to be modular, so a user plugs in and modifies pieces to suit their needs. For instance, replacing the MNIST dataset and the LeNet5 <span>architecture</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning-based system for the automatic detection of software vulnerabilities</h1>
                </header>
            
            <article>
                
<p>Experts in information security can usually identify potentially exploitable pieces of code. Yet, the work is intensive and costly, and may not be sufficient to make a program secure. One of the great advantages of deep learning over traditional machine learning is that features can be automatically discovered. This allows us to alleviate the need for a human expert on vulnerabilities, as well as to produce more effective systems. In this recipe, we'll utilize a modified version of <em>VulDeePecker : </em><span><em>A Deep Learning-Based System for Vulnerability Detection</em> (<a href="https://arxiv.org/pdf/1801.01681.pdf">https://arxiv.org/pdf/1801.01681.pdf</a>), to automatically detect buffer</span> error v<span>ulnerabilitie</span>s and res<span>ource management errors in C/C++ software.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>The preparation for this recipe consists of installing the <kbd>pandas</kbd>, <kbd>gensim</kbd>, <kbd>keras</kbd>, <kbd>tensorflow</kbd>, and <kbd>sklearn</kbd> packages in <kbd>pip</kbd>. The instructions are as follows:</p>
</div>
</div>
</div>
</div>
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<pre><strong>pip install pandas gensim keras tensorflow sklearn</strong></pre></div>
</div>
</div>
</div>
<p>In addition, for this recipe, clone the repository for VulDeePecker:</p>
<ol>
<li>Install <kbd>git</kbd> and then, in a Terminal, run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/emmanueltsukerman/Deep-Learning-Based-System-for-Automatic-Detection-of-Software-Vulnerabilities.git</strong></pre>
<p>Two datasets are available in the <span><kbd>datasets</kbd> </span>folder, <kbd>cwe119_cgd.7z</kbd> and <kbd>cwe399_cgd.7z</kbd>. If you wish to use them for this recipe, extract them.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<ol>
<li>Collect a training dataset of gadgets and place it under <kbd>datasets</kbd>. Two datasets are available in the <kbd>datasets</kbd> <span>folder,</span> and they are of this form:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1186 image-border" src="assets/5bbdf15e-6406-47bc-8453-6fb22a8ac5f2.png" style="width:77.92em;height:63.92em;"/></p>
<ol start="2">
<li>Train and test the deep learning model on your dataset.</li>
</ol>
<p style="padding-left: 60px">This is accomplished by running the following command:</p>
<pre style="padding-left: 60px"><strong>python vuldeepecker_train.py "path to dataset"</strong></pre>
<p style="padding-left: 60px"><strong> </strong>The output is displayed in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1187 image-border" src="assets/714ed965-843e-4b21-9bd8-1d3c857a5f23.png" style="width:158.08em;height:58.75em;"/></p>
<ol start="3">
<li>Collect the dataset you would like to predict on and place it under <kbd>datasets</kbd>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/86517346-69b4-49ec-97f7-9d758835d94b.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Use your trained model to predict whether these are vulnerable pieces of code by running the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>python vuldeepecker_predict.py "path to data" "path to model"</strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0422d9eb-52b2-4079-a859-beecd558e5de.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>For machine learning to work for vulnerability detection, you need to find representations of the software programs that are amenable to learning. For this purpose, we use code gadgets, which are transformed into vectors. A code gadget is a selection of lines of code that are semantically related to each other. In <em>step 1</em>, we collect such code gadgets for training. You can see an image of three code gadgets, along with labels. Here, a label of 1 indicates a vulnerability, while a label of 0 indicates no vulnerability. To extract gadgets from the desired program, it is advised to use the commercial product Checkmarx to extract program slices, and then assemble them into code gadgets. Another dataset is available. That dataset, <kbd>cwe-119</kbd>, corresponds to buffer error vulnerabilities. Next, we train a deep learning model on our vulnerability dataset (<em>step 2</em>). The deep learning model used is a <strong>Bidirectional Long Short-Term Memory</strong> (<strong>BLSTM</strong>), whose architecture is given as follows:</p>
<pre>Bidirectional(LSTM(300), input_shape=(50, 50))<br/>Dense(300)<br/>LeakyReLU()<br/>Dropout(0.5)<br/>Dense(300)<br/>LeakyReLU()<br/>Dropout(0.5)<br/>Dense(2, activation='softmax')<br/>Adamax(lr=0.002)<br/>'categorical_crossentropy'</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Note that the training phase automatically saves the model as <kbd>[base-name-of-training-dataset]_model.h5</kbd>. We are now ready to look for new vulnerabilities. So, we place a testing set in <kbd>datasets</kbd> (<em>step 3</em>) and then put our neural network to use by predicting vulnerabilities in this new set (<em>step 4</em>).</p>


            </article>

            
        </section>
    </body></html>