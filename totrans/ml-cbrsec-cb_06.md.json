["```\npip install sklearn\n```", "```\nimport os\n\nspam_emails_path = os.path.join(\"spamassassin-public-corpus\", \"spam\")\nham_emails_path = os.path.join(\"spamassassin-public-corpus\", \"ham\")\nlabeled_file_directories = [(spam_emails_path, 0), (ham_emails_path, 1)]\n```", "```\nemail_corpus = []\nlabels = []\n\nfor class_files, label in labeled_file_directories:\n    files = os.listdir(class_files)\n    for file in files:\n        file_path = os.path.join(class_files, file)\n        try:\n            with open(file_path, \"r\") as currentFile:\n                email_content = currentFile.read().replace(\"\\n\", \"\")\n                email_content = str(email_content)\n                email_corpus.append(email_content)\n                labels.append(label)\n        except:\n            pass\n```", "```\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    email_corpus, labels, test_size=0.2, random_state=11\n)\n```", "```\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\nfrom sklearn import tree\n\nnlp_followed_by_dt = Pipeline(\n    [\n        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 3))),\n        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n        (\"dt\", tree.DecisionTreeClassifier(class_weight=\"balanced\")),\n    ]\n)\nnlp_followed_by_dt.fit(X_train, y_train)\n```", "```\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\ny_test_pred = nlp_followed_by_dt.predict(X_test)\nprint(accuracy_score(y_test, y_test_pred))\nprint(confusion_matrix(y_test, y_test_pred))\n```", "```\n0.9761620977353993\n[[291 7]\n[ 13 528]]\n```", "```\npip install sklearn pandas\n```", "```\nimport pandas as pd\nimport os\n\ntrain_CSV = os.path.join(\"phishing-dataset\", \"train.csv\")\ntest_CSV = os.path.join(\"phishing-dataset\", \"test.csv\")\ntrain_df = pd.read_csv(train_CSV)\ntest_df = pd.read_csv(test_CSV)\n```", "```\ny_train = train_df.pop(\"target\").values\ny_test = test_df.pop(\"target\").values\n```", "```\nX_train = train_df.values\nX_test = test_df.values\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_test_pred = clf.predict(X_test)\nprint(accuracy_score(y_test, y_test_pred))\nprint(confusion_matrix(y_test, y_test_pred))\n```", "```\n0.9820846905537459\n[[343 4]\n[ 7 260]]\n```", "```\npip install pyshark\n```", "```\npip show pyshark\n```", "```\nimport pyshark\n\ncapture_time = 20\n```", "```\nimport datetime\nstart = datetime.datetime.now()\nend = start+datetime.timedelta(seconds=capture_time)\nfile_name = \"networkTrafficCatpureFrom\"+str(start).replace(\" \", \"T\")+\"to\"+str(end).replace(\" \",\"T\")+\".pcap\"\n```", "```\ncap = pyshark.LiveCapture(output_file=file_name)\ncap.sniff(timeout=capture_time)\n```", "```\npip install sklearn pandas matplotlib\n```", "```\nimport pandas as pd\n\nkdd_df = pd.read_csv(\"kddcup_dataset.csv\", index_col=None)\n```", "```\ny = kdd_df[\"label\"].values\nfrom collections import Counter\n\nCounter(y).most_common()\n```", "```\n[('normal', 39247),\n('back', 1098),\n('apache2', 794),\n('neptune', 93),\n('phf', 2),\n('portsweep', 2),\n('saint', 1)]\n```", "```\ndef label_anomalous(text):\n    \"\"\"Binarize target labels into normal or anomalous.\"\"\"\n    if text == \"normal\":\n        return 0\n    else:\n        return 1\n\nkdd_df[\"label\"] = kdd_df[\"label\"].apply(label_anomalous)\n```", "```\ny = kdd_df[\"label\"].values\ncounts = Counter(y).most_common()\ncontamination_parameter = counts[1][1] / (counts[0][1] + counts[1][1])\n```", "```\nfrom sklearn.preprocessing import LabelEncoder\n\nencodings_dictionary = dict()\nfor c in kdd_df.columns:\n    if kdd_df[c].dtype == \"object\":\n        encodings_dictionary[c] = LabelEncoder()\n        kdd_df[c] = encodings_dictionary[c].fit_transform(kdd_df[c])\n```", "```\nkdd_df_normal = kdd_df[kdd_df[\"label\"] == 0]\nkdd_df_abnormal = kdd_df[kdd_df[\"label\"] == 1]\ny_normal = kdd_df_normal.pop(\"label\").values\nX_normal = kdd_df_normal.values\ny_anomaly = kdd_df_abnormal.pop(\"label\").values\nX_anomaly = kdd_df_abnormal.values\n```", "```\nfrom sklearn.model_selection import train_test_split\n\nX_normal_train, X_normal_test, y_normal_train, y_normal_test = train_test_split(\n    X_normal, y_normal, test_size=0.3, random_state=11\n)\nX_anomaly_train, X_anomaly_test, y_anomaly_train, y_anomaly_test = train_test_split(\n    X_anomaly, y_anomaly, test_size=0.3, random_state=11\n)\n\nimport numpy as np\n\nX_train = np.concatenate((X_normal_train, X_anomaly_train))\ny_train = np.concatenate((y_normal_train, y_anomaly_train))\nX_test = np.concatenate((X_normal_test, X_anomaly_test))\ny_test = np.concatenate((y_normal_test, y_anomaly_test))\n```", "```\nfrom sklearn.ensemble import IsolationForest\n\nIF = IsolationForest(contamination=contamination_parameter)\nIF.fit(X_train\n```", "```\ndecisionScores_train_normal = IF.decision_function(X_normal_train)\ndecisionScores_train_anomaly = IF.decision_function(X_anomaly_train)\n```", "```\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.figure(figsize=(20, 10))\n_ = plt.hist(decisionScores_train_normal, bins=50)\n```", "```\nplt.figure(figsize=(20, 10))\n_ = plt.hist(decisionScores_train_anomaly, bins=50)\n```", "```\ncutoff = 0\n```", "```\nprint(Counter(y_test))\nprint(Counter(y_test[cutoff > IF.decision_function(X_test)]))\n```", "```\nCounter({0: 11775, 1: 597})\nCounter({1: 595, 0: 85})\n```", "```\npip install sklearn\n```", "```\nimport pickle\n\nfile = open('CTU13Scenario1flowData.pickle', 'rb')\nbotnet_dataset = pickle.load(file)\n```", "```\nX_train, y_train, X_test, y_test = (\n    botnet_dataset[0],\n    botnet_dataset[1],\n    botnet_dataset[2],\n    botnet_dataset[3],\n)\n```", "```\nfrom sklearn.tree import *\n\nclf = DecisionTreeClassifier()\n```", "```\nclf.fit(X_train, y_train)\n```", "```\nclf.score(X_test, y_test)\n```", "```\n0.9991001799640072\n```", "```\npip install pandas\n```", "```\nimport numpy as np\nimport pandas as pd path_to_dataset = \"./r42short/\"\n```", "```\nlog_types = [\"device\", \"email\", \"file\", \"logon\", \"http\"]\nlog_fields_list = [\n    [\"date\", \"user\", \"activity\"],\n    [\"date\", \"user\", \"to\", \"cc\", \"bcc\"],\n    [\"date\", \"user\", \"filename\"],\n    [\"date\", \"user\", \"activity\"],\n    [\"date\", \"user\", \"url\"],\n]\n```", "```\nfeatures = 0\nfeature_map = {}\n\ndef add_feature(name):\n    \"\"\"Add a feature to a dictionary to be encoded.\"\"\"\n    if name not in feature_map:\n        global features\n        feature_map[name] = features\n        features += 1\n```", "```\nadd_feature(\"Weekday_Logon_Normal\")\nadd_feature(\"Weekday_Logon_After\")\nadd_feature(\"Weekend_Logon\")\nadd_feature(\"Logoff\")\n\nadd_feature(\"Connect_Normal\")\nadd_feature(\"Connect_After\")\nadd_feature(\"Connect_Weekend\")\nadd_feature(\"Disconnect\")\n\nadd_feature(\"Email_In\")\nadd_feature(\"Email_Out\")\n\nadd_feature(\"File_exe\")\nadd_feature(\"File_jpg\")\nadd_feature(\"File_zip\")\nadd_feature(\"File_txt\")\nadd_feature(\"File_doc\")\nadd_feature(\"File_pdf\")\nadd_feature(\"File_other\")\n\nadd_feature(\"url\")\n```", "```\ndef file_features(row):\n    \"\"\"Creates a feature recording the file extension of the file used.\"\"\"\n    if row[\"filename\"].endswith(\".exe\"):\n        return feature_map[\"File_exe\"]\n    if row[\"filename\"].endswith(\".jpg\"):\n        return feature_map[\"File_jpg\"]\n    if row[\"filename\"].endswith(\".zip\"):\n        return feature_map[\"File_zip\"]\n    if row[\"filename\"].endswith(\".txt\"):\n        return feature_map[\"File_txt\"]\n    if row[\"filename\"].endswith(\".doc\"):\n        return feature_map[\"File_doc\"]\n    if row[\"filename\"].endswith(\".pdf\"):\n        return feature_map[\"File_pdf\"]\n    else:\n        return feature_map[\"File_other\"]\n```", "```\ndef email_features(row):\n    \"\"\"Creates a feature recording whether an email has been sent externally.\"\"\"\n    outsider = False\n    if not pd.isnull(row[\"to\"]):\n        for address in row[\"to\"].split(\";\"):\n            if not address.endswith(\"dtaa.com\"):\n                outsider = True\n\n    if not pd.isnull(row[\"cc\"]):\n        for address in row[\"cc\"].split(\";\"):\n            if not address.endswith(\"dtaa.com\"):\n                outsider = True\n\n    if not pd.isnull(row[\"bcc\"]):\n        for address in row[\"bcc\"].split(\";\"):\n            if not address.endswith(\"dtaa.com\"):\n                outsider = True\n    if outsider:\n        return feature_map[\"Email_Out\"]\n    else:\n        return feature_map[\"Email_In\"]\n```", "```\ndef device_features(row):\n    \"\"\"Creates a feature for whether the user has connected during normal hours or otherwise.\"\"\"\n    if row[\"activity\"] == \"Connect\":\n        if row[\"date\"].weekday() < 5:\n            if row[\"date\"].hour >= 8 and row[\"date\"].hour < 17:\n                return feature_map[\"Connect_Normal\"]\n            else:\n                return feature_map[\"Connect_After\"]\n        else:\n            return feature_map[\"Connect_Weekend\"]\n    else:\n        return feature_map[\"Disconnect\"]\n```", "```\ndef logon_features(row):\n    \"\"\"Creates a feature for whether the user logged in during normal hours or otherwise.\"\"\"\n    if row[\"activity\"] == \"Logon\":\n        if row[\"date\"].weekday() < 5:\n            if row[\"date\"].hour >= 8 and row[\"date\"].hour < 17:\n                return feature_map[\"Weekday_Logon_Normal\"]\n            else:\n                return feature_map[\"Weekday_Logon_After\"]\n        else:\n            return feature_map[\"Weekend_Logon\"]\n    else:\n        return feature_map[\"Logoff\"]\n```", "```\ndef http_features(row):\n    \"\"\"Encodes the URL visited.\"\"\"\n    return feature_map[\"url\"]\n```", "```\ndef date_to_day(row):\n    \"\"\"Converts a full datetime to date only.\"\"\"\n    day_only = row[\"date\"].date()\n    return day_only\n```", "```\nlog_feature_functions = [\n    device_features,\n    email_features,\n    file_features,\n    logon_features,\n    http_features,\n]\ndfs = []\nfor i in range(len(log_types)):\n    log_type = log_types[i]\n    log_fields = log_fields_list[i]\n    log_feature_function = log_feature_functions[i]\n    df = pd.read_csv(\n        path_to_dataset + log_type + \".csv\", usecols=log_fields, index_col=None\n    )\n```", "```\n    date_format = \"%m/%d/%Y %H:%M:%S\"\n    df[\"date\"] = pd.to_datetime(df[\"date\"], format=date_format)\n```", "```\n    new_feature = df.apply(log_feature_function, axis=1)\n    df[\"feature\"] = new_feature\n\n    cols_to_keep = [\"date\", \"user\", \"feature\"]\n    df = df[cols_to_keep]\n```", "```\n    df[\"date\"] = df.apply(date_to_day, axis=1)\n\n    dfs.append(df)\n```", "```\njoint = pd.concat(dfs)\njoint = joint.sort_values(by=\"date\")\n```", "```\npip install sklearn pandas matplotlib\n```", "```\nthreat_actors = [\n    \"AAM0658\",\n    \"AJR0932\",\n    \"BDV0168\",\n    <snip>\n    \"MSO0222\",\n]\n```", "```\nstart_date = joint[\"date\"].iloc[0]\nend_date = joint[\"date\"].iloc[-1]\ntime_horizon = (end_date - start_date).days + 1\n\ndef date_to_index(date):\n    \"\"\"Indexes dates by counting the number of days since the starting date of the dataset.\"\"\"\n    return (date - start_date).days\n```", "```\ndef extract_time_series_by_user(user_name, df):\n    \"\"\"Filters the dataframe down to a specific user.\"\"\"\n    return df[df[\"user\"] == user_name]\n```", "```\ndef vectorize_user_time_series(user_name, df):\n    \"\"\"Convert the sequence of features of a user to a vector-valued time series.\"\"\"\n    user_time_series = extract_time_series_by_user(user_name, df)\n    x = np.zeros((len(feature_map), time_horizon))\n    event_date_indices = user_time_series[\"date\"].apply(date_to_index).to_numpy()\n    event_features = user_time_series[\"feature\"].to_numpy()\n    for i in range(len(event_date_indices)):\n        x[event_features[i], event_date_indices[i]] += 1\n    return x\n```", "```\ndef vectorize_dataset(df):\n    \"\"\"Takes the dataset and featurizes it.\"\"\"\n    users = set(df[\"user\"].values)\n    X = np.zeros((len(users), len(feature_map), time_horizon))\n    y = np.zeros((len(users)))\n    for index, user in enumerate(users):\n        x = vectorize_user_time_series(user, df)\n        X[index, :, :] = x\n        y[index] = int(user in threat_actors)\n    return X, y\n```", "```\nX, y = vectorize_dataset(joint)\n```", "```\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n```", "```\nX_train_reshaped = X_train.reshape(\n    [X_train.shape[0], X_train.shape[1] * X_train.shape[2]]\n)\nX_test_reshaped = X_test.reshape([X_test.shape[0], X_test.shape[1] * X_test.shape[2]])\n```", "```\nX_train_normal = X_train_reshaped[y_train == 0, :]\nX_train_threat = X_train_reshaped[y_train == 1, :]\nX_test_normal = X_test_reshaped[y_test == 0, :]\nX_test_threat = X_test_reshaped[y_test == 1, :]\n```", "```\nfrom sklearn.ensemble import IsolationForest\n\ncontamination_parameter = 0.035\nIF = IsolationForest(\n    n_estimators=100, max_samples=256, contamination=contamination_parameter\n)\n```", "```\nIF.fit(X_train_reshaped)\n```", "```\nnormal_scores = IF.decision_function(X_train_normal)\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(8, 4), dpi=600, facecolor=\"w\", edgecolor=\"k\")\n\nnormal = plt.hist(normal_scores, 50, density=True)\n\nplt.xlim((-0.2, 0.2))\nplt.xlabel(\"Anomaly score\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Distribution of anomaly score for non threats\")\n```", "```\nanomaly_scores = IF.decision_function(X_train_threat)\nfig = plt.figure(figsize=(8, 4), dpi=600, facecolor=\"w\", edgecolor=\"k\")\n\nanomaly = plt.hist(anomaly_scores, 50, density=True)\n\nplt.xlim((-0.2, 0.2))\nplt.xlabel(\"Anomaly score\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Distribution of anomaly score for threats\") \n```", "```\ncutoff = 0.12\n```", "```\nfrom collections import Counter\n\ns = IF.decision_function(X_train_reshaped)\nprint(Counter(y_train[cutoff > s]))\n```", "```\nCounter({0.0: 155, 1.0: 23})\n```", "```\ns = IF.decision_function(X_test_reshaped)\nprint(Counter(y_test[cutoff > s]))\n```", "```\nCounter({0.0: 46, 1.0: 8})\n```", "```\npip install sklearn pandas\n```", "```\nimport pandas as pd\n\nfeatures = [\n    \"Fwd Seg Size Min\",\n    \"Init Bwd Win Byts\",\n    \"Init Fwd Win Byts\",\n    \"Fwd Seg Size Min\",\n    \"Fwd Pkt Len Mean\",\n    \"Fwd Seg Size Avg\",\n    \"Label\",\n    \"Timestamp\",\n]\ndtypes = {\n    \"Fwd Pkt Len Mean\": \"float\",\n    \"Fwd Seg Size Avg\": \"float\",\n    \"Init Fwd Win Byts\": \"int\",\n    \"Init Bwd Win Byts\": \"int\",\n    \"Fwd Seg Size Min\": \"int\",\n    \"Label\": \"str\",\n}\ndate_columns = [\"Timestamp\"]\n```", "```\ndf = pd.read_csv(\"ddos_dataset.csv\", usecols=features, dtype=dtypes,parse_dates=date_columns,index_col=None)\n```", "```\ndf2 = df.sort_values(\"Timestamp\")\n```", "```\ndf3 = df2.drop(columns=[\"Timestamp\"])\n```", "```\nl = len(df3.index)\ntrain_df = df3.head(int(l * 0.8))\ntest_df = df3.tail(int(l * 0.2))\n```", "```\ny_train = train_df.pop(\"Label\").values\ny_test = test_df.pop(\"Label\").values\n```", "```\nX_train = train_df.values\nX_test = test_df.values\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=50)\n```", "```\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n```", "```\n0.83262\n```", "```\npip install sklearn pandas matplotlib costcla\n```", "```\nimport pandas as pd\n\nfraud_df = pd.read_csv(\"FinancialFraudDB.csv\", index_col=None)\n```", "```\ncard_replacement_cost = 5\ncustomer_freeze_cost = 3\n```", "```\nimport numpy as np\n\ncost_matrix = np.zeros((len(fraud_df.index), 4))\ncost_matrix[:, 0] = customer_freeze_cost * np.ones(len(fraud_df.index))\ncost_matrix[:, 1] = fraud_df[\"Amount\"].values\ncost_matrix[:, 2] = card_replacement_cost * np.ones(len(fraud_df.index))\n```", "```\ny = fraud_df.pop(\"Class\").values\nX = fraud_df.values\n```", "```\nfrom sklearn.model_selection import train_test_split\n\nsets = train_test_split(X, y, cost_matrix, test_size=0.25, random_state=11)\nX_train, X_test, y_train, y_test, cost_matrix_train, cost_matrix_test = sets\n```", "```\nfrom sklearn import tree\n\ny_pred_test_dt = tree.DecisionTreeClassifier().fit(X_train, y_train).predict(X_test)\n```", "```\nfrom costcla.models import CostSensitiveDecisionTreeClassifier\n\ny_pred_test_csdt = CostSensitiveDecisionTreeClassifier().fit(X_train, y_train, cost_matrix_train).predict(X_test)\n```", "```\nfrom costcla.metrics import savings_score\n\nprint(savings_score(y_test, y_pred_test_dt, cost_matrix_test))\nprint(savings_score(y_test, y_pred_test_csdt, cost_matrix_test))\n```", "```\n0.5231523713991505 0.5994028394464614\n```", "```\npip install sklearn pandas\n```", "```\nimport pandas as pd\n\ndf = pd.read_csv(\"data_banknote_authentication.txt\", header=None)\ndf.columns = [\"0\", \"1\", \"2\", \"3\", \"label\"]\n```", "```\nfeature 1 feature 2 feature 3 feature 4 label\n0 3.62160 8.6661 -2.8073 -0.44699 0\n1 4.54590 8.1674 -2.4586 -1.46210 0\n2 3.86600 -2.6383 1.9242 0.10645 0\n3 3.45660 9.5228 -4.0112 -3.59440 0\n4 0.32924 -4.4552 4.5718 -0.98880 0\n```", "```\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df)\n```", "```\ny_train = df_train.pop(\"label\").values\nX_train = df_train.values\ny_test = df_test.pop(\"label\").values\nX_test = df_test.values\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\n```", "```\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\n```", "```\n0.9825072886297376\n```", "```\npip install sklearn pandas\n```", "```\nimport pandas as pd\n\ndf = pd.read_csv(\"ad.data\", header=None)\ndf.rename(columns={1558: \"label\"}, inplace=True)\n```", "```\nimproper_rows = []\nfor index, row in df.iterrows():\n    for col in df.columns:\n        val = str(row[col]).strip()\n        if val == \"?\":\n            improper_rows.append(index)\n```", "```\ndf = df.drop(df.index[list(set(improper_rows))])\n```", "```\ndef label_to_numeric(row):\n    \"\"\"Binarize the label.\"\"\"\n    if row[\"label\"] == \"ad.\":\n        return 1\n    else:\n        return 0\n\ndf[\"label\"] = df.apply(label_to_numeric, axis=1)\n```", "```\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df)\n```", "```\ny_train = df_train.pop(\"label\").values\ny_test = df_test.pop(\"label\").values\nX_train = df_train.values\nX_test = df_test.values\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n```", "```\nclf.score(X_test, y_test)\n```", "```\n0.9847457627118644\n```", "```\npip install sklearn pandas\n```", "```\nimport pandas as pd\n\ndf = pd.read_csv(\"wifi_localization.txt\", sep=\"\\t\", header=None)\ndf = df.rename(columns={7: \"room\"})\n```", "```\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df)\n```", "```\ny_train = df_train.pop(\"room\").values\ny_test = df_test.pop(\"room\").values\nX_train = df_train.values\nX_test = df_test.values\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\n```", "```\nclf.fit(X_train, y_train)\n```", "```\ny_pred = clf.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\n```", "```\n[[124   0   0   0]\n [  0 124   4   0]\n [  0   2 134   0]\n [  1   0   0 111]]\n```"]