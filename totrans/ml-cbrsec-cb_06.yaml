- en: Automatic Intrusion Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An intrusion detection system monitors a network or a collection of systems
    for malicious activity or policy violations. Any malicious activity or violation
    caught is stopped or reported. In this chapter, we will design and implement several
    intrusion detection systems using machine learning. We will begin with the classical
    problem of detecting spam email. We will then move on to classifying malicious
    URLs. We will take a brief detour to explain how to capture network traffic, so
    that we may tackle more challenging network problems, such as botnet and DDoS
    detection. We will construct a classifier for insider threats. Finally, we will
    address the example-dependent, cost-sensitive, radically imbalanced, and challenging
    problem of credit card fraud.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Spam filtering using machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phishing URL detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing network traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network behavior anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Botnet traffic detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering for insider threat detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing anomaly detection for insider threats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting DDoS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credit card fraud detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfeit bank note detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ad blocking using machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wireless indoor localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical prerequisites for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Wireshark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyShark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: costcla
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code and datasets may be found at [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter06](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: Spam filtering using machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spam mails (unwanted mails) constitute around 60% of global email traffic. Aside
    from the fact that spam detection software has progressed since the first spam
    message in 1978, anyone with an email account knows that spam continues to be
    a time-consuming and expensive problem. Here, we provide a recipe for spam-ham
    (non-spam) classification using machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing the `scikit-learn` package
    in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In addition, extract `spamassassin-public-corpus.7z` into a folder named `spamassassin-public-corpus`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we build a classifier for wanted and unwanted email:'
  prefs: []
  type: TYPE_NORMAL
- en: Unzip the `spamassassin-public-corpus.7z` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Specify the path of your `spam` and `ham` directories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create labels for the two classes and read the emails into a corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Train-test split the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Train an NLP pipeline on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the classifier on the testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by preparing a dataset consisting of raw emails (*Step 1*), which the
    reader can examine by looking at the dataset. In *Step 2*, we specify the paths
    of the spam and ham emails, as well as assign labels to their directories. We
    proceed to read all of the emails into an array, and create a labels array in
    *Step 3*. Next, we train-test split our dataset (*Step 4*), and then fit an NLP
    pipeline on it in *Step 5*. Finally, in *Step 6*, we test our pipeline. We see
    that accuracy is pretty high. Since the dataset is relatively balanced, there
    is no need to use special metrics to evaluate success.
  prefs: []
  type: TYPE_NORMAL
- en: Phishing URL detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A phishing website is a website that tries to obtain your account password or
    other personal information by making you think that you are on a legitimate website.
    Some phishing URLs differ from the intended URL in a single character specially
    chosen to increase the odds of a typo, while others utilize other channels to
    generate traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a phishing website attempting to obtain a user''s email
    address by pressuring a user into believing that their email will be shut down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a1e670df-c239-4c89-b24c-7251a90549ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Since phishing is one of the most successful modes of attack, it is crucial
    to be able to identify when a URL is not legitimate. In this recipe, we will build
    a machine learning model to detect phishing URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn` and `pandas`
    in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In addition, extract the archive named `phishing-dataset.7z`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following steps, we will read in a featurized dataset of URLs and train
    a classifier on it.
  prefs: []
  type: TYPE_NORMAL
- en: Download the phishing dataset from this chapter's directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read in the training and testing data using `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare the labels of the phishing web pages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Train, test, and assess a classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin by downloading the dataset, and then reading it into data frames (*Steps
    1* and *2*) for convenient examination and manipulation. Moving on, we place the
    dataset into arrays in preparation for machine learning (*Steps 3* and *4*). The
    dataset consists of several thousand feature vectors for phishing URLs. There
    are 30 features, whose names and values are tabulated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attributes | Values | Column name |'
  prefs: []
  type: TYPE_TB
- en: '| Having an IP address | { 1,0 } | `has_ip` |'
  prefs: []
  type: TYPE_TB
- en: '| Having a long URL | { 1,0,-1 } | `long_url` |'
  prefs: []
  type: TYPE_TB
- en: '| Uses Shortening Service | { 0,1 } | `short_service` |'
  prefs: []
  type: TYPE_TB
- en: '| Having the ''@'' symbol | { 0,1 } | `has_at` |'
  prefs: []
  type: TYPE_TB
- en: '| Double slash redirecting | { 0,1 } | `double_slash_redirect` |'
  prefs: []
  type: TYPE_TB
- en: '| Having a prefix and suffix | { -1,0,1 } | `pref_suf` |'
  prefs: []
  type: TYPE_TB
- en: '| Having a subdomain | { -1,0,1 } | `has_sub_domain` |'
  prefs: []
  type: TYPE_TB
- en: '| SSLfinal state | { -1,1,0 } | `ssl_state` |'
  prefs: []
  type: TYPE_TB
- en: '| Domain registration length | { 0,1,-1 } | `long_domain` |'
  prefs: []
  type: TYPE_TB
- en: '| Favicon | { 0,1 } | `favicon` |'
  prefs: []
  type: TYPE_TB
- en: '| Is a standard port | { 0,1 } | `port` |'
  prefs: []
  type: TYPE_TB
- en: '| Uses HTTPS tokens | { 0,1 } | `https_token` |'
  prefs: []
  type: TYPE_TB
- en: '| Request_URL | { 1,-1 } | `req_url` |'
  prefs: []
  type: TYPE_TB
- en: '| Abnormal URL anchor | { -1,0,1 } | `url_of_anchor` |'
  prefs: []
  type: TYPE_TB
- en: '| Links_in_tags | { 1,-1,0 } | `tag_links` |'
  prefs: []
  type: TYPE_TB
- en: '| SFH | { -1,1 } | `SFH` |'
  prefs: []
  type: TYPE_TB
- en: '| Submitting to email | { 1,0 } | `submit_to_email` |'
  prefs: []
  type: TYPE_TB
- en: '| Abnormal URL | { 1,0 } | `abnormal_url` |'
  prefs: []
  type: TYPE_TB
- en: '| Redirect | { 0,1 } | `redirect` |'
  prefs: []
  type: TYPE_TB
- en: '| On mouseover | { 0,1 } | `mouseover` |'
  prefs: []
  type: TYPE_TB
- en: '| Right-click | { 0,1 } | `right_click` |'
  prefs: []
  type: TYPE_TB
- en: '| Pop-up window | { 0,1 } | `popup` |'
  prefs: []
  type: TYPE_TB
- en: '| Iframe | { 0,1 } | `iframe` |'
  prefs: []
  type: TYPE_TB
- en: '| Age of domain | { -1,0,1 } | `domain_age` |'
  prefs: []
  type: TYPE_TB
- en: '| DNS record | { 1,0 } | `dns_record` |'
  prefs: []
  type: TYPE_TB
- en: '| Web traffic | { -1,0,1 } | `traffic` |'
  prefs: []
  type: TYPE_TB
- en: '| Page rank | { -1,0,1 } | `page_rank` |'
  prefs: []
  type: TYPE_TB
- en: '| Google index | { 0,1 } | `google_index` |'
  prefs: []
  type: TYPE_TB
- en: '| Links pointing to page | { 1,0,-1 } | `links_to_page` |'
  prefs: []
  type: TYPE_TB
- en: '| Statistical report | { 1,0 } | `stats_report` |'
  prefs: []
  type: TYPE_TB
- en: '| Result | { 1,-1 } | `target` |'
  prefs: []
  type: TYPE_TB
- en: In *Step 5*, we train and test a random forest classifier. The accuracy is pretty
    high, but depending on how balanced the dataset is, it might be necessary to consider
    an FP constraint. There are many ways to expand upon such a detector, such as
    by adding other features and growing the dataset. Given that most websites contain
    some images, an image classifier is just one way in which the model may improve
    its results.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing network traffic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capturing network traffic is important for troubleshooting, analysis, and software
    and communications protocol development. For the security-minded individual, monitoring
    network traffic is crucial for detecting malicious activity or policy violation.
    In this recipe, we will demonstrate how to capture and inspect network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In preparation for this recipe, observe the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `pyshark`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Install `wireshark`. The latest version can be found at [https://www.wireshark.org/download.html](https://www.wireshark.org/download.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following steps, we utilize a Python library named PyShark, along with
    Wireshark, to capture and examine network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'You must add `tshark` to PyShark''s configuration path. Tshark is a command-line
    variant of Wireshark. To do this, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note the location of the package. In the `pyshark` directory in this location,
    find the file, `config.ini`. Edit `tshark_path` to the location of `tshark` inside
    your `wireshark` installation folder. Similarly, edit `dumpcap_path` to the location
    of `dumpcap` inside your `wireshark` installation folder.
  prefs: []
  type: TYPE_NORMAL
- en: '*Steps 2* and *4* should be executed in a Python environment. Note that, as
    of the current version, `pyshark` may have some bugs when run in a Jupyter notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pyshark` and specify the duration of the capture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the name of the file to output the capture, `to`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Capture network traffic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To examine the capture, open the `pcap` file in Wireshark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/f1bee8ed-2b47-411b-a417-3af9f6829a1d.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start this recipe by configuring `tshark`, the command-line variant of Wireshark.
    Once we are finished configuring `tshark`, it is now accessible through `pyshark`.
    We import `pyshark` and specify the duration of the network capture (*Step 2*).
    Captured network traffic data can be overwhelming in size, so it is important
    to control the duration. Next, we specify the name of the output capture in a
    way that makes it unique and easily understandable (*Step 3*), and then, in *Step
    4*, we proceed to capture traffic. Finally, in *Step 6*, we employ Wireshark for
    its GUI to examine the captured network traffic. In able hands, such network traffic
    facilitates the detection of insecure IoT devices, misconfigurations, anomalous
    events, hacking attempts, and even data exfiltration.
  prefs: []
  type: TYPE_NORMAL
- en: Network behavior anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Network behavior anomaly detection** (**NBAD**) is the continuous monitoring
    of a network for unusual events or trends. Ideally, an NBAD program tracks critical
    network characteristics in real time and generates an alarm if a strange event
    or trend is detected that indicates a threat. In this recipe, we will build an
    NBAD using machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used is a modified subset from a famous dataset known as the KDD
    dataset, and is a standard set for testing and constructing IDS systems. This
    dataset contains a wide variety of intrusions simulated in a military network
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn`, `pandas`, and
    `matplotlib`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In addition, extract the archive, `kddcup_dataset.7z`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will utilize isolation forest to detect anomalies
    in the KDD dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and read the dataset into a data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the proportion of types of traffic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output will be observed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert all non-normal observations into a single class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtain the ratio of anomalies to normal observations. This is the contamination
    parameter that will be used in our isolation forest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert all categorical features into numerical form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the dataset into normal and abnormal observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Train-test split the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate and train an isolation forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Score the classifier on normal and anomalous observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the scores for the normal set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph provides the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e34915a8-d4cf-467a-bedd-25d7c7fe16d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, plot the scores on the anomalous observations for a visual examination:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph provides the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8d5553be-1729-4ae1-b3bd-7b25c92c7fcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select a cut-off so as to separate out the anomalies from the normal observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine this cut-off on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by reading the `KDD cup` dataset into a data frame. Next, in *Step
    2*, we examine our data, to see that a majority of the traffic is normal, as expected,
    but a small amount is abnormal. Evidently, the problem is highly imbalanced. Consequently,
    this problem is a promising candidate for an anomaly detection approach. In *Steps
    3* and *5*, we transform all non-normal traffic into a single class, namely, **anomalous**.
  prefs: []
  type: TYPE_NORMAL
- en: We also make sure to compute the ratio of anomalies to normal observations (*Step
    4*), known as the contamination parameter. This is one of the parameters that
    facilitates setting of the sensitivity of isolation forest. This is optional,
    but is likely to improve performance. We split our dataset into normal and anomalous
    observations in *Step 6*, as well as split our dataset into training and testing
    versions of the normal and anomalous data (*Step 7*). We instantiate an isolation
    forest classifier, and set its contamination parameter (*Step 8*). The default
    parameters, `n_estimators` and `max_samples`, are recommended in the paper *Isolation
    Forest* by Liu et al. In *Steps 9* and *10*, we use the decision function of isolation
    forest to provide a score to the normal training set, and then examine the results
    in a plot. In *Step 11*, we similarly provide a score to the anomalous training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that the decision function is a measure of how simple a point is to
    describe, we would like to separate out simple points from complicated points
    by picking a numerical cut-off that gives clear separation. A visual examination
    suggests the value chosen in *Step 12*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can use our model to make predictions and provide an assessment
    of its performance. In *Step 13*, we see that the model was able to pick up on
    a large number of anomalies without triggering too many false positives (instances
    of normal traffic), speaking proportion-wise.
  prefs: []
  type: TYPE_NORMAL
- en: Botnet traffic detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A botnet is a network of internet-connected compromised devices. Botnets can
    be used to perform a distributed **denial-of-service attack** (**DDoS attack**),
    steal data, send spam, among many other creative malicious uses. Botnets can cause
    absurd amounts of damage. For example, a quick search for the word botnet on Google
    shows that 3 days before the time of writing, the Electrum Botnet Stole $4.6 Million
    in cryptocurrencies. In this recipe, we build a classifier to detect botnet traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used is a processed subset of a dataset called **CTU-13**, and consists
    of botnet traffic captured in Czechia, at the CTU University in 2011\. The dataset
    is a large capture of real botnet traffic mixed with normal and background traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn` in `pip`. The
    command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, extract `CTU13Scenario1flowData.7z`. To unpickle the `CTU13Scenario1flowData.pickle`
    file, you will need to use Python 2:'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Begin by reading in the pickled data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is already split into train-test sets, and you only need assign these
    to their respective variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a decision tree classifier with default parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the classifier to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Test it on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin *Step 1* by loading the data by unpickling it. The dataset has been
    pre-engineered to be balanced, so we do not need to worry about imbalanced data
    challenges. In practice, the detection of botnets may require satisfying a constraint
    on false positives. Moving on, we utilize the already predefined train-test split
    to split our data (*Step 2*). We can now instantiate a classifier, fit it to the
    data, and then test it (*Steps 3* and *5*). Looking at the accuracy, we see that
    it is quite high. Since the dataset is already balanced, we need not worry that
    our metric is misleading. In general, detecting botnets can be challenging. The
    difficulty in detecting botnets is illustrated by the GameOver Zeus botnet malware
    package. Originally discovered in 2007, it operated for over three years, eventually
    resulting in an estimated $70 million in stolen funds that led to the arrest of
    over a hundred individuals by the FBI in 2010\. It wasn't until March 2012 that
    Microsoft announced that it was able to shut down the majority of the botnet's
    command and control (C&C) servers.
  prefs: []
  type: TYPE_NORMAL
- en: Insider threat detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Insider threat is a complex and growing challenge for employers. It is generally
    defined as any actions taken by an employee that are potentially harmful to the
    organization. These can include actions such as unsanctioned data transfer or
    the sabotaging of resources. Insider threats may manifest in various and novel
    forms motivated by differing goals, ranging from a disgruntled employee subverting
    the prestige of an employer, to **advanced persistent threats** (**APT**).
  prefs: []
  type: TYPE_NORMAL
- en: The insider risk database of the CERT Program of the Carnegie Mellon University
    Software Engineering Institute contains the largest public archive of red team
    scenarios. The simulation is built by combining real-world insider risk case studies
    with actual neutral clients secretly obtained from a defense corporation. The
    dataset represents months of traffic in a single engineering company from internet,
    phone, logon, folder, and system access (dtaa.com). The mock company employs several
    thousand people who each perform an average of 1,000 logged activities per day.
    There are several threat scenarios depicted, such as a leaker, thief, and saboteur.
    A notable feature of the issue is its very low signal-to-noise, whether this is
    expressed in total malicious users, frequent tallies, or overall usage.
  prefs: []
  type: TYPE_NORMAL
- en: The analysis we perform is on the CERT insider threat scenario (v.4.2), specifically
    because it represents a dense needle dataset, meaning that it has a high incidence
    of attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic plan of attack is, first, to hand-engineer new features, such as
    whether an email has been sent to an outsider or a login has occurred outside
    of business hours. Next, the idea is to extract a multivariate time series per
    user. This time series will consist of a sequence of vectors—each vector constitutes
    a count of the number of times our hand-engineered features took place in a day.
    Hence, the shape of our input dataset will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '(# of users, total # of features examined per day, # of days in the time series).'
  prefs: []
  type: TYPE_NORMAL
- en: We will then flatten the time series of each user, and utilize isolation forest
    to detect anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering for insider threat detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, whenever a machine learning solution does not rely on end-to-end
    deep learning, performance can be improved by creating insightful and informative
    features. In this recipe, we will construct several promising new features for
    insider threat detection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `pandas` in `pip`. The command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, download the CERT insider threat dataset from the following link:
    [ftp://ftp.sei.cmu.edu/pub/cert-data/r4.2.tar.bz2](ftp://ftp.sei.cmu.edu/pub/cert-data/r4.2.tar.bz2).
    More information about the dataset, as well as answers, can be found at [https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=508099](https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=508099).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will construct new features for the CERT insider
    threat dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `numpy` and `pandas`, and point to where the downloaded data is located:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the `.csv` files and which of their columns to read:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We will hand-engineer a number of features and encode them, thereby creating
    a dictionary to track these.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the features we will be using to our dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to note the file type that was copied to removable media:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to identify whether an employee has sent an email to a non-company
    email:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to note whether the employee used removable media outside
    of business hours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to note whether an employee has logged onto a machine outside
    of business hours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We will not take advantage of the information contained in URLs visited by
    an employee:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We preserve only the day when an event has occurred, rather than the full timestamp:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We loop over the `.csv` files containing the logs and read them into pandas
    data frames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the `date` data to a `pandas` timestamp:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the new features defined above and then drop all features except the
    date, user, and our new feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the date to just a day:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate all the data frames into one and sort by `date`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start by importing `pandas` and `numpy` and creating a variable pointing to
    the dataset (*Step 1*). There are several datasets available from CERT. Version
    4.2 is distinguished in being a dense needle dataset, meaning that it has a higher
    incidence of insider threats than the other datasets. Since the dataset is so
    massive, it is convenient to filter and downsample it, at the very least during
    the experimentation phases, so we do so in *Step 2*. In the following steps, we
    will hand-engineer features that we believe will help our classifier catch insider
    threats. In *Step 3*, we create a convenient function to encode features, so that
    a dictionary can track these. We provide the names of the features we will be
    adding in *Step 4*. In *Step 5*, we create a feature that will track the file
    type of a file copied to removable media. Presumably, this is indicative of criminal
    data leaking. In *Step 6*, we create a feature that tracks whether the employee
    has emailed an external entity. We create another feature to track whether an
    employee has used a removable media device outside of business hours (*Step 7*).
  prefs: []
  type: TYPE_NORMAL
- en: An additional feature tracks whether an employee has logged into a device outside
    of business hours (*Step 8*). For simplicity, we do not utilize the URLs visited
    by employees (*Step 9*), though these may be indicative of malicious behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we simplify our data by using only the date (*Step 10*), rather than the
    full timestamp in our featurized data. In *Step 11*, we read our data into a pandas
    data frame. We then edit the current date format to fit pandas (*Step 12*), and
    then gather up all of the new features, while dropping the old ones (*Step 13*).
    In *Step 14*, we transform the data into a time series whose delta are single
    days. Finally, in *Step 15*, we aggregate all of the data into one large sorted
    data frame. We have now completed the first iteration of the feature-engineering
    phase. There are many directions you can pursue in order to improve performance
    and add features. These include observing email text for negative sentiment and
    analyzing personality using psychometrics.
  prefs: []
  type: TYPE_NORMAL
- en: Employing anomaly detection for insider threats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having engineered promising new features, our next steps are to train-test split,
    process the data into a convenient time series form, and then classify. Our training
    and testing sets will be two temporal halves of the dataset. This way, we can
    easily ensure that the shape of the input for training is the same as the shape
    of the input for testing, without cheating in our evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn`, `pandas`, and
    `matplotlib` in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In preparation for this recipe, you will want to load in the data frame from
    the previous recipe (or just continue from where the previous recipe ended).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will convert the featurized data into a collection
    of time series and detect crime using isolation forest:'
  prefs: []
  type: TYPE_NORMAL
- en: 'List all threat actors in preparation for creating labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We then index the dates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to extract the time series information of a given user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to vectorize the time series information of a user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to vectorize a time series of all users'' features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Vectorize the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Train-test split the vectorized data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the vectorized data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the training and testing datasets into threat and non-threat subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Define and instantiate an isolation forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the isolation forest classifier to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the decision scores of the normal subset of the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c2b73e67-f7c5-49a7-a4ff-de067fc826eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Do the same for the threat actors in the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cda00f69-d7e1-49da-adf6-e2eeddd31596.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select a cut-off score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe the results of the cut-off on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Measure the results of the cut-off choice on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having completed the feature-engineering phase in the previous recipe, we went
    ahead and created a model. In *Step 1*, we listed all threat actors in preparation
    for the next steps. In *Step 2*, we created an indexing for the dates, so that
    `0` corresponded to the starting date, `1` to the next day, and so on. In the
    subsequent *Steps 3* and *5*, we defined functions to read in the whole dataset
    time series, filter it down to individual users, and then vectorize the time series
    for each user. We went ahead and vectorized the dataset (*Step 6*) and then train-test
    split it (*Step 7*). We reshaped the data in *Step 8* in order to be able to feed
    it into the isolation forest classifier. We split the data further into benign
    and threat subsets (*Step 9*) to allow us to tune our parameters. We instantiated
    an isolation forest classifier in *Step 10* and then fit it on the data in *Step
    11*. For our contamination parameter, we used a value corresponding to the proportion
    of threats-to-benign actors.
  prefs: []
  type: TYPE_NORMAL
- en: In the next three steps (*Steps 12*-*14*), we examined the decision scores of
    isolation forest on benign and threat actors, and concluded, via inspection, that
    the cut-off value of 0.12 detects a large proportion of the threat actors without
    flagging too many of the benign actors. Finally, assessing our performance in
    *Steps* 15 and *16*, we saw that there were some false positives, but also a significant
    number of insider threats detected. Since the ratio was not too high, the classifier
    can be of great help in informing analysts about plausible threats.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting DDoS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DDoS**, or **Distributed Denial of Service**, is an attack in which traffic
    from different sources floods a victim, resulting in service interruption. There
    are many types of DDoS attacks, falling under three general categories: application-level,
    protocol, and volumetric attacks. Much of the DDoS defense today is manual. Certain
    IP addresses or domains are identified and then blocked. As DDoS bots become more
    sophisticated, such approaches are becoming outdated. Machine learning offers
    a promising automated solution.'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will be working with is a subsampling of the CSE-CIC-IDS2018,
    CICIDS2017, and CIC DoS datasets (2017). It consists of 80% benign and 20% DDoS
    traffic, in order to represent a more realistic ratio of normal-to-DDoS traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing a couple of packages in `pip`,
    namely, `scikit-learn` and `pandas`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: In preparation for this recipe, extract the archive, `ddos_dataset.7z`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will train a random forest classifier to detect
    DDoS traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and specify the data types for the columns you will be reading
    in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the `.csv` file containing the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Sort the data by date:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Drop the date column, as it is no longer needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and testing subsets, consisting of the first 80%
    and last 20% of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare the feature vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Import and instantiate a random forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit random forest to the training data and score it on the testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the dataset is large, even importing all of it is computationally intensive.
    For this reason, we begin *Step 1* by specifying a subset of features from our
    dataset, the ones we consider most promising, as well as recording their data
    type so that we don't have to convert them later. We then proceed to read the
    data into a data frame in *Step 2*. In *Steps 3* and *4*, we sort the data by
    date, since the problem requires being able to predict events in the future, and
    then drop the date column since we will not be employing it further. In the next
    two steps, we perform a train-test split, keeping in mind temporal progression.
    We then instantiate, fit, and test a random forest classifier in *Steps 8* and
    *9*. Depending on the application, the accuracy achieved is a good starting point.
    A promising direction to improve performance is to account for the source and
    destination IPs. The reasoning is that, intuitively, where a connection is coming
    from should have a significant bearing on whether it is part of a DDoS.
  prefs: []
  type: TYPE_NORMAL
- en: Credit card fraud detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Credit card companies must monitor for fraudulent transactions in order to keep
    their customers from being charged for items they have not purchased. Such data
    is unique in being extremely imbalanced, with the particular dataset we will be
    working on in this chapter having fraud constituting 0.172% of the total transactions.
    It contains only numeric input variables, which are the result of a PCA transformation,
    and the features *Time* and *Amount*. The *Time* feature contains the seconds
    elapsed between each transaction and the first transaction in the dataset. The
    *Amount* feature is the amount transaction, a feature that we will use, for instance,
    in cost-sensitive learning. The *Class* feature is the response parameter and,
    in case of fraud, it takes the value `1`, and `0` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what is example-dependent, cost-senstive learning? Consider the costs associated
    with each type of classification. If the program does not identify a fraudulent
    transaction, the money will be wasted and the card holder must be reimbursed for
    the entire amount of the transaction. If a payment is considered fraudulent by
    the program, the transaction will be stopped. In that situation, administrative
    costs arise due to the need to contact the card holder and the card needs to be
    replaced (if the transaction was correctly labeled fraudulent) or reactivated
    (if the transaction was actually legitimate). Let''s assume, for simplicity''s
    sake, that administrative costs are always the same. If the system finds the transaction
    valid, then the transaction will automatically be accepted and there will be no
    charge. This results in the following costs associated with each prediction scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Fraudy = 1 | Benigny = 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Predicted fraudy_pred = 1 | TPcost = administrative | FPcost = administrative
    |'
  prefs: []
  type: TYPE_TB
- en: '| Predicted benigny_pred = 0 | FNcost = transaction amount | TNcost = $0 |'
  prefs: []
  type: TYPE_TB
- en: Unlike most scenarios, our interest is to minimize the total cost, derived from
    the above considerations, rather than accuracy, precision, or recall.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn`, `pandas`, and
    `matplotlib` in `pip`, as well as a new package called `costcla`. The command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: In preparation for this recipe, download the credit card transactions dataset
    from [https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcardfraud.zip/3](https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcardfraud.zip/3)
    (open database license).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will build an example-dependent, cost-sensitive
    classifier using the `costcla` library on credit card transaction data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and read the data pertaining to transactions into a data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Set a cost to `false` positives and `false` negatives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a cost matrix corresponding to the figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Create labels and feature matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a train-test split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the decision tree, fit it to the training data, and then predict on
    the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the cost-sensitive decision tree, fit it to the training data, and then
    predict on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the savings score of the two models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is simply to load the data. In *Step 2*, we set an administrative
    cost based on the expected cost of replacing a credit card. In addition, we estimate
    the business cost of freezing a customer's banking operations until all transactions
    are verified. In practice, you should obtain an accurate figure that is appropriate
    to the credit card company or business use case in question. Using the parameters
    we have defined, we define a cost matrix in *Step 3* that takes into account the
    administrative cost of replacing a credit card, business interruption from freezing
    a customer, and so on. In *Steps 4* and *5*, we train-test split our data. Next,
    we would like to see how the example-dependent, cost-sensitive classifier performs
    as compared with a vanilla classifier. To that end, we instantiate a simple classifier,
    train it, and then use it to predict on the testing set in *Step 6*, and then
    utilize the cost-sensitive random forest model from the `costcla` library in *Step
    7* to do the same. Finally, in *Step 8*, we utilize the `savings_score` function
    from `costcla` to calculate the savings cost of using `y_pred` on `y_true` with
    a cost matrix. The higher the number, the larger the cost savings. Consequently,
    we see that the cost-sensitive random forest model outperformed the vanilla model.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfeit bank note detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Counterfeit money is a currency created without the state or government's legal
    sanction, usually in a deliberate attempt to imitate the currency and to deceive
    its user. In this recipe, you will train a machine learning classifier to distinguish
    between genuine and fake bank notes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn` and `pandas`
    in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'In preparation for this recipe, download the banknote authentication dataset
    from UCI''s machine learning repository: [https://archive.ics.uci.edu/ml/datasets/banknote+authentication](https://archive.ics.uci.edu/ml/datasets/banknote+authentication).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will download a labeled dataset of counterfeit
    and legitimate bank notes and construct a classifier to detect counterfeit currency:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain a labeled dataset of authentic and counterfeit bank notes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read in the bank note dataset using `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a train-test split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Collect the features and labels into arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a random forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Train and test the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The greatest potential for a counterfeiting solution lies in obtaining a large
    dataset of images and using deep learning technology. In a regime where the dataset
    is relatively small, as is the case here, however, feature-engineering is mandatory.
    We begin attacking our problem by loading and then reading in a dataset into pandas
    (*Steps 1* and *2*). In the case of this dataset, a wavelet transform tool was
    used to extract features from the images. Next, in *Steps 3* and *4*, we train-test
    split the data and gather it into arrays. Finally, we fit and test a basic classifier
    on the dataset in *Steps 5* and *6*. The high score (98%) suggests that the features
    extracted for this dataset are indeed able to distinguish between authentic and
    counterfeit notes.
  prefs: []
  type: TYPE_NORMAL
- en: Ad blocking using machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ad blocking is the operation of removing or altering online advertising in a
    web browser or an application. In this recipe, you will utilize machine learning
    to detect ads so that they can be blocked and you can browse hassle-free!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn` and `pandas`
    in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'In preparation for this recipe, download the internet advertisements dataset
    from UCI''s machine learning repository: [https://archive.ics.uci.edu/ml/datasets/internet+advertisements](https://archive.ics.uci.edu/ml/datasets/internet+advertisements).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps show how ad blocking is implemented using machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect a dataset of internet advertisements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the data into a data frame using `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is dirty in the sense of having missing values. Let''s find all the
    rows that have a missing value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case at hand, it makes sense to drop the rows with missing values, as
    seen in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the label into numerical form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Distribute the data into feature arrays and label arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a random forest classifier and train it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Score the classifier on the testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin our recipe for blocking unwanted ads by importing the dataset. The
    data we have used in this recipe has been feature-engineered for us. In *Step
    2*, we import the data into a data frame. Looking at the data, we see that it
    consists of 1,558 numerical features and an ad or non-ad label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/53966614-0e11-4d70-ac4b-cf54579b9517.png)'
  prefs: []
  type: TYPE_IMG
- en: The features encode the geometry of the image, sentences in the URL, the URL
    of the image, alt text, anchor text, and words near the anchor text. Our goal
    is to predict whether an image is an advertisement (ad) or not (non-ad). We proceed
    to clean our data by dropping rows with missing values in *Steps*3 and *4*. Generally,
    it may make sense to use other techniques to impute missing values, such as using
    an average or most common value. Proceeding to *Step 5*, we convert our target
    to numerical form. Then, we train-test split our data in preparation for learning
    in *Steps 6* and *7*. Finally, in *Steps 8* and *9*, we fit and test a basic classifier
    on the data. The results suggest that the features do provide high discrimination
    power.
  prefs: []
  type: TYPE_NORMAL
- en: Recent approaches have utilized deep learning on screen images to tackle ads.
    The approach is very promising, but so far has been unsuccessful due to deep learning's
    adversarial sensitivity. With robustness to adversarial attacks improving in the
    field, deep learning-based ad blockers may become commonplace.
  prefs: []
  type: TYPE_NORMAL
- en: Wireless indoor localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tales of a hacker parked outside a home, and hacking into their network for
    malice, are legendary. Though these tales may exaggerate the ease and motivation
    of this scenario, there are many situations where it is best to only permit users
    inside the home, or, in the case of an enterprise environment, in a designated
    area, to have specified network privileges. In this recipe, you will utilize machine
    learning to localize an entity based on the Wi-Fi signal. The dataset we will
    be working with was collected in an indoor space by observing signal strengths
    of seven Wi-Fi signals visible on a smartphone. One of the four rooms is the decision
    factor.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing `scikit-learn` and `pandas`.
    In your Python environment, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'In preparation for this recipe, download the wireless indoor localization dataset
    from UCI''s machine learning repository: [https://archive.ics.uci.edu/ml/datasets/Wireless+Indoor+Localization.](https://archive.ics.uci.edu/ml/datasets/Wireless+Indoor+Localization)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To localize an entity based on the Wi-Fi signal using machine learning, observe
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect a dataset of Wi-Fi signal strengths from different locations in the
    area of interest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the data into a data frame using `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Train-test split the data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Distribute the features and labels into an array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a random forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the classifier to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict on the testing dataset and print out the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows us the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Step 1* consists of assembling a dataset of Wi-Fi signal strengths from different
    locations in the area of interest. This is something that can be done relatively
    easily, simply by walking through a room with a GPS-enabled phone, and running
    a script to record the strength of the Wi-Fi. In *Step 2*, we read the data into
    a data frame, and then rename the target column to `room` so we know what it refers
    to. Moving on, in *Step*3, we train-test split our data in preparation for learning.
    We divide up the features and labels into arrays (*Step*4). Finally, in *Steps*5
    and 6, we train and test a basic classifier. Observe that the performance of the
    model is very high. This suggests that it is not a difficult task to localize
    a device based on the strength of the Wi-Fi signals that it is able to pick up,
    provided the region has been learned previously.'
  prefs: []
  type: TYPE_NORMAL
