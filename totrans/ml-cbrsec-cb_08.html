<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Secure and Private AI</h1>
                </header>
            
            <article>
                
<p>Machine<span> </span>learning can help us diagnose and<span> </span>fight<span> </span>cancer, decide which school is the best for our children and make the smartest real estate investment. But you can only answer these questions with access to private and personal data, which requires a novel approach to machine learning. This approach is called<span> </span><em>Secure and Private AI</em><span> </span>and, in recent years, has seen great strides, as you will see in the following recipes.</p>
<p class="mce-root">This chapter contains the following recipes:</p>
<ul>
<li class="mce-root">Federated learning</li>
<li class="mce-root">Encrypted computation</li>
<li class="mce-root">Private deep learning prediction</li>
<li class="mce-root">Testing the adversarial robustness of neural networks</li>
<li class="mce-root">Differential privacy using TensorFlow Privacy</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The following are the technical prerequisites for this chapter:</p>
<ul>
<li>TensorFlow Federated</li>
<li>Foolbox</li>
<li>PyTorch</li>
<li>Torchvision</li>
<li>TensorFlow Privacy</li>
</ul>
<p><span>Installation instructions, code, and datasets may be found at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter08">https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter08</a>.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Federated learning</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will train a federated learning model using the TensorFlow federated <span>framework</span>.</p>
<p>To understand why federated learning is valuable, consider the <em>next word prediction</em> model on your mobile phone when you write an SMS message. For privacy reasons, you wouldn't want the data, that is, your text messages, to be sent to a central server to be used for training the next word predictor. But it's still nice to have an accurate next word prediction algorithm. What to do? This is where federated learning comes in, which is a machine learning technique developed to tackle such privacy concerns.<br/></p>
<p>The core idea in federated learning is that a training dataset remains in the hands of its producers, preserving privacy and ownership, while still being used to train a centralized model. This feature is especially attractive in cybersecurity, where, for example, collecting benign and malicious samples from many different sources is crucial to creating a strong model, but difficult on account of privacy concerns (by way of an example, a benign sample can be a personal or confidential document).</p>
<p>In passing, we should mention the fact that federated learning has been gaining more and more traction due to the increasing importance of data privacy (for <span>example</span>, the enactment of GDPR). Large actors, such as Apple and Google, have started investing heavily in this technology.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe involves installing the <kbd>tensorflow_federated</kbd>, <kbd>tensorflow_datasets</kbd>, and <kbd>tensorflow</kbd> packages in <kbd>pip</kbd>. <span><span>The command is as follows</span></span>:</p>
<pre><strong>pip install tensorflow_federated==0.2.0 tensorflow-datasets tensorflow==1.13.1</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>We will install specific versions of these packages to prevent any breaks in the code.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>In the following steps, you will create two virtual dataset environments—one belonging to Alice, and one belonging to Bob<span>—</span>and use federated averaging to preserve data confidentiality.</p>
<ol>
<li>Import TensorFlow and enable eager execution:</li>
</ol>
<pre style="padding-left: 60px">import tensorflow as tf<br/><br/>tf.compat.v1.enable_v2_behavior()</pre>
<ol start="2">
<li>Prepare a dataset by importing Fashion MNIST and splitting it into two separate environments, Alice and Bob:</li>
</ol>
<pre style="padding-left: 60px">import tensorflow_datasets as tfds<br/><br/>first_50_percent = tfds.Split.TRAIN.subsplit(tfds.percent[:50])<br/>last_50_percent = tfds.Split.TRAIN.subsplit(tfds.percent[-50:])<br/><br/>alice_dataset = tfds.load("fashion_mnist", split=first_50_percent)<br/>bob_dataset = tfds.load("fashion_mnist", split=last_50_percent)</pre>
<ol start="3">
<li>Now, define a <kbd>helper</kbd> function to cast the data type from integer to float:</li>
</ol>
<pre style="padding-left: 60px">def cast(element):<br/>    """Casts an image's pixels into float32."""<br/>    out = {}<br/>    out["image"] = tf.image.convert_image_dtype(element["image"], dtype=tf.float32)<br/>    out["label"] = element["label"]<br/>    return out</pre>
<ol start="4">
<li>Then, define a <kbd>helper</kbd> function to flatten the data to be fed into a neural network:</li>
</ol>
<pre style="padding-left: 60px">def flatten(element):<br/>    """Flattens an image in preparation for the neural network."""<br/>    return collections.OrderedDict(<br/>        [<br/>            ("x", tf.reshape(element["image"], [-1])),<br/>            ("y", tf.reshape(element["label"], [1])),<br/>        ]<br/>    )</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Now, define a <kbd>helper</kbd> function to pre-process the data:</li>
</ol>
<pre style="padding-left: 60px">import collections<br/><br/>BATCH_SIZE = 32<br/><br/>def preprocess(dataset):<br/>    """Preprocesses images to be fed into neural network."""<br/>    return dataset.map(cast).map(flatten).batch(BATCH_SIZE)</pre>
<p> </p>
<ol start="6">
<li>Pre-process the data:</li>
</ol>
<pre style="padding-left: 60px">preprocessed_alice_dataset = preprocess(alice_dataset)<br/>preprocessed_bob_dataset = preprocess(bob_dataset)<br/>federated_data = [preprocessed_alice_dataset, preprocessed_bob_dataset]</pre>
<ol start="7">
<li>Now, define a <kbd>loss</kbd> function for our neural network:</li>
</ol>
<pre style="padding-left: 60px">def custom_loss_function(y_true, y_pred):<br/>    """Custom loss function."""<br/>    return tf.reduce_mean(<br/>        tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)<br/>    )</pre>
<ol start="8">
<li>Define a function to instantiate a simple Keras neural network:</li>
</ol>
<pre style="padding-left: 60px">from tensorflow.python.keras.optimizer_v2 import gradient_descent<br/><br/>LEARNING_RATE = 0.02<br/>def create_compiled_keras_model():<br/>    """Compiles the keras model."""<br/>    model = tf.keras.models.Sequential(<br/>        [<br/>            tf.keras.layers.Dense(<br/>                10,<br/>                activation=tf.nn.softmax,<br/>                kernel_initializer="zeros",<br/>                input_shape=(784,),<br/>            )<br/>        ]<br/>    )<br/>    model.compile(<br/>        loss=custom_loss_function,<br/>        optimizer=gradient_descent.SGD(learning_rate=LEARNING_RATE),<br/>        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],<br/>    )<br/>    return model</pre>
<ol start="9">
<li>Then, create a dummy batch of samples and define a function to return a federated learning model from the Keras model:</li>
</ol>
<pre style="padding-left: 60px">batch_of_samples = tf.contrib.framework.nest.map_structure(<br/>    lambda x: x.numpy(), iter(preprocessed_alice_dataset).next()<br/>)<br/><br/><br/>def model_instance():<br/>    """Instantiates the keras model."""<br/>    keras_model = create_compiled_keras_model()<br/>    return tff.learning.from_compiled_keras_model(keras_model, batch_of_samples)</pre>
<ol start="10">
<li>Declare an iterative process of federated averaging, and run one stage of the computation:</li>
</ol>
<pre style="padding-left: 60px">from tensorflow_federated import python as tff<br/><br/>federated_learning_iterative_process = tff.learning.build_federated_averaging_process(<br/>    model_instance<br/>)<br/>state = federated_learning_iterative_process.initialize()<br/>state, performance = federated_learning_iterative_process.next(state, federated_data)</pre>
<ol start="11">
<li>Then, display the metrics of the computation by running the following command:</li>
</ol>
<pre style="padding-left: 60px">performance</pre>
<p>The output is as follows:</p>
<pre>AnonymousTuple([(sparse_categorical_accuracy, 0.74365), (loss, 0.82071316)])</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We begin by importing TensorFlow and enabling eager execution (<em>Step 1</em>). Ordinarily, in TensorFlow, operations are not performed immediately. Rather, a computation graph is built, and, at the very end, all operations are run together. In eager execution, computations are executed as soon as possible. Next, in <em>Step 2</em>, we import the Fashion MNIST dataset. This dataset has become a <em>de facto</em> replacement for MNIST, offering several improvements over it (such as added challenges). We then subdivide the dataset 50:50 between Alice and Bob. We then define a function to cast the pixel values of Fashion MNIST from integers to floats to be used in the training of our neural network (<em>Step 3</em>) and another function to flatten the images into a single vector (<em>Step 4</em>). This enables us to feed the data into a fully connected neural network. In <em>Steps 5</em> and <em>6</em>, we employ the previously defined convenience functions to pre-process Alice and Bob's datasets.</p>
<p>Next, we define a loss function that makes sense for our 10-class classification task (<em>Step 7</em>), and then define our Keras neural network in preparation for training (<em>Step 8</em>). In <em>Step 9</em>, we create a dummy batch of samples and define a function to return a federated learning model from the Keras model. The dummy batch of samples specifies the shape of input for the model to expect. In <em>Step 10</em>, we run one stage of the federated averaging process. Details regarding the algorithm can be found in the paper entitled <em>Communication-Efficient Learning of Deep Networks from Decentralized Data</em>.</p>
<p>At a basic level, the algorithm combines local <strong>stochastic gradient descent</strong> (<strong>SGD</strong>) on the data of each client, and then uses a server that performs model averaging. The result is conserved confidentiality for the clients (in our case, Alice and Bob). Finally, in <em>Step 11</em>, we observe our performance, seeing that the algorithm indeed does train and improve accuracy, as intended.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encrypted computation</h1>
                </header>
            
            <article>
                
<p>In this recipe, we're going to walk through the basics of encrypted computation. In particular, we're going to focus on one popular approach, called Secure Multi-Party Computation. You'll learn how to build a simple encrypted calculator that can perform addition on encrypted numbers. The ideas in this recipe will come in handy in the <em>Private deep learning prediction</em> <span>recipe.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The following recipe has no installation requirements other than Python.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<ol>
<li>Import the random library and select a large prime number, <kbd>P</kbd>:</li>
</ol>
<pre style="padding-left: 60px">import random<br/><br/>P = 67280421310721</pre>
<ol start="2">
<li>Define an encryption function for three parties:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def encrypt(x):<br/>    """Encrypts an integer between 3 partires."""<br/>    share_a = random.randint(0, P)<br/>    share_b = random.randint(0, P)<br/>    share_c = (x - share_a - share_b) % P<br/>    return (share_a, share_b, share_c)</pre>
<ol start="3">
<li>Encrypt a numerical variable:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">x = 17<br/>share_a, share_b, share_c = encrypt(x)<br/>print(share_a, share_b, share_c)<br/><br/>16821756678516 13110264723730 37348399908492</pre>
<ol start="4">
<li>Define a function to decrypt, given the three shares:</li>
</ol>
<pre style="padding-left: 60px">def decrypt(share_a, share_b, share_c):<br/>    """Decrypts the integer from 3 shares."""<br/>    return (share_a + share_b + share_c) % P</pre>
<ol start="5">
<li>Decrypt the encrypted variable <kbd>x</kbd>:</li>
</ol>
<pre style="padding-left: 60px">decrypt(share_a, share_b, share_c)</pre>
<p style="padding-left: 60px">The output is as follows:</p>
<pre style="padding-left: 60px">17</pre>
<ol start="6">
<li>Define a function to add two encrypted numbers:</li>
</ol>
<pre style="padding-left: 60px">def add(x, y):<br/>    """Addition of encrypted integers."""<br/>    z = list()<br/>    z.append((x[0] + y[0]) % P)<br/>    z.append((x[1] + y[1]) % P)<br/>    z.append((x[2] + y[2]) % P)<br/>    return z</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="7">
<li>Add two encrypted variables and decrypt their sum:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">x = encrypt(5)<br/>y = encrypt(9)<br/>decrypt(*add(x, y))<br/><br/>14</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>We begin <em>Step 1</em> by importing the random library in order to generate random integers in <em>Step 2</em>. We also define a large prime number, P, as we will be wanting a random distribution modulo, P. In <em>Step 2</em>, we define how a function encrypts an integer by splitting it between three parties. The value of x here is randomly additively split between the three parties. All operations take place in the field of integer modulo P. Next, in <em>Step 3</em>, we demonstrate the result of encrypting an integer using our approach. Proceeding to <em>Steps 4</em> and <em>5</em>, we define a function to reverse encryption, that is decrypt, and then show that the operation is reversible. In <em>Step 6</em>, we define a function to add two encrypted numbers(!). Note that encrypted addition is simply addition of the individual components, modulo P. In the <em>Encrypted deep learning prediction</em> <span>recipe, </span>the <kbd>.share(client, server,...)</kbd> <span>command </span>from PySyft is used. This command is basically the same encryption procedure we have used in this recipe, so keep in mind that these encryption schemes use the techniques we are discussing here. Finally, in <em>Step 7</em>, we demonstrate that we can perform computations on encrypted entities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Private deep learning prediction</h1>
                </header>
            
            <article>
                
<p>In many situations, company A might have a trained model that it wishes to offer as a service. At the same time, company A may be reluctant to share this model, so as to avoid having its intellectual property stolen. The simple solution to this problem is to have customers send their data to company A, and then receive from it predictions. However, this becomes a problem when the customer wishes to preserve the privacy of their data. To resolve this tricky situation, the company, as well as its customers, can utilize encrypted computation.</p>
<p>In this recipe, you will learn how to share an encrypted pre-trained deep learning model with a client and allow the client to predict using the encrypted model on their own private data.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe involves installing PyTorch, Torchvision, and PySyft in <kbd>pip</kbd>. The command is as follows:</p>
<pre><strong>pip install torch torchvision syft</strong></pre>
<p>In addition, a pre-trained model named <kbd>server_trained_model.pt</kbd> has been included to be used in this recipe.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>The following steps utilize PySyft to simulate a client-server interaction in which the server has a pre-trained deep learning model to be kept as a black box, and the client wishes to use the model to predict on data that is kept private.</p>
<ol start="1">
<li> Import <kbd>torch</kbd> and access its datasets:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>from torchvision import datasets, transforms</pre>
<ol start="2">
<li>Import PySyft and hook it onto <kbd>torch</kbd>:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import syft as sy<br/><br/>hook = sy.TorchHook(torch)<br/>client = sy.VirtualWorker(hook, id="client")<br/>server = sy.VirtualWorker(hook, id="server")<br/>crypto_provider = sy.VirtualWorker(hook, id="crypto_provider")</pre>
<ol start="3">
<li>Define a simple neural network:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">class Net(nn.Module):<br/>    def __init__(self):<br/>        super(Net, self).__init__()<br/>        self.fc1 = nn.Linear(784, 500)<br/>        self.fc2 = nn.Linear(500, 10)<br/><br/>    def forward(self, x):<br/>        x = x.view(-1, 784)<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.fc2(x)<br/>        return x</pre>
<ol start="4">
<li>Instantiate the model and load its pre-trained weights, which are trained on MNIST:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">model = Net()<br/>model.load_state_dict(torch.load("server_trained_model.pt"))<br/>model.eval()</pre>
<ol start="5">
<li> Encrypt the network between <kbd>client</kbd> and <kbd>server</kbd>:</li>
</ol>
<pre style="padding-left: 60px">model.fix_precision().share(client, server, crypto_provider=crypto_provider)</pre>
<ol start="6">
<li>Define a loader for MNIST data:</li>
</ol>
<pre style="padding-left: 60px">test_loader = torch.utils.data.DataLoader(<br/>    datasets.MNIST(<br/>        "data",<br/>        train=False,<br/>        download=True,<br/>        transform=transforms.Compose(<br/>            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]<br/>        ),<br/>    ),<br/>    batch_size=64,<br/>    shuffle=True,<br/>)</pre>
<ol start="7">
<li>Define a private loader utilizing the loader for MNIST data:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">private_test_loader = []<br/>for data, target in test_loader:<br/>    private_test_loader.append(<br/>        (<br/>            data.fix_precision().share(client, server, crypto_provider=crypto_provider),<br/>            target.fix_precision().share(<br/>                client, server, crypto_provider=crypto_provider<br/>            ),<br/>        )<br/>    )</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="8">
<li>Define a function to evaluate the private test set:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">def test(model, test_loader):<br/>    """Test the model."""<br/>    model.eval()<br/>    n_correct_priv = 0<br/>    n_total = 0</pre>
<ol start="9">
<li>Iterate over the private data, predict using the model, decrypt the results, and then print them out:</li>
</ol>
<pre class="mce-root">    with torch.no_grad():<br/>        for data, target in test_loader:<br/>            output = model(data)<br/>            pred = output.argmax(dim=1)<br/>            n_correct_priv += pred.eq(target.view_as(pred)).sum()<br/>            n_total += 64<br/>            n_correct = <br/>            n_correct_priv.copy().get().float_precision().long().item()<br/>            print(<br/>                "Test set: Accuracy: {}/{} ({:.0f}%)".format(<br/>                    n_correct, n_total, 100.0 * n_correct / n_total<br/>                )<br/>            )</pre>
<ol start="10">
<li>Run the testing procedure:</li>
</ol>
<pre style="padding-left: 60px">test(model, private_test_loader)</pre>
<p>The result is as follows:</p>
<pre>Test set: Accuracy: 63/64 (98%)<br/>Test set: Accuracy: 123/128 (96%)<br/>Test set: Accuracy: 185/192 (96%)<br/>Test set: Accuracy: 248/256 (97%)<br/>Test set: Accuracy: 310/320 (97%)<br/>Test set: Accuracy: 373/384 (97%)<br/>Test set: Accuracy: 433/448 (97%)<br/>&lt;snip&gt;<br/>Test set: Accuracy: 9668/9920 (97%)<br/>Test set: Accuracy: 9727/9984 (97%)<br/>Test set: Accuracy: 9742/10048 (97%)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>We begin by importing <kbd>torch</kbd> and its datasets, as well as some associated libraries (<em>Step 1</em>). We then import <kbd>pysyft</kbd> and hook it into <kbd>torch</kbd> (<em>Step 2</em>). We also create virtual environments for the client and server to simulate a real separation of data. In this step, the <kbd>crypto_provider</kbd> serves as a trusted third party to be used for encryption and decryption purposes. In <em>Step 3</em>, we define a simple neural network and, in <em>Step 4</em>, we load-in its pretrained weights. Note that, in <em>Step 5</em>, and, more generally, whenever the <kbd>.share(...)</kbd> <span>command </span>is used, you should think of the shared object as being encrypted, and that it is only possible to decrypt it with the assistance of all parties involved. In particular, in <em>Step 9</em>, the test function performs encrypted evaluation; the weights of the model, the data inputs, the prediction, and the target used for scoring are all encrypted. However, for the purpose of verifying that the model is working properly, we decrypt and display its accuracy. In <em>Step 5</em>, we encrypt the network so that only when the server and client are cooperating can they decrypt the network.</p>
<p>In the next two steps, we define regular and private loaders for MNIST data. The regular loader simply loads MNIST data, while the private loader encrypts the outputs of the regular loader. In <em>Steps 8</em> and <em>9</em>, we define a <kbd>helper</kbd> function to evaluate the private test set. In this function, we iterate over the private data, predict using the model, decrypt the results, and then print them out. Finally, we apply the function defined in <em>Steps 8</em> and <em>9</em> to establish that the model is performing well, while preserving privacy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the adversarial robustness of neural networks</h1>
                </header>
            
            <article>
                
<p>The study of adversarial attacks on neural networks has revealed a surprising sensitivity to adversarial perturbations. Even the most accurate of neural networks, when left undefended, has been shown to be vulnerable to single pixel attacks and the peppering of invisible-to-the-human-eye noise. Fortunately, recent advances in the field have offered solutions on how to harden neural networks to adversarial attacks of all sorts. <span>One such solution is a neural network</span> design <span>called</span> <strong>Analysis by Synthesis</strong> <span>(</span><strong>ABS</strong><span>). The main idea behind the model is that it is a Bayesian model. Rather than directly predicting the label given the input, the model also learns class-conditional, sample distributions using</span> <strong>variational autoencoders</strong> <span>(</span><strong>VAEs</strong><span>). More</span> information <span>can be found in</span> <a href="https://arxiv.org/abs/1805.09190" target="_blank">https://arxiv.org/abs/1805.09190</a><span>.</span></p>
<p>In this recipe, you will load a pre-trained ABS network for MNIST and learn how to test a neural network for adversarial robustness.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div>
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>The following recipe has been tested in Python 3.6. Preparation for this recipe involves installing the Pytorch, Torchvision, SciPy, Foolbox, and Matplotlib packages in <kbd>pip</kbd>. The command is as follows:</p>
<pre><strong>pip install torch torchvision scipy foolbox==1.8 matplotlib</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the following steps, we will load a pre-trained ABS model and a traditional CNN model for MNIST. We will attack both models using Foolbox to see how well they can defend against adversarial attacks:</p>
<ol>
<li>Begin by importing a pre-trained ABS model:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from abs_models import models<br/>from abs_models import utils<br/><br/>ABS_model = models.get_VAE(n_iter=50)</pre>
<ol start="2">
<li>Define a <kbd>convenience</kbd> function to predict a batch of MNIST images using a model:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/><br/>def predict_on_batch(model, batch, batch_size):<br/>    """Predicts the digits of an MNIST batch."""<br/>    preds = []<br/>    labels = []<br/>    for i in range(batch_size):<br/>        point, label = utils.get_batch()<br/>        labels.append(label[0])<br/>        tensor_point = utils.n2t(point)<br/>        logits = model(tensor_point)[0]<br/>        logits = [x for x in logits]<br/>        pred = np.argmax(logits)<br/>        preds.append(int(pred))<br/>    return preds, labels</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li> Predict on a batch:</li>
</ol>
<pre style="padding-left: 60px">batch = utils.get_batch()<br/>preds, labels = predict_on_batch(ABS_model, batch, 5)<br/>print(preds)<br/>print(labels)</pre>
<p style="padding-left: 60px">The result is as follows:</p>
<pre style="padding-left: 60px">[4, 4, 9, 1, 8]<br/>[4, 4, 9, 1, 8]</pre>
<ol start="4">
<li>Wrap the model using Foolbox to enable adversarial testing:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">import foolbox<br/><br/>if ABS_model.code_base == "tensorflow":<br/>    fmodel = foolbox.models.TensorFlowModel(<br/>        ABS_model.x_input, ABS_model.pre_softmax, (0.0, 1.0), channel_axis=3<br/>    )<br/>elif ABS_model.code_base == "pytorch":<br/>    ABS_model.eval()<br/>    fmodel = foolbox.models.PyTorchModel(<br/>        ABS_model, bounds=(0.0, 1.0), num_classes=10, device=utils.dev()<br/>    )</pre>
<ol start="5">
<li>Import the library of attacks from Foolbox and select an MNIST image:</li>
</ol>
<pre style="padding-left: 60px">from foolbox import attacks<br/><br/>images, labels = utils.get_batch(bs=1)</pre>
<ol start="6">
<li>Select the attack type, in this case, a boundary attack:</li>
</ol>
<pre style="padding-left: 60px">attack = attacks.DeepFoolL2Attack(fmodel)<br/>metric = foolbox.distances.MSE<br/>criterion = foolbox.criteria.Misclassification()</pre>
<ol start="7">
<li>Display the original image and its label using Matplotlib:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">from matplotlib import pyplot as plt<br/>%matplotlib inline<br/><br/>plt.imshow(images[0, 0], cmap="gray")<br/>plt.title("original image")<br/>plt.axis("off")<br/>plt.show()</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The image produced is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0cb4d4d5-9a8f-43b7-ad67-f60877d32b84.png" style="width:18.17em;height:18.83em;"/></p>
<ol start="8">
<li>Search for an adversarial instance using Foolbox:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">gradient_estimator = foolbox.gradient_estimators.CoordinateWiseGradientEstimator(0.1)<br/>fmodel = foolbox.models.ModelWithEstimatedGradients(fmodel, gradient_estimator)<br/><br/>adversary = foolbox.adversarial.Adversarial(<br/>    fmodel, criterion, images[0], labels[0], distance=metric<br/>)<br/>attack(adversary)</pre>
<ol start="9">
<li> Show the discovered adversarial example:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">plt.imshow(a.image[0], cmap="gray")<br/>plt.title("adversarial image")<br/>plt.axis("off")<br/>plt.show()<br/>print("Model prediction:", np.argmax(fmodel.predictions(adversary.image)))</pre>
<p style="padding-left: 30px">The adversarial image produced is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f3113288-d9d3-4d8b-81d2-0c0179c5685f.png" style="width:17.50em;height:18.08em;"/></p>
<ol start="10">
<li>Instantiate a traditional CNN model trained on MNIST:</li>
</ol>
<pre style="padding-left: 60px">from abs_models import models<br/><br/>traditional_model = models.get_CNN()</pre>
<p style="padding-left: 60px">The model architecture is as follows:</p>
<pre style="padding-left: 60px">CNN(
  (net): NN(
    (conv_0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))
    (bn_0): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (nl_0): ELU(alpha=1.0)
    (conv_1): Conv2d(20, 70, kernel_size=(4, 4), stride=(2, 2))
    (bn_1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (nl_1): ELU(alpha=1.0)
    (conv_2): Conv2d(70, 256, kernel_size=(3, 3), stride=(2, 2))
    (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (nl_2): ELU(alpha=1.0)
    (conv_3): Conv2d(256, 10, kernel_size=(5, 5), stride=(1, 1))
  )
  (model): NN(
    (conv_0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))
    (bn_0): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (nl_0): ELU(alpha=1.0)
    (conv_1): Conv2d(20, 70, kernel_size=(4, 4), stride=(2, 2))
    (bn_1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (nl_1): ELU(alpha=1.0)
    (conv_2): Conv2d(70, 256, kernel_size=(3, 3), stride=(2, 2))
    (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (nl_2): ELU(alpha=1.0)
    (conv_3): Conv2d(256, 10, kernel_size=(5, 5), stride=(1, 1))
  )
)</pre>
<ol start="11">
<li>Perform a sanity check to make sure that the model is performing as expected:</li>
</ol>
<pre style="padding-left: 60px">preds, labels = predict_on_batch(traditional_model, batch, 5)<br/>print(preds)<br/>print(labels)</pre>
<p style="padding-left: 60px">The printout is as follows:</p>
<pre style="padding-left: 60px">[7, 9, 5, 3, 3]<br/>[7, 9, 5, 3, 3]</pre>
<ol start="12">
<li>Wrap the traditional model using Foolbox:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">if traditional_model.code_base == "tensorflow":<br/>    fmodel_traditional = foolbox.models.TensorFlowModel(<br/>        traditional_model.x_input,<br/>        traditional_model.pre_softmax,<br/>        (0.0, 1.0),<br/>        channel_axis=3,<br/>    )<br/>elif traditional_model.code_base == "pytorch":<br/>    traditional_model.eval()<br/>    fmodel_traditional = foolbox.models.PyTorchModel(<br/>        traditional_model, bounds=(0.0, 1.0), num_classes=10, device=u.dev()<br/>    )</pre>
<ol start="13">
<li> Attack the traditional CNN model:</li>
</ol>
<pre style="padding-left: 60px">fmodel_traditional = foolbox.models.ModelWithEstimatedGradients(fmodel_traditional, GE)<br/><br/>adversarial_traditional = foolbox.adversarial.Adversarial(<br/>    fmodel_traditional, criterion, images[0], labels[0], distance=metric<br/>)<br/>attack(adversarial_traditional)</pre>
<p class="mce-root"/>
<ol start="14">
<li>Display the adversarial example discovered:</li>
</ol>
<pre style="padding-left: 60px">plt.imshow(adversarial_traditional.image[0], cmap="gray")<br/>plt.title("adversarial image")<br/>plt.axis("off")<br/>plt.show()<br/>print(<br/>    "Model prediction:",<br/>    np.argmax(fmodel_traditional.predictions(adversarial_traditional.image)),<br/>)</pre>
<p style="padding-left: 60px">The adversarial image produced is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f9fbf766-8c65-4f3d-98fd-5e2f36e9fa55.png" style="width:16.33em;height:16.92em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We begin by importing a pre-trained ABS model (<em>Step 1</em>). In <em>Steps 2</em> and <em>3</em>, we defined a <kbd>convenience</kbd> function to predict a batch of MNIST images and to verify that the model is working properly. Next, we wrapped the model using Foolbox in preparation for testing its adversarial robustness (<em>Step 4</em>). Note that Foolbox facilitates the attacking of either TensorFlow or PyTorch models using the same API once wrapped. Nice! In <em>Step 5</em>, we select an MNIST image to use as the medium for our attack. To clarify, this image gets tweaked and mutated until the result fools the model. In <em>Step 6</em>, we select the attack type we want to implement. We select a boundary attack, which is a decision-based attack that starts from a large adversarial perturbation and then gradually reduces the perturbation while remaining adversarial. The attack requires little hyperparameter tuning, hence, no substitute models and no gradient computations. For more information about decision-based attacks, refer to <a href="https://arxiv.org/abs/1712.04248" target="_blank">https://arxiv.org/abs/1712.04248</a>.</p>
<p>In addition, note that the metric used here is <strong>mean squared error</strong> (<strong>MSE</strong>), which determines how the adversarial example is assessed as close to, or far from, the original image. The criterion used is misclassification, meaning that the search terminates once the target model misclassifies the image. Alternative criteria may include a confidence level or a specific type of misclassification. In <em>Steps 7-9</em>, we display the original image, as well as the adversarial example generated from it. In the next two steps, we instantiate a standard CNN and verify that it is working properly. In <em>Steps 12-14</em>, we repeat the attack from the previous steps on the standard CNN. Looking at the result, we see that the experiment is a strong visual indicator that the ABS model is more robust to adversarial perturbations than a vanilla CNN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differential privacy using TensorFlow Privacy</h1>
                </header>
            
            <article>
                
<p>TensorFlow Privacy (<a href="https://github.com/tensorflow/privacy" target="_blank">https://github.com/tensorflow/privacy</a>) is a relatively new addition to the TensorFlow family. This Python library includes implementations of TensorFlow optimizers for training machine learning models with <em>differential privacy</em>. A model that has been trained to be differentially private does not non-trivially change as a result of the removal of any single training instance from its dataset. (Approximate) differential privacy is quantified using <em>epsilon</em> and <em>delta</em>, which give a measure of how sensitive the model is to a change in a single training example. Using the Privacy library is as simple as wrapping the familiar optimizers (for example, RMSprop, Adam, and SGD) to convert them to a differentially private version. This library also provides convenient tools for measuring the privacy guarantees, epsilon, and delta.</p>
<p class="mce-root"/>
<p>In this recipe, we show you how to implement and train a differentially private deep neural network for MNIST using Keras and TensorFlow Privacy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<div class="a-b-r">
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<p>Preparation for this recipe involves installing Keras and TensorFlow. The command is as follows:</p>
</div>
</div>
</div>
<div class="a-b-r-Tl a-b-va-Zf">
<div class="a-b-r-Ec a-b-tb-Ce">
<div class="a-b-r-x">
<pre><strong>pip install keras tensorflow</strong></pre>
<p>Installation instructions for TensorFlow Privacy can be found at <a href="https://github.com/tensorflow/privacy">https://github.com/tensorflow/privacy</a>.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Begin by defining a few convenience functions for pre-processing the MNIST dataset:</li>
</ol>
<pre style="padding-left: 60px">import tensorflow as tf<br/><br/><br/>def preprocess_observations(data):<br/>    """Preprocesses MNIST images."""<br/>    data = np.array(data, dtype=np.float32) / 255<br/>    data = data.reshape(data.shape[0], 28, 28, 1)<br/>    return data<br/><br/><br/>def preprocess_labels(labels):<br/>    """Preprocess MNIST labels."""<br/>    labels = np.array(labels, dtype=np.int32)<br/>    labels = tf.keras.utils.to_categorical(labels, num_classes=10)</pre>
<ol start="2">
<li>Write a convenience function to load MNIST:</li>
</ol>
<pre style="padding-left: 60px">def load_mnist():<br/>    """Loads the MNIST dataset."""<br/>    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()<br/>    X_train = preprocess_observations(X_train)<br/>    X_test = preprocess_observations(X_test)<br/>    y_train = preprocess_labels(y_train)<br/>    y_test = preprocess_labels(y_test)<br/>    return X_train, y_train, X_test, y_test</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="3">
<li> Load the MNIST dataset:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/><br/>X_train, y_train, X_test, y_test = load_mnist()</pre>
<p style="padding-left: 60px">The training set is 60 k in size, and the testing set 10 k.</p>
<p style="padding-left: 60px">4. Import a differentially private optimizer and define a few parameters that control the learning rate and extent of differential privacy:</p>
<pre style="padding-left: 60px">from privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer<br/><br/>optimizer = DPGradientDescentGaussianOptimizer(<br/>    l2_norm_clip=1.0, noise_multiplier=1.1, num_microbatches=250, learning_rate=0.15<br/>)<br/>loss = tf.keras.losses.CategoricalCrossentropy(<br/>    from_logits=True, reduction=tf.losses.Reduction.NONE<br/>)</pre>
<ol start="5">
<li>In order to measure privacy, define a function to compute epsilon:</li>
</ol>
<pre style="padding-left: 60px">from privacy.analysis.rdp_accountant import compute_rdp<br/>from privacy.analysis.rdp_accountant import get_privacy_spent<br/><br/><br/>def compute_epsilon(steps):<br/>    """Compute the privacy epsilon."""<br/>    orders = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))<br/>    sampling_probability = 250 / 60000<br/>    rdp = compute_rdp(<br/>        q=sampling_probability, noise_multiplier=1.1, steps=steps, orders=orders<br/>    )<br/>    return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]</pre>
<ol start="6">
<li>Define a standard Keras CNN for MNIST:</li>
</ol>
<pre style="padding-left: 60px">NN_model = tf.keras.Sequential(<br/>    [<br/>        tf.keras.layers.Conv2D(<br/>            16, 8, strides=2, padding="same", activation="relu", input_shape=(28, 28, 1)<br/>        ),<br/>        tf.keras.layers.MaxPool2D(2, 1),<br/>        tf.keras.layers.Conv2D(32, 4, strides=2, padding="valid", activation="relu"),<br/>        tf.keras.layers.MaxPool2D(2, 1),<br/>        tf.keras.layers.Flatten(),<br/>        tf.keras.layers.Dense(32, activation="relu"),<br/>        tf.keras.layers.Dense(10),<br/>    ]<br/>)</pre>
<ol start="7">
<li> Compiling <kbd>model</kbd>:</li>
</ol>
<pre style="padding-left: 60px">NN_model.compile(optimizer=optimizer, loss=loss, metrics=["accuracy"])</pre>
<ol start="8">
<li>Fit and test <kbd>model</kbd>:</li>
</ol>
<pre style="padding-left: 60px">NN_model.fit(<br/>    X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=250<br/>)</pre>
<ol start="9">
<li>Compute the value of <kbd>epsilon</kbd>, the measure of privacy:</li>
</ol>
<pre style="padding-left: 60px">eps = compute_epsilon(1 * 60000 // 250)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We begin <em>Steps 1-3</em> by preparing and loading the MNIST dataset. Next, in <em>Step 4</em>, we import <kbd>DPGradientDescentGaussianOptimizer</kbd>, an optimizer that allows the model to become differentially private. A number of parameters are used at this stage, and these stand to be clarified. The <kbd>l2_norm_clip</kbd> <span>parameter </span>refers to the maximum norm of each gradient computed on an individual training datapoint from a minibatch. This parameter bounds the sensitivity of the optimizer to individual training points, thereby moving the model toward differential privacy. The <kbd>noise_multiplier</kbd> <span>parameter </span>controls the amount of random noise added to gradients. Generally, the more noise, the greater the privacy. Having finished this step, in <em>Step 5</em>, we define a function that computes the epsilon of the epsilon-delta definition of differential privacy. We instantiate a standard Keras neural network (<em>Step 6</em>), compile it (<em>Step 7</em>), and then train it on MNIST using the differentially private optimizer (<em>Step 8</em>). Finally, in <em>Step 9</em>, we compute the value of epsilon, which measures the extent to which the model is differentially private. Typical values for this recipe are an epsilon value of around 1.</p>


            </article>

            
        </section>
    </body></html>