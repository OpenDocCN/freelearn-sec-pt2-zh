["```\npip install tensorflow_federated==0.2.0 tensorflow-datasets tensorflow==1.13.1\n```", "```\nimport tensorflow as tf\n\ntf.compat.v1.enable_v2_behavior()\n```", "```\nimport tensorflow_datasets as tfds\n\nfirst_50_percent = tfds.Split.TRAIN.subsplit(tfds.percent[:50])\nlast_50_percent = tfds.Split.TRAIN.subsplit(tfds.percent[-50:])\n\nalice_dataset = tfds.load(\"fashion_mnist\", split=first_50_percent)\nbob_dataset = tfds.load(\"fashion_mnist\", split=last_50_percent)\n```", "```\ndef cast(element):\n    \"\"\"Casts an image's pixels into float32.\"\"\"\n    out = {}\n    out[\"image\"] = tf.image.convert_image_dtype(element[\"image\"], dtype=tf.float32)\n    out[\"label\"] = element[\"label\"]\n    return out\n```", "```\ndef flatten(element):\n    \"\"\"Flattens an image in preparation for the neural network.\"\"\"\n    return collections.OrderedDict(\n        [\n            (\"x\", tf.reshape(element[\"image\"], [-1])),\n            (\"y\", tf.reshape(element[\"label\"], [1])),\n        ]\n    )\n```", "```\nimport collections\n\nBATCH_SIZE = 32\n\ndef preprocess(dataset):\n    \"\"\"Preprocesses images to be fed into neural network.\"\"\"\n    return dataset.map(cast).map(flatten).batch(BATCH_SIZE)\n```", "```\npreprocessed_alice_dataset = preprocess(alice_dataset)\npreprocessed_bob_dataset = preprocess(bob_dataset)\nfederated_data = [preprocessed_alice_dataset, preprocessed_bob_dataset]\n```", "```\ndef custom_loss_function(y_true, y_pred):\n    \"\"\"Custom loss function.\"\"\"\n    return tf.reduce_mean(\n        tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n    )\n```", "```\nfrom tensorflow.python.keras.optimizer_v2 import gradient_descent\n\nLEARNING_RATE = 0.02\ndef create_compiled_keras_model():\n    \"\"\"Compiles the keras model.\"\"\"\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Dense(\n                10,\n                activation=tf.nn.softmax,\n                kernel_initializer=\"zeros\",\n                input_shape=(784,),\n            )\n        ]\n    )\n    model.compile(\n        loss=custom_loss_function,\n        optimizer=gradient_descent.SGD(learning_rate=LEARNING_RATE),\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n    )\n    return model\n```", "```\nbatch_of_samples = tf.contrib.framework.nest.map_structure(\n    lambda x: x.numpy(), iter(preprocessed_alice_dataset).next()\n)\n\ndef model_instance():\n    \"\"\"Instantiates the keras model.\"\"\"\n    keras_model = create_compiled_keras_model()\n    return tff.learning.from_compiled_keras_model(keras_model, batch_of_samples)\n```", "```\nfrom tensorflow_federated import python as tff\n\nfederated_learning_iterative_process = tff.learning.build_federated_averaging_process(\n    model_instance\n)\nstate = federated_learning_iterative_process.initialize()\nstate, performance = federated_learning_iterative_process.next(state, federated_data)\n```", "```\nperformance\n```", "```\nAnonymousTuple([(sparse_categorical_accuracy, 0.74365), (loss, 0.82071316)])\n```", "```\nimport random\n\nP = 67280421310721\n```", "```\ndef encrypt(x):\n    \"\"\"Encrypts an integer between 3 partires.\"\"\"\n    share_a = random.randint(0, P)\n    share_b = random.randint(0, P)\n    share_c = (x - share_a - share_b) % P\n    return (share_a, share_b, share_c)\n```", "```\nx = 17\nshare_a, share_b, share_c = encrypt(x)\nprint(share_a, share_b, share_c)\n\n16821756678516 13110264723730 37348399908492\n```", "```\ndef decrypt(share_a, share_b, share_c):\n    \"\"\"Decrypts the integer from 3 shares.\"\"\"\n    return (share_a + share_b + share_c) % P\n```", "```\ndecrypt(share_a, share_b, share_c)\n```", "```\n17\n```", "```\ndef add(x, y):\n    \"\"\"Addition of encrypted integers.\"\"\"\n    z = list()\n    z.append((x[0] + y[0]) % P)\n    z.append((x[1] + y[1]) % P)\n    z.append((x[2] + y[2]) % P)\n    return z\n```", "```\nx = encrypt(5)\ny = encrypt(9)\ndecrypt(*add(x, y))\n\n14\n```", "```\npip install torch torchvision syft\n```", "```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n```", "```\nimport syft as sy\n\nhook = sy.TorchHook(torch)\nclient = sy.VirtualWorker(hook, id=\"client\")\nserver = sy.VirtualWorker(hook, id=\"server\")\ncrypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\")\n```", "```\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 784)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x\n```", "```\nmodel = Net()\nmodel.load_state_dict(torch.load(\"server_trained_model.pt\"))\nmodel.eval()\n```", "```\nmodel.fix_precision().share(client, server, crypto_provider=crypto_provider)\n```", "```\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\n        \"data\",\n        train=False,\n        download=True,\n        transform=transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        ),\n    ),\n    batch_size=64,\n    shuffle=True,\n)\n```", "```\nprivate_test_loader = []\nfor data, target in test_loader:\n    private_test_loader.append(\n        (\n            data.fix_precision().share(client, server, crypto_provider=crypto_provider),\n            target.fix_precision().share(\n                client, server, crypto_provider=crypto_provider\n            ),\n        )\n    )\n```", "```\ndef test(model, test_loader):\n    \"\"\"Test the model.\"\"\"\n    model.eval()\n    n_correct_priv = 0\n    n_total = 0\n```", "```\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data)\n            pred = output.argmax(dim=1)\n            n_correct_priv += pred.eq(target.view_as(pred)).sum()\n            n_total += 64\n            n_correct = \n            n_correct_priv.copy().get().float_precision().long().item()\n            print(\n                \"Test set: Accuracy: {}/{} ({:.0f}%)\".format(\n                    n_correct, n_total, 100.0 * n_correct / n_total\n                )\n            )\n```", "```\ntest(model, private_test_loader)\n```", "```\nTest set: Accuracy: 63/64 (98%)\nTest set: Accuracy: 123/128 (96%)\nTest set: Accuracy: 185/192 (96%)\nTest set: Accuracy: 248/256 (97%)\nTest set: Accuracy: 310/320 (97%)\nTest set: Accuracy: 373/384 (97%)\nTest set: Accuracy: 433/448 (97%)\n<snip>\nTest set: Accuracy: 9668/9920 (97%)\nTest set: Accuracy: 9727/9984 (97%)\nTest set: Accuracy: 9742/10048 (97%)\n```", "```\npip install torch torchvision scipy foolbox==1.8 matplotlib\n```", "```\nfrom abs_models import models\nfrom abs_models import utils\n\nABS_model = models.get_VAE(n_iter=50)\n```", "```\nimport numpy as np\n\ndef predict_on_batch(model, batch, batch_size):\n    \"\"\"Predicts the digits of an MNIST batch.\"\"\"\n    preds = []\n    labels = []\n    for i in range(batch_size):\n        point, label = utils.get_batch()\n        labels.append(label[0])\n        tensor_point = utils.n2t(point)\n        logits = model(tensor_point)[0]\n        logits = [x for x in logits]\n        pred = np.argmax(logits)\n        preds.append(int(pred))\n    return preds, labels\n```", "```\nbatch = utils.get_batch()\npreds, labels = predict_on_batch(ABS_model, batch, 5)\nprint(preds)\nprint(labels)\n```", "```\n[4, 4, 9, 1, 8]\n[4, 4, 9, 1, 8]\n```", "```\nimport foolbox\n\nif ABS_model.code_base == \"tensorflow\":\n    fmodel = foolbox.models.TensorFlowModel(\n        ABS_model.x_input, ABS_model.pre_softmax, (0.0, 1.0), channel_axis=3\n    )\nelif ABS_model.code_base == \"pytorch\":\n    ABS_model.eval()\n    fmodel = foolbox.models.PyTorchModel(\n        ABS_model, bounds=(0.0, 1.0), num_classes=10, device=utils.dev()\n    )\n```", "```\nfrom foolbox import attacks\n\nimages, labels = utils.get_batch(bs=1)\n```", "```\nattack = attacks.DeepFoolL2Attack(fmodel)\nmetric = foolbox.distances.MSE\ncriterion = foolbox.criteria.Misclassification()\n```", "```\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nplt.imshow(images[0, 0], cmap=\"gray\")\nplt.title(\"original image\")\nplt.axis(\"off\")\nplt.show()\n```", "```\ngradient_estimator = foolbox.gradient_estimators.CoordinateWiseGradientEstimator(0.1)\nfmodel = foolbox.models.ModelWithEstimatedGradients(fmodel, gradient_estimator)\n\nadversary = foolbox.adversarial.Adversarial(\n    fmodel, criterion, images[0], labels[0], distance=metric\n)\nattack(adversary)\n```", "```\nplt.imshow(a.image[0], cmap=\"gray\")\nplt.title(\"adversarial image\")\nplt.axis(\"off\")\nplt.show()\nprint(\"Model prediction:\", np.argmax(fmodel.predictions(adversary.image)))\n```", "```\nfrom abs_models import models\n\ntraditional_model = models.get_CNN()\n```", "```\nCNN(\n  (net): NN(\n    (conv_0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n    (bn_0): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (nl_0): ELU(alpha=1.0)\n    (conv_1): Conv2d(20, 70, kernel_size=(4, 4), stride=(2, 2))\n    (bn_1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (nl_1): ELU(alpha=1.0)\n    (conv_2): Conv2d(70, 256, kernel_size=(3, 3), stride=(2, 2))\n    (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (nl_2): ELU(alpha=1.0)\n    (conv_3): Conv2d(256, 10, kernel_size=(5, 5), stride=(1, 1))\n  )\n  (model): NN(\n    (conv_0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n    (bn_0): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (nl_0): ELU(alpha=1.0)\n    (conv_1): Conv2d(20, 70, kernel_size=(4, 4), stride=(2, 2))\n    (bn_1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (nl_1): ELU(alpha=1.0)\n    (conv_2): Conv2d(70, 256, kernel_size=(3, 3), stride=(2, 2))\n    (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (nl_2): ELU(alpha=1.0)\n    (conv_3): Conv2d(256, 10, kernel_size=(5, 5), stride=(1, 1))\n  )\n)\n```", "```\npreds, labels = predict_on_batch(traditional_model, batch, 5)\nprint(preds)\nprint(labels)\n```", "```\n[7, 9, 5, 3, 3]\n[7, 9, 5, 3, 3]\n```", "```\nif traditional_model.code_base == \"tensorflow\":\n    fmodel_traditional = foolbox.models.TensorFlowModel(\n        traditional_model.x_input,\n        traditional_model.pre_softmax,\n        (0.0, 1.0),\n        channel_axis=3,\n    )\nelif traditional_model.code_base == \"pytorch\":\n    traditional_model.eval()\n    fmodel_traditional = foolbox.models.PyTorchModel(\n        traditional_model, bounds=(0.0, 1.0), num_classes=10, device=u.dev()\n    )\n```", "```\nfmodel_traditional = foolbox.models.ModelWithEstimatedGradients(fmodel_traditional, GE)\n\nadversarial_traditional = foolbox.adversarial.Adversarial(\n    fmodel_traditional, criterion, images[0], labels[0], distance=metric\n)\nattack(adversarial_traditional)\n```", "```\nplt.imshow(adversarial_traditional.image[0], cmap=\"gray\")\nplt.title(\"adversarial image\")\nplt.axis(\"off\")\nplt.show()\nprint(\n    \"Model prediction:\",\n    np.argmax(fmodel_traditional.predictions(adversarial_traditional.image)),\n)\n```", "```\npip install keras tensorflow\n```", "```\nimport tensorflow as tf\n\ndef preprocess_observations(data):\n    \"\"\"Preprocesses MNIST images.\"\"\"\n    data = np.array(data, dtype=np.float32) / 255\n    data = data.reshape(data.shape[0], 28, 28, 1)\n    return data\n\ndef preprocess_labels(labels):\n    \"\"\"Preprocess MNIST labels.\"\"\"\n    labels = np.array(labels, dtype=np.int32)\n    labels = tf.keras.utils.to_categorical(labels, num_classes=10)\n```", "```\ndef load_mnist():\n    \"\"\"Loads the MNIST dataset.\"\"\"\n    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n    X_train = preprocess_observations(X_train)\n    X_test = preprocess_observations(X_test)\n    y_train = preprocess_labels(y_train)\n    y_test = preprocess_labels(y_test)\n    return X_train, y_train, X_test, y_test\n```", "```\nimport numpy as np\n\nX_train, y_train, X_test, y_test = load_mnist()\n```", "```\nfrom privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n\noptimizer = DPGradientDescentGaussianOptimizer(\n    l2_norm_clip=1.0, noise_multiplier=1.1, num_microbatches=250, learning_rate=0.15\n)\nloss = tf.keras.losses.CategoricalCrossentropy(\n    from_logits=True, reduction=tf.losses.Reduction.NONE\n)\n```", "```\nfrom privacy.analysis.rdp_accountant import compute_rdp\nfrom privacy.analysis.rdp_accountant import get_privacy_spent\n\ndef compute_epsilon(steps):\n    \"\"\"Compute the privacy epsilon.\"\"\"\n    orders = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n    sampling_probability = 250 / 60000\n    rdp = compute_rdp(\n        q=sampling_probability, noise_multiplier=1.1, steps=steps, orders=orders\n    )\n    return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n```", "```\nNN_model = tf.keras.Sequential(\n    [\n        tf.keras.layers.Conv2D(\n            16, 8, strides=2, padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1)\n        ),\n        tf.keras.layers.MaxPool2D(2, 1),\n        tf.keras.layers.Conv2D(32, 4, strides=2, padding=\"valid\", activation=\"relu\"),\n        tf.keras.layers.MaxPool2D(2, 1),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(32, activation=\"relu\"),\n        tf.keras.layers.Dense(10),\n    ]\n)\n```", "```\nNN_model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n```", "```\nNN_model.fit(\n    X_train, y_train, epochs=1, validation_data=(X_test, y_test), batch_size=250\n)\n```", "```\neps = compute_epsilon(1 * 60000 // 250)\n```"]