- en: Secure and Private AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning can help us diagnose and fight cancer, decide which school
    is the best for our children and make the smartest real estate investment. But
    you can only answer these questions with access to private and personal data,
    which requires a novel approach to machine learning. This approach is called *Secure
    and Private AI* and, in recent years, has seen great strides, as you will see
    in the following recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Federated learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encrypted computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Private deep learning prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the adversarial robustness of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differential privacy using TensorFlow Privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical prerequisites for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Federated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foolbox
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torchvision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation instructions, code, and datasets may be found at [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter08](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Federated learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will train a federated learning model using the TensorFlow
    federated framework.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why federated learning is valuable, consider the *next word prediction*
    model on your mobile phone when you write an SMS message. For privacy reasons,
    you wouldn't want the data, that is, your text messages, to be sent to a central
    server to be used for training the next word predictor. But it's still nice to
    have an accurate next word prediction algorithm. What to do? This is where federated
    learning comes in, which is a machine learning technique developed to tackle such
    privacy concerns.
  prefs: []
  type: TYPE_NORMAL
- en: The core idea in federated learning is that a training dataset remains in the
    hands of its producers, preserving privacy and ownership, while still being used
    to train a centralized model. This feature is especially attractive in cybersecurity,
    where, for example, collecting benign and malicious samples from many different
    sources is crucial to creating a strong model, but difficult on account of privacy
    concerns (by way of an example, a benign sample can be a personal or confidential
    document).
  prefs: []
  type: TYPE_NORMAL
- en: In passing, we should mention the fact that federated learning has been gaining
    more and more traction due to the increasing importance of data privacy (for example,
    the enactment of GDPR). Large actors, such as Apple and Google, have started investing
    heavily in this technology.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing the `tensorflow_federated`,
    `tensorflow_datasets`, and `tensorflow` packages in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will install specific versions of these packages to prevent any breaks in
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following steps, you will create two virtual dataset environments—one
    belonging to Alice, and one belonging to Bob—and use federated averaging to preserve
    data confidentiality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import TensorFlow and enable eager execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare a dataset by importing Fashion MNIST and splitting it into two separate
    environments, Alice and Bob:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define a `helper` function to cast the data type from integer to float:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, define a `helper` function to flatten the data to be fed into a neural
    network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define a `helper` function to pre-process the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Pre-process the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define a `loss` function for our neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to instantiate a simple Keras neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a dummy batch of samples and define a function to return a federated
    learning model from the Keras model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare an iterative process of federated averaging, and run one stage of the
    computation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, display the metrics of the computation by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by importing TensorFlow and enabling eager execution (*Step 1*). Ordinarily,
    in TensorFlow, operations are not performed immediately. Rather, a computation
    graph is built, and, at the very end, all operations are run together. In eager
    execution, computations are executed as soon as possible. Next, in *Step 2*, we
    import the Fashion MNIST dataset. This dataset has become a *de facto* replacement
    for MNIST, offering several improvements over it (such as added challenges). We
    then subdivide the dataset 50:50 between Alice and Bob. We then define a function
    to cast the pixel values of Fashion MNIST from integers to floats to be used in
    the training of our neural network (*Step 3*) and another function to flatten
    the images into a single vector (*Step 4*). This enables us to feed the data into
    a fully connected neural network. In *Steps 5* and *6*, we employ the previously
    defined convenience functions to pre-process Alice and Bob's datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a loss function that makes sense for our 10-class classification
    task (*Step 7*), and then define our Keras neural network in preparation for training
    (*Step 8*). In *Step 9*, we create a dummy batch of samples and define a function
    to return a federated learning model from the Keras model. The dummy batch of
    samples specifies the shape of input for the model to expect. In *Step 10*, we
    run one stage of the federated averaging process. Details regarding the algorithm
    can be found in the paper entitled *Communication-Efficient Learning of Deep Networks
    from Decentralized Data*.
  prefs: []
  type: TYPE_NORMAL
- en: At a basic level, the algorithm combines local **stochastic gradient descent**
    (**SGD**) on the data of each client, and then uses a server that performs model
    averaging. The result is conserved confidentiality for the clients (in our case,
    Alice and Bob). Finally, in *Step 11*, we observe our performance, seeing that
    the algorithm indeed does train and improve accuracy, as intended.
  prefs: []
  type: TYPE_NORMAL
- en: Encrypted computation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we're going to walk through the basics of encrypted computation.
    In particular, we're going to focus on one popular approach, called Secure Multi-Party
    Computation. You'll learn how to build a simple encrypted calculator that can
    perform addition on encrypted numbers. The ideas in this recipe will come in handy
    in the *Private deep learning prediction* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following recipe has no installation requirements other than Python.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the random library and select a large prime number, `P`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Define an encryption function for three parties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Encrypt a numerical variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to decrypt, given the three shares:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Decrypt the encrypted variable `x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to add two encrypted numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Add two encrypted variables and decrypt their sum:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin *Step 1* by importing the random library in order to generate random
    integers in *Step 2*. We also define a large prime number, P, as we will be wanting
    a random distribution modulo, P. In *Step 2*, we define how a function encrypts
    an integer by splitting it between three parties. The value of x here is randomly
    additively split between the three parties. All operations take place in the field
    of integer modulo P. Next, in *Step 3*, we demonstrate the result of encrypting
    an integer using our approach. Proceeding to *Steps 4* and *5*, we define a function
    to reverse encryption, that is decrypt, and then show that the operation is reversible.
    In *Step 6*, we define a function to add two encrypted numbers(!). Note that encrypted
    addition is simply addition of the individual components, modulo P. In the *Encrypted
    deep learning prediction* recipe, the `.share(client, server,...)` command from
    PySyft is used. This command is basically the same encryption procedure we have
    used in this recipe, so keep in mind that these encryption schemes use the techniques
    we are discussing here. Finally, in *Step 7*, we demonstrate that we can perform
    computations on encrypted entities.
  prefs: []
  type: TYPE_NORMAL
- en: Private deep learning prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many situations, company A might have a trained model that it wishes to offer
    as a service. At the same time, company A may be reluctant to share this model,
    so as to avoid having its intellectual property stolen. The simple solution to
    this problem is to have customers send their data to company A, and then receive
    from it predictions. However, this becomes a problem when the customer wishes
    to preserve the privacy of their data. To resolve this tricky situation, the company,
    as well as its customers, can utilize encrypted computation.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to share an encrypted pre-trained deep learning
    model with a client and allow the client to predict using the encrypted model
    on their own private data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing PyTorch, Torchvision, and PySyft
    in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In addition, a pre-trained model named `server_trained_model.pt` has been included
    to be used in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following steps utilize PySyft to simulate a client-server interaction in
    which the server has a pre-trained deep learning model to be kept as a black box,
    and the client wishes to use the model to predict on data that is kept private.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `torch` and access its datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Import PySyft and hook it onto `torch`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a simple neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the model and load its pre-trained weights, which are trained on
    MNIST:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Encrypt the network between `client` and `server`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a loader for MNIST data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a private loader utilizing the loader for MNIST data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to evaluate the private test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate over the private data, predict using the model, decrypt the results,
    and then print them out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the testing procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by importing `torch` and its datasets, as well as some associated libraries
    (*Step 1*). We then import `pysyft` and hook it into `torch` (*Step 2*). We also
    create virtual environments for the client and server to simulate a real separation
    of data. In this step, the `crypto_provider` serves as a trusted third party to
    be used for encryption and decryption purposes. In *Step 3*, we define a simple
    neural network and, in *Step 4*, we load-in its pretrained weights. Note that,
    in *Step 5*, and, more generally, whenever the `.share(...)` command is used,
    you should think of the shared object as being encrypted, and that it is only
    possible to decrypt it with the assistance of all parties involved. In particular,
    in *Step 9*, the test function performs encrypted evaluation; the weights of the
    model, the data inputs, the prediction, and the target used for scoring are all
    encrypted. However, for the purpose of verifying that the model is working properly,
    we decrypt and display its accuracy. In *Step 5*, we encrypt the network so that
    only when the server and client are cooperating can they decrypt the network.
  prefs: []
  type: TYPE_NORMAL
- en: In the next two steps, we define regular and private loaders for MNIST data.
    The regular loader simply loads MNIST data, while the private loader encrypts
    the outputs of the regular loader. In *Steps 8* and *9*, we define a `helper`
    function to evaluate the private test set. In this function, we iterate over the
    private data, predict using the model, decrypt the results, and then print them
    out. Finally, we apply the function defined in *Steps 8* and *9* to establish
    that the model is performing well, while preserving privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the adversarial robustness of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The study of adversarial attacks on neural networks has revealed a surprising
    sensitivity to adversarial perturbations. Even the most accurate of neural networks,
    when left undefended, has been shown to be vulnerable to single pixel attacks
    and the peppering of invisible-to-the-human-eye noise. Fortunately, recent advances
    in the field have offered solutions on how to harden neural networks to adversarial
    attacks of all sorts. One such solution is a neural network design called **Analysis
    by Synthesis** (**ABS**). The main idea behind the model is that it is a Bayesian
    model. Rather than directly predicting the label given the input, the model also
    learns class-conditional, sample distributions using **variational autoencoders**
    (**VAEs**). More information can be found in [https://arxiv.org/abs/1805.09190](https://arxiv.org/abs/1805.09190).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will load a pre-trained ABS network for MNIST and learn
    how to test a neural network for adversarial robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following recipe has been tested in Python 3.6\. Preparation for this recipe
    involves installing the Pytorch, Torchvision, SciPy, Foolbox, and Matplotlib packages
    in `pip`. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, we will load a pre-trained ABS model and a traditional
    CNN model for MNIST. We will attack both models using Foolbox to see how well
    they can defend against adversarial attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by importing a pre-trained ABS model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `convenience` function to predict a batch of MNIST images using a
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict on a batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Wrap the model using Foolbox to enable adversarial testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the library of attacks from Foolbox and select an MNIST image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the attack type, in this case, a boundary attack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the original image and its label using Matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The image produced is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0cb4d4d5-9a8f-43b7-ad67-f60877d32b84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Search for an adversarial instance using Foolbox:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Show the discovered adversarial example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The adversarial image produced is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f3113288-d9d3-4d8b-81d2-0c0179c5685f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instantiate a traditional CNN model trained on MNIST:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The model architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform a sanity check to make sure that the model is performing as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The printout is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Wrap the traditional model using Foolbox:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Attack the traditional CNN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the adversarial example discovered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The adversarial image produced is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f9fbf766-8c65-4f3d-98fd-5e2f36e9fa55.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by importing a pre-trained ABS model (*Step 1*). In *Steps 2* and *3*,
    we defined a `convenience` function to predict a batch of MNIST images and to
    verify that the model is working properly. Next, we wrapped the model using Foolbox
    in preparation for testing its adversarial robustness (*Step 4*). Note that Foolbox
    facilitates the attacking of either TensorFlow or PyTorch models using the same
    API once wrapped. Nice! In *Step 5*, we select an MNIST image to use as the medium
    for our attack. To clarify, this image gets tweaked and mutated until the result
    fools the model. In *Step 6*, we select the attack type we want to implement.
    We select a boundary attack, which is a decision-based attack that starts from
    a large adversarial perturbation and then gradually reduces the perturbation while
    remaining adversarial. The attack requires little hyperparameter tuning, hence,
    no substitute models and no gradient computations. For more information about
    decision-based attacks, refer to [https://arxiv.org/abs/1712.04248](https://arxiv.org/abs/1712.04248).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, note that the metric used here is **mean squared error** (**MSE**),
    which determines how the adversarial example is assessed as close to, or far from,
    the original image. The criterion used is misclassification, meaning that the
    search terminates once the target model misclassifies the image. Alternative criteria
    may include a confidence level or a specific type of misclassification. In *Steps
    7-9*, we display the original image, as well as the adversarial example generated
    from it. In the next two steps, we instantiate a standard CNN and verify that
    it is working properly. In *Steps 12-14*, we repeat the attack from the previous
    steps on the standard CNN. Looking at the result, we see that the experiment is
    a strong visual indicator that the ABS model is more robust to adversarial perturbations
    than a vanilla CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy using TensorFlow Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow Privacy ([https://github.com/tensorflow/privacy](https://github.com/tensorflow/privacy)) is
    a relatively new addition to the TensorFlow family. This Python library includes
    implementations of TensorFlow optimizers for training machine learning models
    with *differential privacy*. A model that has been trained to be differentially
    private does not non-trivially change as a result of the removal of any single
    training instance from its dataset. (Approximate) differential privacy is quantified
    using *epsilon* and *delta*, which give a measure of how sensitive the model is
    to a change in a single training example. Using the Privacy library is as simple
    as wrapping the familiar optimizers (for example, RMSprop, Adam, and SGD) to convert
    them to a differentially private version. This library also provides convenient
    tools for measuring the privacy guarantees, epsilon, and delta.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we show you how to implement and train a differentially private
    deep neural network for MNIST using Keras and TensorFlow Privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparation for this recipe involves installing Keras and TensorFlow. The command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Installation instructions for TensorFlow Privacy can be found at [https://github.com/tensorflow/privacy](https://github.com/tensorflow/privacy).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Begin by defining a few convenience functions for pre-processing the MNIST
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a convenience function to load MNIST:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the MNIST dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The training set is 60 k in size, and the testing set 10 k.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Import a differentially private optimizer and define a few parameters that
    control the learning rate and extent of differential privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to measure privacy, define a function to compute epsilon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a standard Keras CNN for MNIST:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling `model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit and test `model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the value of `epsilon`, the measure of privacy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin *Steps 1-3* by preparing and loading the MNIST dataset. Next, in *Step
    4*, we import `DPGradientDescentGaussianOptimizer`, an optimizer that allows the
    model to become differentially private. A number of parameters are used at this
    stage, and these stand to be clarified. The `l2_norm_clip` parameter refers to
    the maximum norm of each gradient computed on an individual training datapoint
    from a minibatch. This parameter bounds the sensitivity of the optimizer to individual
    training points, thereby moving the model toward differential privacy. The `noise_multiplier`
    parameter controls the amount of random noise added to gradients. Generally, the
    more noise, the greater the privacy. Having finished this step, in *Step 5*, we
    define a function that computes the epsilon of the epsilon-delta definition of
    differential privacy. We instantiate a standard Keras neural network (*Step 6*),
    compile it (*Step 7*), and then train it on MNIST using the differentially private
    optimizer (*Step 8*). Finally, in *Step 9*, we compute the value of epsilon, which
    measures the extent to which the model is differentially private. Typical values
    for this recipe are an epsilon value of around 1.
  prefs: []
  type: TYPE_NORMAL
